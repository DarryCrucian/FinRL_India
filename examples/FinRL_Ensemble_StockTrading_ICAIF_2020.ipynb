{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lb9q2_QZgdNk"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/AI4Finance-Foundation/FinRL-Tutorials/blob/master/2-Advance/FinRL_Ensemble_StockTrading_ICAIF_2020.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gXaoZs2lh1hi"
   },
   "source": [
    "# Deep Reinforcement Learning for Stock Trading from Scratch: Multiple Stock Trading Using Ensemble Strategy\n",
    "\n",
    "Tutorials to use OpenAI DRL to trade multiple stocks using ensemble strategy in one Jupyter Notebook | Presented at ICAIF 2020\n",
    "\n",
    "* This notebook is the reimplementation of our paper: Deep Reinforcement Learning for Automated Stock Trading: An Ensemble Strategy, using FinRL.\n",
    "* Check out medium blog for detailed explanations: https://medium.com/@ai4finance/deep-reinforcement-learning-for-automated-stock-trading-f1dad0126a02\n",
    "* Please report any issues to our Github: https://github.com/AI4Finance-LLC/FinRL-Library/issues\n",
    "* **Pytorch Version** \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lGunVt8oLCVS"
   },
   "source": [
    "# Content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HOzAKQ-SLGX6"
   },
   "source": [
    "* [1. Problem Definition](#0)\n",
    "* [2. Getting Started - Load Python packages](#1)\n",
    "    * [2.1. Install Packages](#1.1)    \n",
    "    * [2.2. Check Additional Packages](#1.2)\n",
    "    * [2.3. Import Packages](#1.3)\n",
    "    * [2.4. Create Folders](#1.4)\n",
    "* [3. Download Data](#2)\n",
    "* [4. Preprocess Data](#3)        \n",
    "    * [4.1. Technical Indicators](#3.1)\n",
    "    * [4.2. Perform Feature Engineering](#3.2)\n",
    "* [5.Build Environment](#4)  \n",
    "    * [5.1. Training & Trade Data Split](#4.1)\n",
    "    * [5.2. User-defined Environment](#4.2)   \n",
    "    * [5.3. Initialize Environment](#4.3)    \n",
    "* [6.Implement DRL Algorithms](#5)  \n",
    "* [7.Backtesting Performance](#6)  \n",
    "    * [7.1. BackTestStats](#6.1)\n",
    "    * [7.2. BackTestPlot](#6.2)   \n",
    "    * [7.3. Baseline Stats](#6.3)   \n",
    "    * [7.3. Compare to Stock Market Index](#6.4)             "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sApkDlD9LIZv"
   },
   "source": [
    "<a id='0'></a>\n",
    "# Part 1. Problem Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HjLD2TZSLKZ-"
   },
   "source": [
    "This problem is to design an automated trading solution for single stock trading. We model the stock trading process as a Markov Decision Process (MDP). We then formulate our trading goal as a maximization problem.\n",
    "\n",
    "The algorithm is trained using Deep Reinforcement Learning (DRL) algorithms and the components of the reinforcement learning environment are:\n",
    "\n",
    "\n",
    "* Action: The action space describes the allowed actions that the agent interacts with the\n",
    "environment. Normally, a ∈ A includes three actions: a ∈ {−1, 0, 1}, where −1, 0, 1 represent\n",
    "selling, holding, and buying one stock. Also, an action can be carried upon multiple shares. We use\n",
    "an action space {−k, ..., −1, 0, 1, ..., k}, where k denotes the number of shares. For example, \"Buy\n",
    "10 shares of AAPL\" or \"Sell 10 shares of AAPL\" are 10 or −10, respectively\n",
    "\n",
    "* Reward function: r(s, a, s′) is the incentive mechanism for an agent to learn a better action. The change of the portfolio value when action a is taken at state s and arriving at new state s',  i.e., r(s, a, s′) = v′ − v, where v′ and v represent the portfolio\n",
    "values at state s′ and s, respectively\n",
    "\n",
    "* State: The state space describes the observations that the agent receives from the environment. Just as a human trader needs to analyze various information before executing a trade, so\n",
    "our trading agent observes many different features to better learn in an interactive environment.\n",
    "\n",
    "* Environment: Dow 30 consituents\n",
    "\n",
    "\n",
    "The data of the single stock that we will be using for this case study is obtained from Yahoo Finance API. The data contains Open-High-Low-Close price and volume.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ffsre789LY08"
   },
   "source": [
    "<a id='1'></a>\n",
    "# Part 2. Getting Started- Load Python Packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uy5_PTmOh1hj"
   },
   "source": [
    "<a id='1.1'></a>\n",
    "## 2.1. Install all the packages through FinRL library\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mPT0ipYE28wL",
    "outputId": "75fcd958-c29f-44f0-85ea-4b4f6ae180ec"
   },
   "outputs": [],
   "source": [
    "# ## install finrl library\n",
    "!pip install wrds\n",
    "!pip install swig\n",
    "!pip install git+https://github.com/DarryCrucian/FinRL_IN.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "osBHhVysOEzi"
   },
   "source": [
    "\n",
    "<a id='1.2'></a>\n",
    "## 2.2. Check if the additional packages needed are present, if not install them. \n",
    "* Yahoo Finance API\n",
    "* pandas\n",
    "* numpy\n",
    "* matplotlib\n",
    "* stockstats\n",
    "* OpenAI gym\n",
    "* stable-baselines\n",
    "* tensorflow\n",
    "* pyfolio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nGv01K8Sh1hn"
   },
   "source": [
    "<a id='1.3'></a>\n",
    "## 2.3. Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "EeMK7Uentj1V"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "lPqeTTwoh1hn"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "# matplotlib.use('Agg')\n",
    "import datetime\n",
    "\n",
    "%matplotlib inline\n",
    "from finrl.config_tickers import SENSEX_TICKER\n",
    "from finrl.meta.preprocessor.yahoodownloader import YahooDownloader\n",
    "from finrl.meta.preprocessor.preprocessors import FeatureEngineer, data_split\n",
    "from finrl.meta.env_stock_trading.env_stocktrading import StockTradingEnv\n",
    "from finrl.agents.stablebaselines3.models import DRLAgent,DRLEnsembleAgent\n",
    "from finrl.plot import backtest_stats, backtest_plot, get_daily_return, get_baseline\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../FinRL-Library\")\n",
    "\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T2owTj985RW4"
   },
   "source": [
    "<a id='1.4'></a>\n",
    "## 2.4. Create Folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "w9A8CN5R5PuZ"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from finrl.main import check_and_make_directories\n",
    "from finrl.config import (\n",
    "    DATA_SAVE_DIR,\n",
    "    TRAINED_MODEL_DIR,\n",
    "    TENSORBOARD_LOG_DIR,\n",
    "    RESULTS_DIR,\n",
    "    INDICATORS,\n",
    "    TRAIN_START_DATE,\n",
    "    TRAIN_END_DATE,\n",
    "    TEST_START_DATE,\n",
    "    TEST_END_DATE,\n",
    "    TRADE_START_DATE,\n",
    "    TRADE_END_DATE,\n",
    ")\n",
    "\n",
    "check_and_make_directories([DATA_SAVE_DIR, TRAINED_MODEL_DIR, TENSORBOARD_LOG_DIR, RESULTS_DIR])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A289rQWMh1hq"
   },
   "source": [
    "<a id='2'></a>\n",
    "# Part 3. Download Data\n",
    "Yahoo Finance is a website that provides stock data, financial news, financial reports, etc. All the data provided by Yahoo Finance is free.\n",
    "* FinRL uses a class **YahooDownloader** to fetch data from Yahoo Finance API\n",
    "* Call Limit: Using the Public API (without authentication), you are limited to 2,000 requests per hour per IP (or up to a total of 48,000 requests a day).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NPeQ7iS-LoMm"
   },
   "source": [
    "\n",
    "\n",
    "-----\n",
    "class YahooDownloader:\n",
    "    Provides methods for retrieving daily stock data from\n",
    "    Yahoo Finance API\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "        start_date : str\n",
    "            start date of the data (modified from config.py)\n",
    "        end_date : str\n",
    "            end date of the data (modified from config.py)\n",
    "        ticker_list : list\n",
    "            a list of stock tickers (modified from config.py)\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    fetch_data()\n",
    "        Fetches data from yahoo API\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JzqRRTOX6aFu",
    "outputId": "178c70ab-72e5-4ed7-cfa8-fd6ea7b1e8ad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ASIANPAINT.NS', 'AXISBANK.NS', 'BAJFINANCE.NS', 'BAJAJFINSV.NS', 'BHARTIARTL.NS', 'HCLTECH.NS', 'HDFCBANK.NS', 'HINDUNILVR.NS', 'ICICIBANK.NS', 'INDUSINDBK.NS', 'INFY.NS', 'JSWSTEEL.NS', 'KOTAKBANK.NS', 'LT.NS', 'M&M.NS', 'MARUTI.NS', 'NESTLEIND.NS', 'NTPC.NS', 'POWERGRID.NS', 'RELIANCE.NS', 'SBIN.NS', 'SUNPHARMA.NS', 'TCS.NS', 'TATAMOTORS.NS', 'TATASTEEL.NS', 'TECHM.NS', 'TITAN.NS', 'ULTRACEMCO.NS', 'WIPRO.NS']\n"
     ]
    }
   ],
   "source": [
    "print(SENSEX_TICKER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yCKm4om-s9kE",
    "outputId": "0a5b0405-7c4f-4afd-c3e1-1dabd55c81fb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of DataFrame:  (97847, 8)\n"
     ]
    }
   ],
   "source": [
    "# TRAIN_START_DATE = '2009-04-01'\n",
    "# TRAIN_END_DATE = '2021-01-01'\n",
    "# TEST_START_DATE = '2021-01-01'\n",
    "# TEST_END_DATE = '2022-06-01'\n",
    "\n",
    "TRAIN_START_DATE = '2010-01-01'\n",
    "TRAIN_END_DATE = '2021-10-01'\n",
    "TEST_START_DATE = '2021-10-01'\n",
    "TEST_END_DATE = '2023-09-01'\n",
    "\n",
    "df = YahooDownloader(start_date = TRAIN_START_DATE,\n",
    "                     end_date = TEST_END_DATE,\n",
    "                     ticker_list = SENSEX_TICKER).fetch_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "GiRuFOTOtj1Y",
    "outputId": "402874c0-b13f-437b-a67f-a83f88de66eb"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>tic</th>\n",
       "      <th>day</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2010-01-04</td>\n",
       "      <td>179.100006</td>\n",
       "      <td>179.990005</td>\n",
       "      <td>178.000000</td>\n",
       "      <td>159.429886</td>\n",
       "      <td>80350</td>\n",
       "      <td>ASIANPAINT.NS</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2010-01-04</td>\n",
       "      <td>198.779999</td>\n",
       "      <td>199.990005</td>\n",
       "      <td>197.619995</td>\n",
       "      <td>180.921799</td>\n",
       "      <td>4371510</td>\n",
       "      <td>AXISBANK.NS</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2010-01-04</td>\n",
       "      <td>34.332169</td>\n",
       "      <td>35.491478</td>\n",
       "      <td>34.017780</td>\n",
       "      <td>34.740162</td>\n",
       "      <td>2469924</td>\n",
       "      <td>BAJAJFINSV.NS</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2010-01-04</td>\n",
       "      <td>32.324509</td>\n",
       "      <td>33.509838</td>\n",
       "      <td>32.324509</td>\n",
       "      <td>30.491297</td>\n",
       "      <td>466064</td>\n",
       "      <td>BAJFINANCE.NS</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2010-01-04</td>\n",
       "      <td>297.279724</td>\n",
       "      <td>299.307526</td>\n",
       "      <td>291.872162</td>\n",
       "      <td>273.851196</td>\n",
       "      <td>3275801</td>\n",
       "      <td>BHARTIARTL.NS</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date        open        high         low       close   volume  \\\n",
       "0  2010-01-04  179.100006  179.990005  178.000000  159.429886    80350   \n",
       "1  2010-01-04  198.779999  199.990005  197.619995  180.921799  4371510   \n",
       "2  2010-01-04   34.332169   35.491478   34.017780   34.740162  2469924   \n",
       "3  2010-01-04   32.324509   33.509838   32.324509   30.491297   466064   \n",
       "4  2010-01-04  297.279724  299.307526  291.872162  273.851196  3275801   \n",
       "\n",
       "             tic  day  \n",
       "0  ASIANPAINT.NS    0  \n",
       "1    AXISBANK.NS    0  \n",
       "2  BAJAJFINSV.NS    0  \n",
       "3  BAJFINANCE.NS    0  \n",
       "4  BHARTIARTL.NS    0  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "DSw4ZEzVtj1Z",
    "outputId": "94617d16-432c-40eb-f758-16d2fdab09e0"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>tic</th>\n",
       "      <th>day</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>97842</th>\n",
       "      <td>2023-08-31</td>\n",
       "      <td>3401.000000</td>\n",
       "      <td>3417.399902</td>\n",
       "      <td>3343.649902</td>\n",
       "      <td>3332.694092</td>\n",
       "      <td>3417652</td>\n",
       "      <td>TCS.NS</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97843</th>\n",
       "      <td>2023-08-31</td>\n",
       "      <td>1199.000000</td>\n",
       "      <td>1214.500000</td>\n",
       "      <td>1198.000000</td>\n",
       "      <td>1189.130249</td>\n",
       "      <td>2817747</td>\n",
       "      <td>TECHM.NS</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97844</th>\n",
       "      <td>2023-08-31</td>\n",
       "      <td>3090.850098</td>\n",
       "      <td>3118.949951</td>\n",
       "      <td>3082.199951</td>\n",
       "      <td>3104.449951</td>\n",
       "      <td>1420498</td>\n",
       "      <td>TITAN.NS</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97845</th>\n",
       "      <td>2023-08-31</td>\n",
       "      <td>8288.799805</td>\n",
       "      <td>8360.000000</td>\n",
       "      <td>8226.000000</td>\n",
       "      <td>8297.450195</td>\n",
       "      <td>482614</td>\n",
       "      <td>ULTRACEMCO.NS</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97846</th>\n",
       "      <td>2023-08-31</td>\n",
       "      <td>408.000000</td>\n",
       "      <td>410.000000</td>\n",
       "      <td>406.450012</td>\n",
       "      <td>407.530853</td>\n",
       "      <td>11030976</td>\n",
       "      <td>WIPRO.NS</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             date         open         high          low        close  \\\n",
       "97842  2023-08-31  3401.000000  3417.399902  3343.649902  3332.694092   \n",
       "97843  2023-08-31  1199.000000  1214.500000  1198.000000  1189.130249   \n",
       "97844  2023-08-31  3090.850098  3118.949951  3082.199951  3104.449951   \n",
       "97845  2023-08-31  8288.799805  8360.000000  8226.000000  8297.450195   \n",
       "97846  2023-08-31   408.000000   410.000000   406.450012   407.530853   \n",
       "\n",
       "         volume            tic  day  \n",
       "97842   3417652         TCS.NS    3  \n",
       "97843   2817747       TECHM.NS    3  \n",
       "97844   1420498       TITAN.NS    3  \n",
       "97845    482614  ULTRACEMCO.NS    3  \n",
       "97846  11030976       WIPRO.NS    3  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CV3HrZHLh1hy",
    "outputId": "73944c23-5a4e-49f8-b9e5-da382b4fc7f5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(97847, 8)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "4hYkeaPiICHS",
    "outputId": "87cca0b1-8d3c-4a61-e061-ea0d9989daa1"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>tic</th>\n",
       "      <th>day</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2010-01-04</td>\n",
       "      <td>179.100006</td>\n",
       "      <td>179.990005</td>\n",
       "      <td>178.000000</td>\n",
       "      <td>159.429886</td>\n",
       "      <td>80350</td>\n",
       "      <td>ASIANPAINT.NS</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2010-01-04</td>\n",
       "      <td>198.779999</td>\n",
       "      <td>199.990005</td>\n",
       "      <td>197.619995</td>\n",
       "      <td>180.921799</td>\n",
       "      <td>4371510</td>\n",
       "      <td>AXISBANK.NS</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2010-01-04</td>\n",
       "      <td>34.332169</td>\n",
       "      <td>35.491478</td>\n",
       "      <td>34.017780</td>\n",
       "      <td>34.740162</td>\n",
       "      <td>2469924</td>\n",
       "      <td>BAJAJFINSV.NS</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2010-01-04</td>\n",
       "      <td>32.324509</td>\n",
       "      <td>33.509838</td>\n",
       "      <td>32.324509</td>\n",
       "      <td>30.491297</td>\n",
       "      <td>466064</td>\n",
       "      <td>BAJFINANCE.NS</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2010-01-04</td>\n",
       "      <td>297.279724</td>\n",
       "      <td>299.307526</td>\n",
       "      <td>291.872162</td>\n",
       "      <td>273.851196</td>\n",
       "      <td>3275801</td>\n",
       "      <td>BHARTIARTL.NS</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date        open        high         low       close   volume  \\\n",
       "0  2010-01-04  179.100006  179.990005  178.000000  159.429886    80350   \n",
       "1  2010-01-04  198.779999  199.990005  197.619995  180.921799  4371510   \n",
       "2  2010-01-04   34.332169   35.491478   34.017780   34.740162  2469924   \n",
       "3  2010-01-04   32.324509   33.509838   32.324509   30.491297   466064   \n",
       "4  2010-01-04  297.279724  299.307526  291.872162  273.851196  3275801   \n",
       "\n",
       "             tic  day  \n",
       "0  ASIANPAINT.NS    0  \n",
       "1    AXISBANK.NS    0  \n",
       "2  BAJAJFINSV.NS    0  \n",
       "3  BAJFINANCE.NS    0  \n",
       "4  BHARTIARTL.NS    0  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sort_values(['date','tic']).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a2vryMsdNL9H",
    "outputId": "6691ba9b-e613-412b-dba5-dee592bb0ff2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df.tic.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XcNyXa7RNPrF",
    "outputId": "edb04575-9b82-4d5e-f13a-55c884214725"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tic\n",
       "HCLTECH.NS       3375\n",
       "ASIANPAINT.NS    3374\n",
       "MARUTI.NS        3374\n",
       "ULTRACEMCO.NS    3374\n",
       "TITAN.NS         3374\n",
       "TECHM.NS         3374\n",
       "TCS.NS           3374\n",
       "TATASTEEL.NS     3374\n",
       "TATAMOTORS.NS    3374\n",
       "SUNPHARMA.NS     3374\n",
       "SBIN.NS          3374\n",
       "RELIANCE.NS      3374\n",
       "POWERGRID.NS     3374\n",
       "NTPC.NS          3374\n",
       "NESTLEIND.NS     3374\n",
       "M&M.NS           3374\n",
       "AXISBANK.NS      3374\n",
       "LT.NS            3374\n",
       "KOTAKBANK.NS     3374\n",
       "JSWSTEEL.NS      3374\n",
       "INFY.NS          3374\n",
       "INDUSINDBK.NS    3374\n",
       "ICICIBANK.NS     3374\n",
       "HINDUNILVR.NS    3374\n",
       "HDFCBANK.NS      3374\n",
       "BHARTIARTL.NS    3374\n",
       "BAJFINANCE.NS    3374\n",
       "BAJAJFINSV.NS    3374\n",
       "WIPRO.NS         3374\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tic.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uqC6c40Zh1iH"
   },
   "source": [
    "# Part 4: Preprocess Data\n",
    "Data preprocessing is a crucial step for training a high quality machine learning model. We need to check for missing data and do feature engineering in order to convert the data into a model-ready state.\n",
    "* Add technical indicators. In practical trading, various information needs to be taken into account, for example the historical stock prices, current holding shares, technical indicators, etc. In this article, we demonstrate two trend-following technical indicators: MACD and RSI.\n",
    "* Add turbulence index. Risk-aversion reflects whether an investor will choose to preserve the capital. It also influences one's trading strategy when facing different market volatility level. To control the risk in a worst-case scenario, such as financial crisis of 2007–2008, FinRL employs the financial turbulence index that measures extreme asset price fluctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "kM5bH9uroCeg"
   },
   "outputs": [],
   "source": [
    "INDICATORS = ['macd',\n",
    "                'rsi_30',\n",
    "                'cci_30',\n",
    "                'dx_30']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jgXfBcjxtj1a",
    "outputId": "bd80d5c7-6ab7-4938-e1aa-f60ff642dc02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully added technical indicators\n",
      "Successfully added turbulence index\n"
     ]
    }
   ],
   "source": [
    "fe = FeatureEngineer(use_technical_indicator=True,\n",
    "                     tech_indicator_list = INDICATORS,\n",
    "                     use_turbulence=True,\n",
    "                     user_defined_feature = False)\n",
    "\n",
    "processed = fe.preprocess_data(df)\n",
    "processed = processed.copy()\n",
    "processed = processed.fillna(0)\n",
    "processed = processed.replace(np.inf,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 337
    },
    "id": "grvhGJJII3Xn",
    "outputId": "06a440dc-bb85-4ce9-83ab-53b3a62f0cc6"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>tic</th>\n",
       "      <th>day</th>\n",
       "      <th>macd</th>\n",
       "      <th>rsi_30</th>\n",
       "      <th>cci_30</th>\n",
       "      <th>dx_30</th>\n",
       "      <th>turbulence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2931</th>\n",
       "      <td>2021-11-18</td>\n",
       "      <td>1148.0000</td>\n",
       "      <td>1155.000000</td>\n",
       "      <td>1113.500000</td>\n",
       "      <td>1008.085876</td>\n",
       "      <td>5384530</td>\n",
       "      <td>HCLTECH.NS</td>\n",
       "      <td>3</td>\n",
       "      <td>-16.992700</td>\n",
       "      <td>43.402816</td>\n",
       "      <td>-114.700846</td>\n",
       "      <td>39.403983</td>\n",
       "      <td>3.391223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1938</th>\n",
       "      <td>2017-11-09</td>\n",
       "      <td>439.7500</td>\n",
       "      <td>444.225006</td>\n",
       "      <td>436.225006</td>\n",
       "      <td>371.745605</td>\n",
       "      <td>1600452</td>\n",
       "      <td>HCLTECH.NS</td>\n",
       "      <td>3</td>\n",
       "      <td>-2.991294</td>\n",
       "      <td>49.801561</td>\n",
       "      <td>-23.629432</td>\n",
       "      <td>3.422008</td>\n",
       "      <td>0.084178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260</th>\n",
       "      <td>2011-01-14</td>\n",
       "      <td>115.7500</td>\n",
       "      <td>120.000000</td>\n",
       "      <td>115.587502</td>\n",
       "      <td>87.932831</td>\n",
       "      <td>3733116</td>\n",
       "      <td>HCLTECH.NS</td>\n",
       "      <td>4</td>\n",
       "      <td>1.987710</td>\n",
       "      <td>60.264347</td>\n",
       "      <td>95.773211</td>\n",
       "      <td>11.588899</td>\n",
       "      <td>0.358184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>633</th>\n",
       "      <td>2012-07-18</td>\n",
       "      <td>119.5625</td>\n",
       "      <td>120.812500</td>\n",
       "      <td>117.862503</td>\n",
       "      <td>90.921898</td>\n",
       "      <td>4858332</td>\n",
       "      <td>HCLTECH.NS</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.128618</td>\n",
       "      <td>47.898062</td>\n",
       "      <td>-38.658945</td>\n",
       "      <td>18.732847</td>\n",
       "      <td>0.059885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2404</th>\n",
       "      <td>2019-10-04</td>\n",
       "      <td>538.0000</td>\n",
       "      <td>540.900024</td>\n",
       "      <td>532.325012</td>\n",
       "      <td>464.367920</td>\n",
       "      <td>2694188</td>\n",
       "      <td>HCLTECH.NS</td>\n",
       "      <td>4</td>\n",
       "      <td>-0.483418</td>\n",
       "      <td>52.252916</td>\n",
       "      <td>-0.413171</td>\n",
       "      <td>8.812808</td>\n",
       "      <td>0.014010</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            date       open         high          low        close   volume  \\\n",
       "2931  2021-11-18  1148.0000  1155.000000  1113.500000  1008.085876  5384530   \n",
       "1938  2017-11-09   439.7500   444.225006   436.225006   371.745605  1600452   \n",
       "260   2011-01-14   115.7500   120.000000   115.587502    87.932831  3733116   \n",
       "633   2012-07-18   119.5625   120.812500   117.862503    90.921898  4858332   \n",
       "2404  2019-10-04   538.0000   540.900024   532.325012   464.367920  2694188   \n",
       "\n",
       "             tic  day       macd     rsi_30      cci_30      dx_30  turbulence  \n",
       "2931  HCLTECH.NS    3 -16.992700  43.402816 -114.700846  39.403983    3.391223  \n",
       "1938  HCLTECH.NS    3  -2.991294  49.801561  -23.629432   3.422008    0.084178  \n",
       "260   HCLTECH.NS    4   1.987710  60.264347   95.773211  11.588899    0.358184  \n",
       "633   HCLTECH.NS    2  -0.128618  47.898062  -38.658945  18.732847    0.059885  \n",
       "2404  HCLTECH.NS    4  -0.483418  52.252916   -0.413171   8.812808    0.014010  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-QsYaY0Dh1iw"
   },
   "source": [
    "<a id='4'></a>\n",
    "# Part 5. Design Environment\n",
    "Considering the stochastic and interactive nature of the automated stock trading tasks, a financial task is modeled as a **Markov Decision Process (MDP)** problem. The training process involves observing stock price change, taking an action and reward's calculation to have the agent adjusting its strategy accordingly. By interacting with the environment, the trading agent will derive a trading strategy with the maximized rewards as time proceeds.\n",
    "\n",
    "Our trading environments, based on OpenAI Gym framework, simulate live stock markets with real market data according to the principle of time-driven simulation.\n",
    "\n",
    "The action space describes the allowed actions that the agent interacts with the environment. Normally, action a includes three actions: {-1, 0, 1}, where -1, 0, 1 represent selling, holding, and buying one share. Also, an action can be carried upon multiple shares. We use an action space {-k,…,-1, 0, 1, …, k}, where k denotes the number of shares to buy and -k denotes the number of shares to sell. For example, \"Buy 10 shares of AAPL\" or \"Sell 10 shares of AAPL\" are 10 or -10, respectively. The continuous action space needs to be normalized to [-1, 1], since the policy is defined on a Gaussian distribution, which needs to be normalized and symmetric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q2zqII8rMIqn",
    "outputId": "e16902dc-86b3-488e-ec15-234a3d6039c2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stock Dimension: 1, State Space: 7\n"
     ]
    }
   ],
   "source": [
    "stock_dimension = len(processed.tic.unique())\n",
    "state_space = 1 + 2*stock_dimension + len(INDICATORS)*stock_dimension\n",
    "print(f\"Stock Dimension: {stock_dimension}, State Space: {state_space}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "AWyp84Ltto19"
   },
   "outputs": [],
   "source": [
    "env_kwargs = {\n",
    "    \"hmax\": 100, \n",
    "    \"initial_amount\": 100000, \n",
    "    \"buy_cost_pct\": 0.001, \n",
    "    \"sell_cost_pct\": 0.001, \n",
    "    \"state_space\": state_space, \n",
    "    \"stock_dim\": stock_dimension, \n",
    "    \"tech_indicator_list\": INDICATORS,\n",
    "    \"action_space\": stock_dimension, \n",
    "    \"reward_scaling\": 1e-4,\n",
    "    \"print_verbosity\":5\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HMNR5nHjh1iz"
   },
   "source": [
    "<a id='5'></a>\n",
    "# Part 6: Implement DRL Algorithms\n",
    "* The implementation of the DRL algorithms are based on **OpenAI Baselines** and **Stable Baselines**. Stable Baselines is a fork of OpenAI Baselines, with a major structural refactoring, and code cleanups.\n",
    "* FinRL library includes fine-tuned standard DRL algorithms, such as DQN, DDPG,\n",
    "Multi-Agent DDPG, PPO, SAC, A2C and TD3. We also allow users to\n",
    "design their own DRL algorithms by adapting these DRL algorithms.\n",
    "\n",
    "* In this notebook, we are training and validating 3 agents (A2C, PPO, DDPG) using Rolling-window Ensemble Method ([reference code](https://github.com/AI4Finance-LLC/Deep-Reinforcement-Learning-for-Automated-Stock-Trading-Ensemble-Strategy-ICAIF-2020/blob/80415db8fa7b2179df6bd7e81ce4fe8dbf913806/model/models.py#L92))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "v-gthCxMtj1d"
   },
   "outputs": [],
   "source": [
    "rebalance_window = 63 # rebalance_window is the number of days to retrain the model\n",
    "validation_window = 63 # validation_window is the number of days to do validation and trading (e.g. if validation_window=63, then both validation and trading period will be 63 days)\n",
    "\n",
    "ensemble_agent = DRLEnsembleAgent(df=processed,\n",
    "                 train_period=(TRAIN_START_DATE,TRAIN_END_DATE),\n",
    "                 val_test_period=(TEST_START_DATE,TEST_END_DATE),\n",
    "                 rebalance_window=rebalance_window, \n",
    "                 validation_window=validation_window, \n",
    "                 **env_kwargs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "KsfEHa_Etj1d",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "A2C_model_kwargs = {\n",
    "                    'n_steps': 5,\n",
    "                    'ent_coef': 0.005,\n",
    "                    'learning_rate': 0.0007\n",
    "                    }\n",
    "\n",
    "PPO_model_kwargs = {\n",
    "                    \"ent_coef\":0.01,\n",
    "                    \"n_steps\": 2048,\n",
    "                    \"learning_rate\": 0.00025,\n",
    "                    \"batch_size\": 128\n",
    "                    }\n",
    "\n",
    "DDPG_model_kwargs = {\n",
    "                      \"action_noise\":\"ornstein_uhlenbeck\",\n",
    "                      \"buffer_size\": 10_000,\n",
    "                      \"learning_rate\": 0.0005,\n",
    "                      \"batch_size\": 64\n",
    "                    }\n",
    "\n",
    "timesteps_dict = {'a2c' : 12_000, \n",
    "                 'ppo' : 12_000, \n",
    "                 'ddpg' : 12_000\n",
    "                 }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_1lyCECstj1e",
    "outputId": "73e2d3f8-463a-42d5-d49f-c71385a26c92",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============Start Ensemble Strategy============\n",
      "============================================\n",
      "turbulence_threshold:  10.227501153831431\n",
      "======Model training from:  2010-01-01 to  2021-10-04\n",
      "======A2C Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/a2c\\a2c_126_2\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 328         |\n",
      "|    iterations         | 100         |\n",
      "|    time_elapsed       | 1           |\n",
      "|    total_timesteps    | 500         |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -1.42       |\n",
      "|    explained_variance | 1.19e-07    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 99          |\n",
      "|    policy_loss        | -0.0351     |\n",
      "|    reward             | -0.07035189 |\n",
      "|    std                | 0.999       |\n",
      "|    value_loss         | 0.00155     |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 329        |\n",
      "|    iterations         | 200        |\n",
      "|    time_elapsed       | 3          |\n",
      "|    total_timesteps    | 1000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.41      |\n",
      "|    explained_variance | -9.45      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 199        |\n",
      "|    policy_loss        | -1.22      |\n",
      "|    reward             | 0.22651808 |\n",
      "|    std                | 0.995      |\n",
      "|    value_loss         | 0.323      |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 341       |\n",
      "|    iterations         | 300       |\n",
      "|    time_elapsed       | 4         |\n",
      "|    total_timesteps    | 1500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.42     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 299       |\n",
      "|    policy_loss        | -0.0435   |\n",
      "|    reward             | 0.6394854 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 0.0171    |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 353         |\n",
      "|    iterations         | 400         |\n",
      "|    time_elapsed       | 5           |\n",
      "|    total_timesteps    | 2000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -1.42       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 399         |\n",
      "|    policy_loss        | -0.597      |\n",
      "|    reward             | -0.41524377 |\n",
      "|    std                | 1           |\n",
      "|    value_loss         | 0.241       |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 358        |\n",
      "|    iterations         | 500        |\n",
      "|    time_elapsed       | 6          |\n",
      "|    total_timesteps    | 2500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.44      |\n",
      "|    explained_variance | 0.238      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 499        |\n",
      "|    policy_loss        | 1.31       |\n",
      "|    reward             | -1.0165094 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 0.973      |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 365         |\n",
      "|    iterations         | 600         |\n",
      "|    time_elapsed       | 8           |\n",
      "|    total_timesteps    | 3000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -1.44       |\n",
      "|    explained_variance | 1.19e-07    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 599         |\n",
      "|    policy_loss        | -0.196      |\n",
      "|    reward             | 0.120107345 |\n",
      "|    std                | 1.02        |\n",
      "|    value_loss         | 0.175       |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 369         |\n",
      "|    iterations         | 700         |\n",
      "|    time_elapsed       | 9           |\n",
      "|    total_timesteps    | 3500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -1.44       |\n",
      "|    explained_variance | -2.26       |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 699         |\n",
      "|    policy_loss        | -0.128      |\n",
      "|    reward             | -0.49420372 |\n",
      "|    std                | 1.02        |\n",
      "|    value_loss         | 0.026       |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 369         |\n",
      "|    iterations         | 800         |\n",
      "|    time_elapsed       | 10          |\n",
      "|    total_timesteps    | 4000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -1.43       |\n",
      "|    explained_variance | 0.178       |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 799         |\n",
      "|    policy_loss        | -2.69       |\n",
      "|    reward             | 0.026457721 |\n",
      "|    std                | 1.01        |\n",
      "|    value_loss         | 2.19        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 366       |\n",
      "|    iterations         | 900       |\n",
      "|    time_elapsed       | 12        |\n",
      "|    total_timesteps    | 4500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.44     |\n",
      "|    explained_variance | -0.083    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 899       |\n",
      "|    policy_loss        | 1.54      |\n",
      "|    reward             | -1.232749 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 0.837     |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 367      |\n",
      "|    iterations         | 1000     |\n",
      "|    time_elapsed       | 13       |\n",
      "|    total_timesteps    | 5000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.45    |\n",
      "|    explained_variance | 0.0334   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 999      |\n",
      "|    policy_loss        | 0.986    |\n",
      "|    reward             | -0.68273 |\n",
      "|    std                | 1.03     |\n",
      "|    value_loss         | 0.556    |\n",
      "------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 370         |\n",
      "|    iterations         | 1100        |\n",
      "|    time_elapsed       | 14          |\n",
      "|    total_timesteps    | 5500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -1.45       |\n",
      "|    explained_variance | -0.0528     |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1099        |\n",
      "|    policy_loss        | 4.56        |\n",
      "|    reward             | -0.27270207 |\n",
      "|    std                | 1.03        |\n",
      "|    value_loss         | 13          |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 374        |\n",
      "|    iterations         | 1200       |\n",
      "|    time_elapsed       | 16         |\n",
      "|    total_timesteps    | 6000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.46      |\n",
      "|    explained_variance | 0.164      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1199       |\n",
      "|    policy_loss        | -0.309     |\n",
      "|    reward             | -0.1877494 |\n",
      "|    std                | 1.04       |\n",
      "|    value_loss         | 0.0427     |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 377         |\n",
      "|    iterations         | 1300        |\n",
      "|    time_elapsed       | 17          |\n",
      "|    total_timesteps    | 6500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -1.47       |\n",
      "|    explained_variance | -0.0726     |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1299        |\n",
      "|    policy_loss        | 0.513       |\n",
      "|    reward             | 0.020384785 |\n",
      "|    std                | 1.05        |\n",
      "|    value_loss         | 0.224       |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 379       |\n",
      "|    iterations         | 1400      |\n",
      "|    time_elapsed       | 18        |\n",
      "|    total_timesteps    | 7000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.47     |\n",
      "|    explained_variance | -0.101    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1399      |\n",
      "|    policy_loss        | 2.19      |\n",
      "|    reward             | 1.0941752 |\n",
      "|    std                | 1.06      |\n",
      "|    value_loss         | 3.24      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 382       |\n",
      "|    iterations         | 1500      |\n",
      "|    time_elapsed       | 19        |\n",
      "|    total_timesteps    | 7500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.49     |\n",
      "|    explained_variance | -0.229    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1499      |\n",
      "|    policy_loss        | -1.24     |\n",
      "|    reward             | 1.9705551 |\n",
      "|    std                | 1.07      |\n",
      "|    value_loss         | 1.26      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 383        |\n",
      "|    iterations         | 1600       |\n",
      "|    time_elapsed       | 20         |\n",
      "|    total_timesteps    | 8000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.49      |\n",
      "|    explained_variance | 0.288      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1599       |\n",
      "|    policy_loss        | -0.291     |\n",
      "|    reward             | 0.18895404 |\n",
      "|    std                | 1.08       |\n",
      "|    value_loss         | 1.45       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 385         |\n",
      "|    iterations         | 1700        |\n",
      "|    time_elapsed       | 22          |\n",
      "|    total_timesteps    | 8500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -1.49       |\n",
      "|    explained_variance | 0.012       |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1699        |\n",
      "|    policy_loss        | 2.01        |\n",
      "|    reward             | -0.53735393 |\n",
      "|    std                | 1.08        |\n",
      "|    value_loss         | 5.54        |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 385         |\n",
      "|    iterations         | 1800        |\n",
      "|    time_elapsed       | 23          |\n",
      "|    total_timesteps    | 9000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -1.48       |\n",
      "|    explained_variance | -0.0213     |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1799        |\n",
      "|    policy_loss        | 0.358       |\n",
      "|    reward             | -0.24328022 |\n",
      "|    std                | 1.07        |\n",
      "|    value_loss         | 0.0776      |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 386         |\n",
      "|    iterations         | 1900        |\n",
      "|    time_elapsed       | 24          |\n",
      "|    total_timesteps    | 9500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -1.49       |\n",
      "|    explained_variance | -0.00477    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1899        |\n",
      "|    policy_loss        | 0.27        |\n",
      "|    reward             | -0.12616055 |\n",
      "|    std                | 1.07        |\n",
      "|    value_loss         | 0.0654      |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 387        |\n",
      "|    iterations         | 2000       |\n",
      "|    time_elapsed       | 25         |\n",
      "|    total_timesteps    | 10000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.49      |\n",
      "|    explained_variance | -0.0262    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1999       |\n",
      "|    policy_loss        | -3.23      |\n",
      "|    reward             | 0.15953857 |\n",
      "|    std                | 1.08       |\n",
      "|    value_loss         | 4.7        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 388        |\n",
      "|    iterations         | 2100       |\n",
      "|    time_elapsed       | 27         |\n",
      "|    total_timesteps    | 10500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.48      |\n",
      "|    explained_variance | 0.0759     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 2099       |\n",
      "|    policy_loss        | -3.19      |\n",
      "|    reward             | -0.1195591 |\n",
      "|    std                | 1.07       |\n",
      "|    value_loss         | 2.72       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 389        |\n",
      "|    iterations         | 2200       |\n",
      "|    time_elapsed       | 28         |\n",
      "|    total_timesteps    | 11000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.49      |\n",
      "|    explained_variance | -0.0131    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 2199       |\n",
      "|    policy_loss        | 4          |\n",
      "|    reward             | -0.9219699 |\n",
      "|    std                | 1.08       |\n",
      "|    value_loss         | 3.27       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 390        |\n",
      "|    iterations         | 2300       |\n",
      "|    time_elapsed       | 29         |\n",
      "|    total_timesteps    | 11500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.49      |\n",
      "|    explained_variance | 0.407      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 2299       |\n",
      "|    policy_loss        | -1.49      |\n",
      "|    reward             | -0.7306294 |\n",
      "|    std                | 1.07       |\n",
      "|    value_loss         | 1.43       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 390        |\n",
      "|    iterations         | 2400       |\n",
      "|    time_elapsed       | 30         |\n",
      "|    total_timesteps    | 12000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.48      |\n",
      "|    explained_variance | -0.142     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 2399       |\n",
      "|    policy_loss        | -0.829     |\n",
      "|    reward             | -0.6695764 |\n",
      "|    std                | 1.07       |\n",
      "|    value_loss         | 0.407      |\n",
      "--------------------------------------\n",
      "======A2C Validation from:  2021-10-04 to  2022-01-04\n",
      "A2C Sharpe Ratio:  0.09083122785580747\n",
      "======PPO Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ppo\\ppo_126_2\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    fps             | 673       |\n",
      "|    iterations      | 1         |\n",
      "|    time_elapsed    | 3         |\n",
      "|    total_timesteps | 2048      |\n",
      "| train/             |           |\n",
      "|    reward          | 0.5916805 |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 600         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005326027 |\n",
      "|    clip_fraction        | 0.0362      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.42       |\n",
      "|    explained_variance   | -0.46       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 0.0298      |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.00254    |\n",
      "|    reward               | 0.260366    |\n",
      "|    std                  | 0.998       |\n",
      "|    value_loss           | 0.126       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 574         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005796569 |\n",
      "|    clip_fraction        | 0.0373      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.41       |\n",
      "|    explained_variance   | 0.0289      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 0.386       |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.00429    |\n",
      "|    reward               | 0.06414701  |\n",
      "|    std                  | 0.988       |\n",
      "|    value_loss           | 0.747       |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 562          |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 14           |\n",
      "|    total_timesteps      | 8192         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0043104934 |\n",
      "|    clip_fraction        | 0.013        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.41        |\n",
      "|    explained_variance   | 0.00216      |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 1.33         |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | -0.00139     |\n",
      "|    reward               | -0.62166643  |\n",
      "|    std                  | 0.989        |\n",
      "|    value_loss           | 2.34         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 556          |\n",
      "|    iterations           | 5            |\n",
      "|    time_elapsed         | 18           |\n",
      "|    total_timesteps      | 10240        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0041832626 |\n",
      "|    clip_fraction        | 0.0222       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.41        |\n",
      "|    explained_variance   | 0.0302       |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 0.756        |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | -0.0021      |\n",
      "|    reward               | -0.23560359  |\n",
      "|    std                  | 0.983        |\n",
      "|    value_loss           | 1.55         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 544          |\n",
      "|    iterations           | 6            |\n",
      "|    time_elapsed         | 22           |\n",
      "|    total_timesteps      | 12288        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0043019494 |\n",
      "|    clip_fraction        | 0.0124       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.4         |\n",
      "|    explained_variance   | -0.00499     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 1.91         |\n",
      "|    n_updates            | 50           |\n",
      "|    policy_gradient_loss | -0.00183     |\n",
      "|    reward               | 0.05277513   |\n",
      "|    std                  | 0.984        |\n",
      "|    value_loss           | 3.8          |\n",
      "------------------------------------------\n",
      "======PPO Validation from:  2021-10-04 to  2022-01-04\n",
      "PPO Sharpe Ratio:  0.011839284510815597\n",
      "======DDPG Training========\n",
      "{'action_noise': OrnsteinUhlenbeckActionNoise(mu=[0.], sigma=[0.1]), 'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ddpg\\ddpg_126_2\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 4         |\n",
      "|    fps             | 59        |\n",
      "|    time_elapsed    | 193       |\n",
      "|    total_timesteps | 11600     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.39e+03 |\n",
      "|    critic_loss     | 6.41      |\n",
      "|    learning_rate   | 0.0005    |\n",
      "|    n_updates       | 11499     |\n",
      "|    reward          | 0.0       |\n",
      "----------------------------------\n",
      "======DDPG Validation from:  2021-10-04 to  2022-01-04\n",
      "======Best Model Retraining from:  2010-01-01 to  2022-01-04\n",
      "======Trading from:  2022-01-04 to  2022-04-06\n",
      "============================================\n",
      "turbulence_threshold:  10.227501153831431\n",
      "======Model training from:  2010-01-01 to  2022-01-04\n",
      "======A2C Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/a2c\\a2c_189_2\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 360      |\n",
      "|    iterations         | 100      |\n",
      "|    time_elapsed       | 1        |\n",
      "|    total_timesteps    | 500      |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.42    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 99       |\n",
      "|    policy_loss        | 0.00336  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1        |\n",
      "|    value_loss         | 1.15e-05 |\n",
      "------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 344         |\n",
      "|    iterations         | 200         |\n",
      "|    time_elapsed       | 2           |\n",
      "|    total_timesteps    | 1000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -1.44       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 199         |\n",
      "|    policy_loss        | -0.0725     |\n",
      "|    reward             | 0.029262297 |\n",
      "|    std                | 1.02        |\n",
      "|    value_loss         | 0.00573     |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 334       |\n",
      "|    iterations         | 300       |\n",
      "|    time_elapsed       | 4         |\n",
      "|    total_timesteps    | 1500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.43     |\n",
      "|    explained_variance | -5.57     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 299       |\n",
      "|    policy_loss        | -0.108    |\n",
      "|    reward             | 0.2510073 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 0.0237    |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 342         |\n",
      "|    iterations         | 400         |\n",
      "|    time_elapsed       | 5           |\n",
      "|    total_timesteps    | 2000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -1.43       |\n",
      "|    explained_variance | -1.17       |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 399         |\n",
      "|    policy_loss        | -0.31       |\n",
      "|    reward             | -0.26479965 |\n",
      "|    std                | 1.01        |\n",
      "|    value_loss         | 0.119       |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 341         |\n",
      "|    iterations         | 500         |\n",
      "|    time_elapsed       | 7           |\n",
      "|    total_timesteps    | 2500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -1.42       |\n",
      "|    explained_variance | 0.372       |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 499         |\n",
      "|    policy_loss        | 0.505       |\n",
      "|    reward             | -0.44439107 |\n",
      "|    std                | 0.999       |\n",
      "|    value_loss         | 0.154       |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 343         |\n",
      "|    iterations         | 600         |\n",
      "|    time_elapsed       | 8           |\n",
      "|    total_timesteps    | 3000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -1.42       |\n",
      "|    explained_variance | 0.286       |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 599         |\n",
      "|    policy_loss        | 0.067       |\n",
      "|    reward             | 0.060861923 |\n",
      "|    std                | 1           |\n",
      "|    value_loss         | 0.0196      |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 345        |\n",
      "|    iterations         | 700        |\n",
      "|    time_elapsed       | 10         |\n",
      "|    total_timesteps    | 3500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.43      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 699        |\n",
      "|    policy_loss        | 0.354      |\n",
      "|    reward             | 0.07271697 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 0.147      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 346        |\n",
      "|    iterations         | 800        |\n",
      "|    time_elapsed       | 11         |\n",
      "|    total_timesteps    | 4000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.43      |\n",
      "|    explained_variance | 0.012      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 799        |\n",
      "|    policy_loss        | -1.68      |\n",
      "|    reward             | -1.2998093 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 2.45       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 341         |\n",
      "|    iterations         | 900         |\n",
      "|    time_elapsed       | 13          |\n",
      "|    total_timesteps    | 4500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -1.43       |\n",
      "|    explained_variance | 0.0939      |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 899         |\n",
      "|    policy_loss        | -0.674      |\n",
      "|    reward             | -0.40903935 |\n",
      "|    std                | 1.01        |\n",
      "|    value_loss         | 0.204       |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 342        |\n",
      "|    iterations         | 1000       |\n",
      "|    time_elapsed       | 14         |\n",
      "|    total_timesteps    | 5000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.44      |\n",
      "|    explained_variance | 0.408      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 999        |\n",
      "|    policy_loss        | 1.54       |\n",
      "|    reward             | 0.16979444 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 1.55       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 344        |\n",
      "|    iterations         | 1100       |\n",
      "|    time_elapsed       | 15         |\n",
      "|    total_timesteps    | 5500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.44      |\n",
      "|    explained_variance | 5.36e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1099       |\n",
      "|    policy_loss        | 4.22       |\n",
      "|    reward             | 0.82648206 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 19.4       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 343        |\n",
      "|    iterations         | 1200       |\n",
      "|    time_elapsed       | 17         |\n",
      "|    total_timesteps    | 6000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.43      |\n",
      "|    explained_variance | 0.00278    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1199       |\n",
      "|    policy_loss        | -0.0249    |\n",
      "|    reward             | 0.14472125 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 0.0169     |\n",
      "--------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 344          |\n",
      "|    iterations         | 1300         |\n",
      "|    time_elapsed       | 18           |\n",
      "|    total_timesteps    | 6500         |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -1.44        |\n",
      "|    explained_variance | 0.712        |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 1299         |\n",
      "|    policy_loss        | -0.0454      |\n",
      "|    reward             | -0.033450164 |\n",
      "|    std                | 1.02         |\n",
      "|    value_loss         | 0.0105       |\n",
      "----------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 345        |\n",
      "|    iterations         | 1400       |\n",
      "|    time_elapsed       | 20         |\n",
      "|    total_timesteps    | 7000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.43      |\n",
      "|    explained_variance | 0.32       |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1399       |\n",
      "|    policy_loss        | -0.565     |\n",
      "|    reward             | -1.7643776 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 0.401      |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 346         |\n",
      "|    iterations         | 1500        |\n",
      "|    time_elapsed       | 21          |\n",
      "|    total_timesteps    | 7500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -1.43       |\n",
      "|    explained_variance | -1.19e-07   |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1499        |\n",
      "|    policy_loss        | -0.0704     |\n",
      "|    reward             | -0.18567236 |\n",
      "|    std                | 1.01        |\n",
      "|    value_loss         | 0.1         |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 346        |\n",
      "|    iterations         | 1600       |\n",
      "|    time_elapsed       | 23         |\n",
      "|    total_timesteps    | 8000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.43      |\n",
      "|    explained_variance | 2.98e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1599       |\n",
      "|    policy_loss        | -0.307     |\n",
      "|    reward             | 0.65951294 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 0.885      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 348        |\n",
      "|    iterations         | 1700       |\n",
      "|    time_elapsed       | 24         |\n",
      "|    total_timesteps    | 8500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.44      |\n",
      "|    explained_variance | -0.000156  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1699       |\n",
      "|    policy_loss        | -0.157     |\n",
      "|    reward             | 0.24631262 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 0.993      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 349        |\n",
      "|    iterations         | 1800       |\n",
      "|    time_elapsed       | 25         |\n",
      "|    total_timesteps    | 9000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.43      |\n",
      "|    explained_variance | 2.71e-05   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1799       |\n",
      "|    policy_loss        | -0.44      |\n",
      "|    reward             | 0.10847856 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 0.152      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 351        |\n",
      "|    iterations         | 1900       |\n",
      "|    time_elapsed       | 27         |\n",
      "|    total_timesteps    | 9500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.43      |\n",
      "|    explained_variance | -3.29      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1899       |\n",
      "|    policy_loss        | -0.458     |\n",
      "|    reward             | -0.4473667 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 0.229      |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 352       |\n",
      "|    iterations         | 2000      |\n",
      "|    time_elapsed       | 28        |\n",
      "|    total_timesteps    | 10000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.45     |\n",
      "|    explained_variance | 0.182     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1999      |\n",
      "|    policy_loss        | 0.0735    |\n",
      "|    reward             | 1.3529105 |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 0.106     |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 353       |\n",
      "|    iterations         | 2100      |\n",
      "|    time_elapsed       | 29        |\n",
      "|    total_timesteps    | 10500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.45     |\n",
      "|    explained_variance | 0.188     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2099      |\n",
      "|    policy_loss        | -1.53     |\n",
      "|    reward             | 0.4941045 |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 1.72      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 355         |\n",
      "|    iterations         | 2200        |\n",
      "|    time_elapsed       | 30          |\n",
      "|    total_timesteps    | 11000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -1.46       |\n",
      "|    explained_variance | 0.207       |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 2199        |\n",
      "|    policy_loss        | 2.67        |\n",
      "|    reward             | -0.48531812 |\n",
      "|    std                | 1.04        |\n",
      "|    value_loss         | 7.16        |\n",
      "---------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 355      |\n",
      "|    iterations         | 2300     |\n",
      "|    time_elapsed       | 32       |\n",
      "|    total_timesteps    | 11500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.44    |\n",
      "|    explained_variance | -0.0322  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2299     |\n",
      "|    policy_loss        | 0.719    |\n",
      "|    reward             | 2.090338 |\n",
      "|    std                | 1.02     |\n",
      "|    value_loss         | 7.45     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 356       |\n",
      "|    iterations         | 2400      |\n",
      "|    time_elapsed       | 33        |\n",
      "|    total_timesteps    | 12000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.44     |\n",
      "|    explained_variance | 0.000285  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2399      |\n",
      "|    policy_loss        | 0.0483    |\n",
      "|    reward             | 0.2874687 |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 0.00669   |\n",
      "-------------------------------------\n",
      "======A2C Validation from:  2022-01-04 to  2022-04-06\n",
      "A2C Sharpe Ratio:  -0.10993778963953098\n",
      "======PPO Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ppo\\ppo_189_2\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 690  |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 2    |\n",
      "|    total_timesteps | 2048 |\n",
      "| train/             |      |\n",
      "|    reward          | 0.0  |\n",
      "-----------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 599         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005800943 |\n",
      "|    clip_fraction        | 0.0297      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.42       |\n",
      "|    explained_variance   | -0.39       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 0.072       |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.00402    |\n",
      "|    reward               | -0.34703386 |\n",
      "|    std                  | 0.999       |\n",
      "|    value_loss           | 0.137       |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 554          |\n",
      "|    iterations           | 3            |\n",
      "|    time_elapsed         | 11           |\n",
      "|    total_timesteps      | 6144         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0056717442 |\n",
      "|    clip_fraction        | 0.0269       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | -0.0361      |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 0.2          |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | -0.00345     |\n",
      "|    reward               | 0.051140174  |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 0.519        |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 530          |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 8192         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0038977815 |\n",
      "|    clip_fraction        | 0.03         |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | -0.00234     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 1.98         |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | -0.00285     |\n",
      "|    reward               | -1.1250067   |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 5.72         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 522          |\n",
      "|    iterations           | 5            |\n",
      "|    time_elapsed         | 19           |\n",
      "|    total_timesteps      | 10240        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0016731344 |\n",
      "|    clip_fraction        | 0.00205      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | -0.0139      |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 0.806        |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | -0.0012      |\n",
      "|    reward               | 0.558754     |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 1.77         |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 524           |\n",
      "|    iterations           | 6             |\n",
      "|    time_elapsed         | 23            |\n",
      "|    total_timesteps      | 12288         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00047088202 |\n",
      "|    clip_fraction        | 0.00195       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.43         |\n",
      "|    explained_variance   | 0.0317        |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 3.18          |\n",
      "|    n_updates            | 50            |\n",
      "|    policy_gradient_loss | -0.000754     |\n",
      "|    reward               | 0.0853179     |\n",
      "|    std                  | 1.01          |\n",
      "|    value_loss           | 6.01          |\n",
      "-------------------------------------------\n",
      "======PPO Validation from:  2022-01-04 to  2022-04-06\n",
      "PPO Sharpe Ratio:  -0.2563596320809846\n",
      "======DDPG Training========\n",
      "{'action_noise': OrnsteinUhlenbeckActionNoise(mu=[0.], sigma=[0.1]), 'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ddpg\\ddpg_189_2\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 4        |\n",
      "|    fps             | 53       |\n",
      "|    time_elapsed    | 223      |\n",
      "|    total_timesteps | 11852    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.9e+03 |\n",
      "|    critic_loss     | 237      |\n",
      "|    learning_rate   | 0.0005   |\n",
      "|    n_updates       | 11751    |\n",
      "|    reward          | 0.0      |\n",
      "---------------------------------\n",
      "======DDPG Validation from:  2022-01-04 to  2022-04-06\n",
      "======Best Model Retraining from:  2010-01-01 to  2022-04-06\n",
      "======Trading from:  2022-04-06 to  2022-07-07\n",
      "============================================\n",
      "turbulence_threshold:  10.227501153831431\n",
      "======Model training from:  2010-01-01 to  2022-04-06\n",
      "======A2C Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/a2c\\a2c_252_2\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 309           |\n",
      "|    iterations         | 100           |\n",
      "|    time_elapsed       | 1             |\n",
      "|    total_timesteps    | 500           |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -1.44         |\n",
      "|    explained_variance | 0             |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 99            |\n",
      "|    policy_loss        | 0.00654       |\n",
      "|    reward             | -0.0014153904 |\n",
      "|    std                | 1.02          |\n",
      "|    value_loss         | 4.4e-05       |\n",
      "-----------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 332      |\n",
      "|    iterations         | 200      |\n",
      "|    time_elapsed       | 3        |\n",
      "|    total_timesteps    | 1000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.44    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 199      |\n",
      "|    policy_loss        | -0.00449 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.02     |\n",
      "|    value_loss         | 1.29e-05 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 333      |\n",
      "|    iterations         | 300      |\n",
      "|    time_elapsed       | 4        |\n",
      "|    total_timesteps    | 1500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.47    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 299      |\n",
      "|    policy_loss        | -0.00345 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.05     |\n",
      "|    value_loss         | 1.2e-05  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 342      |\n",
      "|    iterations         | 400      |\n",
      "|    time_elapsed       | 5        |\n",
      "|    total_timesteps    | 2000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.49    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 399      |\n",
      "|    policy_loss        | 0.00198  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.07     |\n",
      "|    value_loss         | 3.46e-06 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 351      |\n",
      "|    iterations         | 500      |\n",
      "|    time_elapsed       | 7        |\n",
      "|    total_timesteps    | 2500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.5     |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 499      |\n",
      "|    policy_loss        | 2.77e-06 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.09     |\n",
      "|    value_loss         | 4.6e-12  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 354      |\n",
      "|    iterations         | 600      |\n",
      "|    time_elapsed       | 8        |\n",
      "|    total_timesteps    | 3000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.53    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 599      |\n",
      "|    policy_loss        | -0.0883  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.12     |\n",
      "|    value_loss         | 0.00216  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 355      |\n",
      "|    iterations         | 700      |\n",
      "|    time_elapsed       | 9        |\n",
      "|    total_timesteps    | 3500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.54    |\n",
      "|    explained_variance | 5.96e-08 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 699      |\n",
      "|    policy_loss        | -0.00226 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.13     |\n",
      "|    value_loss         | 3.73e-06 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 359       |\n",
      "|    iterations         | 800       |\n",
      "|    time_elapsed       | 11        |\n",
      "|    total_timesteps    | 4000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.58     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 799       |\n",
      "|    policy_loss        | -0.000935 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 1.17      |\n",
      "|    value_loss         | 5.87e-07  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 362       |\n",
      "|    iterations         | 900       |\n",
      "|    time_elapsed       | 12        |\n",
      "|    total_timesteps    | 4500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.6      |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 899       |\n",
      "|    policy_loss        | -0.0175   |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 1.2       |\n",
      "|    value_loss         | 0.000141  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 364      |\n",
      "|    iterations         | 1000     |\n",
      "|    time_elapsed       | 13       |\n",
      "|    total_timesteps    | 5000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.62    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 999      |\n",
      "|    policy_loss        | 0.0332   |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.22     |\n",
      "|    value_loss         | 0.000559 |\n",
      "------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 362          |\n",
      "|    iterations         | 1100         |\n",
      "|    time_elapsed       | 15           |\n",
      "|    total_timesteps    | 5500         |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -1.63        |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 1099         |\n",
      "|    policy_loss        | 0.00546      |\n",
      "|    reward             | -0.001601417 |\n",
      "|    std                | 1.24         |\n",
      "|    value_loss         | 2.36e-05     |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 360          |\n",
      "|    iterations         | 1200         |\n",
      "|    time_elapsed       | 16           |\n",
      "|    total_timesteps    | 6000         |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -1.64        |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 1199         |\n",
      "|    policy_loss        | -0.188       |\n",
      "|    reward             | -0.009110682 |\n",
      "|    std                | 1.24         |\n",
      "|    value_loss         | 0.0208       |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 361          |\n",
      "|    iterations         | 1300         |\n",
      "|    time_elapsed       | 17           |\n",
      "|    total_timesteps    | 6500         |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -1.65        |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 1299         |\n",
      "|    policy_loss        | 0.0384       |\n",
      "|    reward             | -0.119777486 |\n",
      "|    std                | 1.26         |\n",
      "|    value_loss         | 0.00102      |\n",
      "----------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 360       |\n",
      "|    iterations         | 1400      |\n",
      "|    time_elapsed       | 19        |\n",
      "|    total_timesteps    | 7000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.66     |\n",
      "|    explained_variance | -0.207    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1399      |\n",
      "|    policy_loss        | 0.169     |\n",
      "|    reward             | 0.0651705 |\n",
      "|    std                | 1.27      |\n",
      "|    value_loss         | 0.217     |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 362         |\n",
      "|    iterations         | 1500        |\n",
      "|    time_elapsed       | 20          |\n",
      "|    total_timesteps    | 7500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -1.67       |\n",
      "|    explained_variance | -0.163      |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1499        |\n",
      "|    policy_loss        | 0.11        |\n",
      "|    reward             | -0.12706219 |\n",
      "|    std                | 1.28        |\n",
      "|    value_loss         | 0.0095      |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 363        |\n",
      "|    iterations         | 1600       |\n",
      "|    time_elapsed       | 22         |\n",
      "|    total_timesteps    | 8000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.68      |\n",
      "|    explained_variance | 0.437      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1599       |\n",
      "|    policy_loss        | -0.342     |\n",
      "|    reward             | 0.30390936 |\n",
      "|    std                | 1.3        |\n",
      "|    value_loss         | 0.114      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 364        |\n",
      "|    iterations         | 1700       |\n",
      "|    time_elapsed       | 23         |\n",
      "|    total_timesteps    | 8500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.69      |\n",
      "|    explained_variance | 0.0907     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1699       |\n",
      "|    policy_loss        | 0.0768     |\n",
      "|    reward             | -0.3793786 |\n",
      "|    std                | 1.31       |\n",
      "|    value_loss         | 0.0394     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 361        |\n",
      "|    iterations         | 1800       |\n",
      "|    time_elapsed       | 24         |\n",
      "|    total_timesteps    | 9000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.68      |\n",
      "|    explained_variance | 0.0188     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1799       |\n",
      "|    policy_loss        | -0.245     |\n",
      "|    reward             | 0.13512598 |\n",
      "|    std                | 1.3        |\n",
      "|    value_loss         | 0.536      |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 354         |\n",
      "|    iterations         | 1900        |\n",
      "|    time_elapsed       | 26          |\n",
      "|    total_timesteps    | 9500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -1.68       |\n",
      "|    explained_variance | 0.366       |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1899        |\n",
      "|    policy_loss        | 0.149       |\n",
      "|    reward             | -0.14092639 |\n",
      "|    std                | 1.29        |\n",
      "|    value_loss         | 0.126       |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 354       |\n",
      "|    iterations         | 2000      |\n",
      "|    time_elapsed       | 28        |\n",
      "|    total_timesteps    | 10000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.68     |\n",
      "|    explained_variance | -0.271    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1999      |\n",
      "|    policy_loss        | -1.41     |\n",
      "|    reward             | 1.0505128 |\n",
      "|    std                | 1.3       |\n",
      "|    value_loss         | 0.667     |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 355         |\n",
      "|    iterations         | 2100        |\n",
      "|    time_elapsed       | 29          |\n",
      "|    total_timesteps    | 10500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -1.68       |\n",
      "|    explained_variance | 0.00281     |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 2099        |\n",
      "|    policy_loss        | 0.554       |\n",
      "|    reward             | -0.35802937 |\n",
      "|    std                | 1.29        |\n",
      "|    value_loss         | 0.539       |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 355         |\n",
      "|    iterations         | 2200        |\n",
      "|    time_elapsed       | 30          |\n",
      "|    total_timesteps    | 11000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -1.67       |\n",
      "|    explained_variance | -0.184      |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 2199        |\n",
      "|    policy_loss        | 0.35        |\n",
      "|    reward             | 0.017921697 |\n",
      "|    std                | 1.29        |\n",
      "|    value_loss         | 0.42        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 356        |\n",
      "|    iterations         | 2300       |\n",
      "|    time_elapsed       | 32         |\n",
      "|    total_timesteps    | 11500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.68      |\n",
      "|    explained_variance | -0.0319    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 2299       |\n",
      "|    policy_loss        | 4.06       |\n",
      "|    reward             | -0.6534288 |\n",
      "|    std                | 1.29       |\n",
      "|    value_loss         | 6.67       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 356       |\n",
      "|    iterations         | 2400      |\n",
      "|    time_elapsed       | 33        |\n",
      "|    total_timesteps    | 12000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.68     |\n",
      "|    explained_variance | 0.0117    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2399      |\n",
      "|    policy_loss        | -6.01     |\n",
      "|    reward             | 0.8513342 |\n",
      "|    std                | 1.3       |\n",
      "|    value_loss         | 20.3      |\n",
      "-------------------------------------\n",
      "======A2C Validation from:  2022-04-06 to  2022-07-07\n",
      "A2C Sharpe Ratio:  -0.4577780982529859\n",
      "======PPO Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ppo\\ppo_252_2\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    fps             | 603       |\n",
      "|    iterations      | 1         |\n",
      "|    time_elapsed    | 3         |\n",
      "|    total_timesteps | 2048      |\n",
      "| train/             |           |\n",
      "|    reward          | 0.7350958 |\n",
      "----------------------------------\n",
      "day: 3025, episode: 5\n",
      "begin_total_asset: 100000.00\n",
      "end_total_asset: 382623.45\n",
      "total_reward: 282623.45\n",
      "total_cost: 57065.63\n",
      "total_trades: 2910\n",
      "Sharpe: 0.747\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 468          |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 8            |\n",
      "|    total_timesteps      | 4096         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0044136997 |\n",
      "|    clip_fraction        | 0.0335       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | -0.238       |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 0.0523       |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.0026      |\n",
      "|    reward               | -0.1575346   |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 0.165        |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 444         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006227303 |\n",
      "|    clip_fraction        | 0.0546      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.42       |\n",
      "|    explained_variance   | 0.00108     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 0.423       |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.00481    |\n",
      "|    reward               | -0.63506085 |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 0.715       |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 449          |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 18           |\n",
      "|    total_timesteps      | 8192         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0040900633 |\n",
      "|    clip_fraction        | 0.00659      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | -0.0181      |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 4.38         |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | -0.00135     |\n",
      "|    reward               | 1.6024191    |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 8.77         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 452          |\n",
      "|    iterations           | 5            |\n",
      "|    time_elapsed         | 22           |\n",
      "|    total_timesteps      | 10240        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0032364714 |\n",
      "|    clip_fraction        | 0.00356      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.43        |\n",
      "|    explained_variance   | -0.0216      |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 0.733        |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | -0.00124     |\n",
      "|    reward               | 0.062282007  |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 1.78         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 443          |\n",
      "|    iterations           | 6            |\n",
      "|    time_elapsed         | 27           |\n",
      "|    total_timesteps      | 12288        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0042151352 |\n",
      "|    clip_fraction        | 0.00806      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.43        |\n",
      "|    explained_variance   | -0.00491     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 3.7          |\n",
      "|    n_updates            | 50           |\n",
      "|    policy_gradient_loss | -0.00128     |\n",
      "|    reward               | -0.06917961  |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 6.99         |\n",
      "------------------------------------------\n",
      "======PPO Validation from:  2022-04-06 to  2022-07-07\n",
      "PPO Sharpe Ratio:  -0.40211003544176205\n",
      "======DDPG Training========\n",
      "{'action_noise': OrnsteinUhlenbeckActionNoise(mu=[0.], sigma=[0.1]), 'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ddpg\\ddpg_252_2\n",
      "day: 3025, episode: 10\n",
      "begin_total_asset: 100000.00\n",
      "end_total_asset: 99876.75\n",
      "total_reward: -123.25\n",
      "total_cost: 325.35\n",
      "total_trades: 92\n",
      "Sharpe: -0.001\n",
      "=================================\n",
      "======DDPG Validation from:  2022-04-06 to  2022-07-07\n",
      "======Best Model Retraining from:  2010-01-01 to  2022-07-07\n",
      "======Trading from:  2022-07-07 to  2022-10-10\n",
      "============================================\n",
      "turbulence_threshold:  10.227501153831431\n",
      "======Model training from:  2010-01-01 to  2022-07-07\n",
      "======A2C Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/a2c\\a2c_315_2\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 272          |\n",
      "|    iterations         | 100          |\n",
      "|    time_elapsed       | 1            |\n",
      "|    total_timesteps    | 500          |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -1.45        |\n",
      "|    explained_variance | -26.2        |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 99           |\n",
      "|    policy_loss        | 0.163        |\n",
      "|    reward             | -0.090628415 |\n",
      "|    std                | 1.03         |\n",
      "|    value_loss         | 0.0492       |\n",
      "----------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 281        |\n",
      "|    iterations         | 200        |\n",
      "|    time_elapsed       | 3          |\n",
      "|    total_timesteps    | 1000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.44      |\n",
      "|    explained_variance | -0.682     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 199        |\n",
      "|    policy_loss        | -0.353     |\n",
      "|    reward             | 0.19352572 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 0.0722     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 287        |\n",
      "|    iterations         | 300        |\n",
      "|    time_elapsed       | 5          |\n",
      "|    total_timesteps    | 1500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.44      |\n",
      "|    explained_variance | -3.53      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 299        |\n",
      "|    policy_loss        | -0.245     |\n",
      "|    reward             | 0.61630654 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 0.0972     |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 282         |\n",
      "|    iterations         | 400         |\n",
      "|    time_elapsed       | 7           |\n",
      "|    total_timesteps    | 2000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -1.45       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 399         |\n",
      "|    policy_loss        | -0.826      |\n",
      "|    reward             | -0.23372263 |\n",
      "|    std                | 1.03        |\n",
      "|    value_loss         | 0.277       |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 287        |\n",
      "|    iterations         | 500        |\n",
      "|    time_elapsed       | 8          |\n",
      "|    total_timesteps    | 2500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.45      |\n",
      "|    explained_variance | -0.522     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 499        |\n",
      "|    policy_loss        | 1.28       |\n",
      "|    reward             | -1.0301331 |\n",
      "|    std                | 1.04       |\n",
      "|    value_loss         | 0.633      |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 290      |\n",
      "|    iterations         | 600      |\n",
      "|    time_elapsed       | 10       |\n",
      "|    total_timesteps    | 3000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.44    |\n",
      "|    explained_variance | -0.2     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 599      |\n",
      "|    policy_loss        | 0.462    |\n",
      "|    reward             | 2.15536  |\n",
      "|    std                | 1.02     |\n",
      "|    value_loss         | 0.453    |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 290        |\n",
      "|    iterations         | 700        |\n",
      "|    time_elapsed       | 12         |\n",
      "|    total_timesteps    | 3500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.45      |\n",
      "|    explained_variance | -0.788     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 699        |\n",
      "|    policy_loss        | -0.216     |\n",
      "|    reward             | 0.26537648 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 0.0615     |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 291      |\n",
      "|    iterations         | 800      |\n",
      "|    time_elapsed       | 13       |\n",
      "|    total_timesteps    | 4000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.45    |\n",
      "|    explained_variance | -0.0682  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 799      |\n",
      "|    policy_loss        | 2.93     |\n",
      "|    reward             | 0.427717 |\n",
      "|    std                | 1.04     |\n",
      "|    value_loss         | 2.33     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 291       |\n",
      "|    iterations         | 900       |\n",
      "|    time_elapsed       | 15        |\n",
      "|    total_timesteps    | 4500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.45     |\n",
      "|    explained_variance | -0.0918   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 899       |\n",
      "|    policy_loss        | -0.295    |\n",
      "|    reward             | 0.5261025 |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 0.614     |\n",
      "-------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 290          |\n",
      "|    iterations         | 1000         |\n",
      "|    time_elapsed       | 17           |\n",
      "|    total_timesteps    | 5000         |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -1.47        |\n",
      "|    explained_variance | -0.106       |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 999          |\n",
      "|    policy_loss        | -0.577       |\n",
      "|    reward             | -0.025097346 |\n",
      "|    std                | 1.05         |\n",
      "|    value_loss         | 0.449        |\n",
      "----------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 289       |\n",
      "|    iterations         | 1100      |\n",
      "|    time_elapsed       | 19        |\n",
      "|    total_timesteps    | 5500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.46     |\n",
      "|    explained_variance | -0.203    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1099      |\n",
      "|    policy_loss        | -1.55     |\n",
      "|    reward             | 0.9452671 |\n",
      "|    std                | 1.04      |\n",
      "|    value_loss         | 2.06      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 290         |\n",
      "|    iterations         | 1200        |\n",
      "|    time_elapsed       | 20          |\n",
      "|    total_timesteps    | 6000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -1.45       |\n",
      "|    explained_variance | 0.0137      |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1199        |\n",
      "|    policy_loss        | -7.39       |\n",
      "|    reward             | 0.086607486 |\n",
      "|    std                | 1.03        |\n",
      "|    value_loss         | 33.8        |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 290         |\n",
      "|    iterations         | 1300        |\n",
      "|    time_elapsed       | 22          |\n",
      "|    total_timesteps    | 6500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -1.45       |\n",
      "|    explained_variance | -0.267      |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1299        |\n",
      "|    policy_loss        | 0.312       |\n",
      "|    reward             | -0.44585696 |\n",
      "|    std                | 1.03        |\n",
      "|    value_loss         | 0.0921      |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 290        |\n",
      "|    iterations         | 1400       |\n",
      "|    time_elapsed       | 24         |\n",
      "|    total_timesteps    | 7000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.46      |\n",
      "|    explained_variance | 0.0703     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1399       |\n",
      "|    policy_loss        | -1.1       |\n",
      "|    reward             | 0.43208283 |\n",
      "|    std                | 1.04       |\n",
      "|    value_loss         | 0.987      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 290        |\n",
      "|    iterations         | 1500       |\n",
      "|    time_elapsed       | 25         |\n",
      "|    total_timesteps    | 7500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.46      |\n",
      "|    explained_variance | -0.112     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1499       |\n",
      "|    policy_loss        | 1.48       |\n",
      "|    reward             | -0.5823774 |\n",
      "|    std                | 1.04       |\n",
      "|    value_loss         | 1.55       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 291         |\n",
      "|    iterations         | 1600        |\n",
      "|    time_elapsed       | 27          |\n",
      "|    total_timesteps    | 8000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -1.46       |\n",
      "|    explained_variance | 0.035       |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1599        |\n",
      "|    policy_loss        | 0.437       |\n",
      "|    reward             | -0.20464556 |\n",
      "|    std                | 1.04        |\n",
      "|    value_loss         | 0.162       |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 291       |\n",
      "|    iterations         | 1700      |\n",
      "|    time_elapsed       | 29        |\n",
      "|    total_timesteps    | 8500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.47     |\n",
      "|    explained_variance | -0.123    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1699      |\n",
      "|    policy_loss        | 0.506     |\n",
      "|    reward             | -1.439414 |\n",
      "|    std                | 1.05      |\n",
      "|    value_loss         | 0.116     |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 290       |\n",
      "|    iterations         | 1800      |\n",
      "|    time_elapsed       | 30        |\n",
      "|    total_timesteps    | 9000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.48     |\n",
      "|    explained_variance | -0.0557   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1799      |\n",
      "|    policy_loss        | -1.34     |\n",
      "|    reward             | 1.5422618 |\n",
      "|    std                | 1.06      |\n",
      "|    value_loss         | 0.819     |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 287        |\n",
      "|    iterations         | 1900       |\n",
      "|    time_elapsed       | 33         |\n",
      "|    total_timesteps    | 9500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.48      |\n",
      "|    explained_variance | -0.0879    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1899       |\n",
      "|    policy_loss        | 0.158      |\n",
      "|    reward             | 0.24970861 |\n",
      "|    std                | 1.06       |\n",
      "|    value_loss         | 0.0344     |\n",
      "--------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 283          |\n",
      "|    iterations         | 2000         |\n",
      "|    time_elapsed       | 35           |\n",
      "|    total_timesteps    | 10000        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -1.48        |\n",
      "|    explained_variance | -0.109       |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 1999         |\n",
      "|    policy_loss        | -0.579       |\n",
      "|    reward             | -0.006258583 |\n",
      "|    std                | 1.06         |\n",
      "|    value_loss         | 0.452        |\n",
      "----------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 282        |\n",
      "|    iterations         | 2100       |\n",
      "|    time_elapsed       | 37         |\n",
      "|    total_timesteps    | 10500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.47      |\n",
      "|    explained_variance | -0.0438    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 2099       |\n",
      "|    policy_loss        | -0.901     |\n",
      "|    reward             | 0.19240561 |\n",
      "|    std                | 1.05       |\n",
      "|    value_loss         | 1.01       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 282        |\n",
      "|    iterations         | 2200       |\n",
      "|    time_elapsed       | 38         |\n",
      "|    total_timesteps    | 11000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.47      |\n",
      "|    explained_variance | 0.0406     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 2199       |\n",
      "|    policy_loss        | 1.58       |\n",
      "|    reward             | -0.2559084 |\n",
      "|    std                | 1.05       |\n",
      "|    value_loss         | 2.61       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 283       |\n",
      "|    iterations         | 2300      |\n",
      "|    time_elapsed       | 40        |\n",
      "|    total_timesteps    | 11500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.48     |\n",
      "|    explained_variance | 0.431     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2299      |\n",
      "|    policy_loss        | -0.0543   |\n",
      "|    reward             | 0.6218724 |\n",
      "|    std                | 1.06      |\n",
      "|    value_loss         | 0.0311    |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 283        |\n",
      "|    iterations         | 2400       |\n",
      "|    time_elapsed       | 42         |\n",
      "|    total_timesteps    | 12000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.48      |\n",
      "|    explained_variance | -0.00596   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 2399       |\n",
      "|    policy_loss        | 0.272      |\n",
      "|    reward             | -1.9061893 |\n",
      "|    std                | 1.06       |\n",
      "|    value_loss         | 3.17       |\n",
      "--------------------------------------\n",
      "======A2C Validation from:  2022-07-07 to  2022-10-10\n",
      "A2C Sharpe Ratio:  -0.19848597421589517\n",
      "======PPO Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ppo\\ppo_315_2\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    fps             | 538       |\n",
      "|    iterations      | 1         |\n",
      "|    time_elapsed    | 3         |\n",
      "|    total_timesteps | 2048      |\n",
      "| train/             |           |\n",
      "|    reward          | 0.3067124 |\n",
      "----------------------------------\n",
      "day: 3088, episode: 5\n",
      "begin_total_asset: 100000.00\n",
      "end_total_asset: 318763.99\n",
      "total_reward: 218763.99\n",
      "total_cost: 62180.10\n",
      "total_trades: 2991\n",
      "Sharpe: 0.596\n",
      "=================================\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 464           |\n",
      "|    iterations           | 2             |\n",
      "|    time_elapsed         | 8             |\n",
      "|    total_timesteps      | 4096          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.0017388163  |\n",
      "|    clip_fraction        | 0.0165        |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.42         |\n",
      "|    explained_variance   | -0.0113       |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 0.146         |\n",
      "|    n_updates            | 10            |\n",
      "|    policy_gradient_loss | -0.00146      |\n",
      "|    reward               | -0.0009651431 |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 0.325         |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 441           |\n",
      "|    iterations           | 3             |\n",
      "|    time_elapsed         | 13            |\n",
      "|    total_timesteps      | 6144          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00071798643 |\n",
      "|    clip_fraction        | 0.00239       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.43         |\n",
      "|    explained_variance   | 0.011         |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 0.349         |\n",
      "|    n_updates            | 20            |\n",
      "|    policy_gradient_loss | -0.00117      |\n",
      "|    reward               | 0.5362225     |\n",
      "|    std                  | 1.01          |\n",
      "|    value_loss           | 0.638         |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 434          |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 18           |\n",
      "|    total_timesteps      | 8192         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0063097235 |\n",
      "|    clip_fraction        | 0.0317       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.43        |\n",
      "|    explained_variance   | -0.0588      |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 0.321        |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | -0.00326     |\n",
      "|    reward               | -0.5379737   |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 0.497        |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 430          |\n",
      "|    iterations           | 5            |\n",
      "|    time_elapsed         | 23           |\n",
      "|    total_timesteps      | 10240        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0061623575 |\n",
      "|    clip_fraction        | 0.0166       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | -0.0142      |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 0.669        |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | -0.00112     |\n",
      "|    reward               | 0.38826132   |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 1.43         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 429          |\n",
      "|    iterations           | 6            |\n",
      "|    time_elapsed         | 28           |\n",
      "|    total_timesteps      | 12288        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0033798462 |\n",
      "|    clip_fraction        | 0.00786      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | -0.00298     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 5.48         |\n",
      "|    n_updates            | 50           |\n",
      "|    policy_gradient_loss | -0.00173     |\n",
      "|    reward               | -0.1475706   |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 10.7         |\n",
      "------------------------------------------\n",
      "======PPO Validation from:  2022-07-07 to  2022-10-10\n",
      "PPO Sharpe Ratio:  0.32841815003609814\n",
      "======DDPG Training========\n",
      "{'action_noise': OrnsteinUhlenbeckActionNoise(mu=[0.], sigma=[0.1]), 'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ddpg\\ddpg_315_2\n",
      "day: 3088, episode: 10\n",
      "begin_total_asset: 100000.00\n",
      "end_total_asset: 100000.00\n",
      "total_reward: 0.00\n",
      "total_cost: 0.00\n",
      "total_trades: 0\n",
      "=================================\n"
     ]
    }
   ],
   "source": [
    "df_summary = ensemble_agent.run_ensemble_strategy(A2C_model_kwargs,\n",
    "                                                 PPO_model_kwargs,\n",
    "                                                 DDPG_model_kwargs,\n",
    "                                                 timesteps_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 175
    },
    "id": "-0qd8acMtj1f",
    "outputId": "9f0cbf89-5f4b-4691-9e43-daa093ebceae"
   },
   "outputs": [],
   "source": [
    "df_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W6vvNSC6h1jZ"
   },
   "source": [
    "<a id='6'></a>\n",
    "# Part 7: Backtest Our Strategy\n",
    "Backtesting plays a key role in evaluating the performance of a trading strategy. Automated backtesting tool is preferred because it reduces the human error. We usually use the Quantopian pyfolio package to backtest our trading strategies. It is easy to use and consists of various individual plots that provide a comprehensive image of the performance of a trading strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X4JKB--8tj1g"
   },
   "outputs": [],
   "source": [
    "unique_trade_date = processed[(processed.date > TEST_START_DATE)&(processed.date <= TEST_END_DATE)].date.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q9mKF7GGtj1g",
    "outputId": "99c5e5f8-2e3f-49c3-e5a6-4e66ed92e40a",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_trade_date = pd.DataFrame({'datadate':unique_trade_date})\n",
    "\n",
    "df_account_value=pd.DataFrame()\n",
    "for i in range(rebalance_window+validation_window, len(unique_trade_date)+1,rebalance_window):\n",
    "    temp = pd.read_csv('results/account_value_trade_{}_{}.csv'.format('ensemble',i))\n",
    "    df_account_value = pd.concat([df_account_value,temp],ignore_index=True)\n",
    "sharpe=(252**0.5)*df_account_value.account_value.pct_change(1).mean()/df_account_value.account_value.pct_change(1).std()\n",
    "print('Sharpe Ratio: ',sharpe)\n",
    "df_account_value=df_account_value.join(df_trade_date[validation_window:].reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "oyosyW7_tj1g",
    "outputId": "0e54f2d5-6057-4a14-c94a-5f2af26ad171"
   },
   "outputs": [],
   "source": [
    "df_account_value.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 293
    },
    "id": "wLsRdw2Ctj1h",
    "outputId": "0e2b0bc2-840c-47fd-87d4-01201d8e4e3d"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "df_account_value.account_value.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lr2zX7ZxNyFQ"
   },
   "source": [
    "<a id='6.1'></a>\n",
    "## 7.1 BackTestStats\n",
    "pass in df_account_value, this information is stored in env class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Nzkr9yv-AdV_",
    "outputId": "ab0971b8-10b0-4fb1-a151-71a1de89cdf2",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"==============Get Backtest Results===========\")\n",
    "now = datetime.datetime.now().strftime('%Y%m%d-%Hh%M')\n",
    "\n",
    "perf_stats_all = backtest_stats(account_value=df_account_value)\n",
    "perf_stats_all = pd.DataFrame(perf_stats_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DiHhM1YkoCel",
    "outputId": "c233f613-67a3-4882-8710-c1839247590e"
   },
   "outputs": [],
   "source": [
    "#baseline stats\n",
    "print(\"==============Get Baseline Stats===========\")\n",
    "df_sensex_ = get_baseline(\n",
    "        ticker=\"^BSESN\", \n",
    "        start = df_account_value.loc[0,'date'],\n",
    "        end = df_account_value.loc[len(df_account_value)-1,'date'])\n",
    "\n",
    "stats = backtest_stats(df_sensex_, value_col_name = 'close')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RhJ9whD75WTs",
    "outputId": "8ae25787-8400-4357-ecc0-af7538689cee"
   },
   "outputs": [],
   "source": [
    "df_sensex = pd.DataFrame()\n",
    "df_sensex['date'] = df_account_value['date']\n",
    "df_sensex['SENSEX'] = df_sensex_['close'] / df_sensex_['close'][0] * env_kwargs[\"initial_amount\"]\n",
    "print(\"df_sensex: \", df_sensex)\n",
    "df_sensex.to_csv(\"df_sensex.csv\")\n",
    "df_sensex = df_sensex.set_index(df_sensex.columns[0])\n",
    "print(\"df_sensex: \", df_sensex)\n",
    "df_sensex.to_csv(\"df_sensex+.csv\")\n",
    "\n",
    "df_account_value.to_csv('df_account_value.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9U6Suru3h1jc"
   },
   "source": [
    "<a id='6.2'></a>\n",
    "## 7.2 BackTestPlot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "HggausPRoCem",
    "outputId": "615e8d79-f3d7-47e9-c886-3cd18e4535f2"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming df_account_value, df_trade_date, and df_sensex are defined dataframes\n",
    "\n",
    "# Creating a dataframe for ensemble results\n",
    "df_result_ensemble = pd.DataFrame({'date': df_account_value['date'], 'ensemble': df_account_value['account_value']})\n",
    "df_result_ensemble.set_index('date', inplace=True)\n",
    "\n",
    "# Saving ensemble results to a CSV file\n",
    "df_result_ensemble.to_csv(\"df_result_ensemble.csv\")\n",
    "\n",
    "# Merging ensemble results with SENSEX data\n",
    "result = pd.merge(df_result_ensemble, df_sensex, left_index=True, right_index=True)\n",
    "\n",
    "# Plotting the comparison between ensemble and SENSEX\n",
    "plt.figure(figsize=(15, 5))\n",
    "result.plot()\n",
    "plt.title('Ensemble vs. SENSEX Performance')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Account Value')\n",
    "plt.legend(['Ensemble', 'SENSEX'])\n",
    "plt.show()\n",
    "\n",
    "# Saving the merged result to a CSV file\n",
    "result.to_csv(\"result.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oBQx4bVQFi-a"
   },
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "FinRL_Ensemble_StockTrading_ICAIF_2020.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

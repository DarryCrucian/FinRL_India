{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QMjwq6pS-kFz"
   },
   "source": [
    "# Stock NeurIPS2018 Part 2. Train\n",
    "This series is a reproduction of *the process in the paper Practical Deep Reinforcement Learning Approach for Stock Trading*. \n",
    "\n",
    "This is the second part of the NeurIPS2018 series, introducing how to use FinRL to make data into the gym form environment, and train DRL agents on it.\n",
    "\n",
    "Other demos can be found at the repo of [FinRL-Tutorials]((https://github.com/AI4Finance-Foundation/FinRL-Tutorials))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gT-zXutMgqOS"
   },
   "source": [
    "# Part 1. Install Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "D0vEcPxSJ8hI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting git+https://github.com/AI4Finance-Foundation/FinRL.git\n",
      "  Cloning https://github.com/AI4Finance-Foundation/FinRL.git to c:\\users\\aryam\\appdata\\local\\temp\\pip-req-build-qv3ndv2y\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Running command git clone --filter=blob:none --quiet https://github.com/AI4Finance-Foundation/FinRL.git 'C:\\Users\\aryam\\AppData\\Local\\Temp\\pip-req-build-qv3ndv2y'\n",
      "  error: RPC failed; curl 56 Recv failure: Connection was reset\n",
      "  error: 6777 bytes of body are still expected\n",
      "  fetch-pack: unexpected disconnect while reading sideband packet\n",
      "  fatal: early EOF\n",
      "  fatal: fetch-pack: invalid index-pack output\n",
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  × git clone --filter=blob:none --quiet https://github.com/AI4Finance-Foundation/FinRL.git 'C:\\Users\\aryam\\AppData\\Local\\Temp\\pip-req-build-qv3ndv2y' did not run successfully.\n",
      "  │ exit code: 128\n",
      "  ╰─> See above for output.\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "error: subprocess-exited-with-error\n",
      "\n",
      "× git clone --filter=blob:none --quiet https://github.com/AI4Finance-Foundation/FinRL.git 'C:\\Users\\aryam\\AppData\\Local\\Temp\\pip-req-build-qv3ndv2y' did not run successfully.\n",
      "│ exit code: 128\n",
      "╰─> See above for output.\n",
      "\n",
      "note: This error originates from a subprocess, and is likely not a problem with pip.\n"
     ]
    }
   ],
   "source": [
    "## install finrl library\n",
    "!pip install git+https://github.com/AI4Finance-Foundation/FinRL.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "xt1317y2ixSS"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aryam\\AppData\\Local\\Temp\\ipykernel_44836\\3164752271.py:1: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was too old on your system - pyarrow 10.0.1 is the current minimum supported version as of this release.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from stable_baselines3.common.logger import configure\n",
    "\n",
    "from finrl.agents.stablebaselines3.models import DRLAgent\n",
    "from finrl.config import INDICATORS, TRAINED_MODEL_DIR, RESULTS_DIR\n",
    "from finrl.main import check_and_make_directories\n",
    "from finrl.meta.env_stock_trading.env_stocktrading import StockTradingEnv\n",
    "\n",
    "check_and_make_directories([TRAINED_MODEL_DIR])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aWrSrQv3i0Ng"
   },
   "source": [
    "# Part 2. Build A Market Environment in OpenAI Gym-style"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wiHhM2U-XBMZ"
   },
   "source": [
    "![rl_diagram_transparent_bg.png](data:image/png;base64,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)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LeneTRdyZDvy"
   },
   "source": [
    "The core element in reinforcement learning are **agent** and **environment**. You can understand RL as the following process: \n",
    "\n",
    "The agent is active in a world, which is the environment. It observe its current condition as a **state**, and is allowed to do certain **actions**. After the agent execute an action, it will arrive at a new state. At the same time, the environment will have feedback to the agent called **reward**, a numerical signal that tells how good or bad the new state is. As the figure above, agent and environment will keep doing this interaction.\n",
    "\n",
    "The goal of agent is to get as much cumulative reward as possible. Reinforcement learning is the method that agent learns to improve its behavior and achieve that goal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w3H88JXkI93v"
   },
   "source": [
    "To achieve this in Python, we follow the OpenAI gym style to build the stock data into environment.\n",
    "\n",
    "state-action-reward are specified as follows:\n",
    "\n",
    "* **State s**: The state space represents an agent's perception of the market environment. Just like a human trader analyzing various information, here our agent passively observes the price data and technical indicators based on the past data. It will learn by interacting with the market environment (usually by replaying historical data).\n",
    "\n",
    "* **Action a**: The action space includes allowed actions that an agent can take at each state. For example, a ∈ {−1, 0, 1}, where −1, 0, 1 represent\n",
    "selling, holding, and buying. When an action operates multiple shares, a ∈{−k, ..., −1, 0, 1, ..., k}, e.g.. \"Buy 10 shares of AAPL\" or \"Sell 10 shares of AAPL\" are 10 or −10, respectively\n",
    "\n",
    "* **Reward function r(s, a, s′)**: Reward is an incentive for an agent to learn a better policy. For example, it can be the change of the portfolio value when taking a at state s and arriving at new state s',  i.e., r(s, a, s′) = v′ − v, where v′ and v represent the portfolio values at state s′ and s, respectively\n",
    "\n",
    "\n",
    "**Market environment**: 30 constituent stocks of Dow Jones Industrial Average (DJIA) index. Accessed at the starting date of the testing period."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SKyZejI0fmp1"
   },
   "source": [
    "## Read data\n",
    "\n",
    "We first read the .csv file of our training data into dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "mFCP1YEhi6oi"
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('train_data.csv')\n",
    "\n",
    "# If you are not using the data generated from part 1 of this tutorial, make sure \n",
    "# it has the columns and index in the form that could be make into the environment. \n",
    "# Then you can comment and skip the following two lines.\n",
    "train = train.set_index(train.columns[0])\n",
    "train.index.names = ['']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yw95ZMicgEyi"
   },
   "source": [
    "## Construct the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5WZ6-9q2gq9S"
   },
   "source": [
    "Calculate and specify the parameters we need for constructing the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7T3DZPoaIm8k",
    "outputId": "4817e063-400a-416e-f8f2-4b1c4d9c8408"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stock Dimension: 29, State Space: 291\n"
     ]
    }
   ],
   "source": [
    "stock_dimension = len(train.tic.unique())\n",
    "state_space = 1 + 2*stock_dimension + len(INDICATORS)*stock_dimension\n",
    "print(f\"Stock Dimension: {stock_dimension}, State Space: {state_space}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "WsOLoeNcJF8Q"
   },
   "outputs": [],
   "source": [
    "buy_cost_list = sell_cost_list = [0.001] * stock_dimension\n",
    "num_stock_shares = [0] * stock_dimension\n",
    "\n",
    "env_kwargs = {\n",
    "    \"hmax\": 100,\n",
    "    \"initial_amount\": 100000,\n",
    "    \"num_stock_shares\": num_stock_shares,\n",
    "    \"buy_cost_pct\": buy_cost_list,\n",
    "    \"sell_cost_pct\": sell_cost_list,\n",
    "    \"state_space\": state_space,\n",
    "    \"stock_dim\": stock_dimension,\n",
    "    \"tech_indicator_list\": INDICATORS,\n",
    "    \"action_space\": stock_dimension,\n",
    "    \"reward_scaling\": 1e-4\n",
    "}\n",
    "\n",
    "\n",
    "e_train_gym = StockTradingEnv(df = train, **env_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7We-q73jjaFQ"
   },
   "source": [
    "## Environment for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aS-SHiGRJK-4",
    "outputId": "a733ecdf-d857-40f5-b399-4325c7ead299"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv'>\n"
     ]
    }
   ],
   "source": [
    "env_train, _ = e_train_gym.get_sb_env()\n",
    "print(type(env_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HMNR5nHjh1iz"
   },
   "source": [
    "# Part 3: Train DRL Agents\n",
    "* Here, the DRL algorithms are from **[Stable Baselines 3](https://stable-baselines3.readthedocs.io/en/master/)**. It's a library that implemented popular DRL algorithms using pytorch, succeeding to its old version: Stable Baselines.\n",
    "* Users are also encouraged to try **[ElegantRL](https://github.com/AI4Finance-Foundation/ElegantRL)** and **[Ray RLlib](https://github.com/ray-project/ray)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "364PsqckttcQ"
   },
   "outputs": [],
   "source": [
    "agent = DRLAgent(env = env_train)\n",
    "\n",
    "# Set the corresponding values to 'True' for the algorithms that you want to use\n",
    "if_using_a2c = True\n",
    "if_using_ddpg = True\n",
    "if_using_ppo = True\n",
    "if_using_td3 = True\n",
    "if_using_sac = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YDmqOyF9h1iz"
   },
   "source": [
    "## Agent Training: 5 algorithms (A2C, DDPG, PPO, TD3, SAC)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uijiWgkuh1jB"
   },
   "source": [
    "### Agent 1: A2C\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GUCnkn-HIbmj",
    "outputId": "2794a094-a916-448c-ead1-6e20184dde2a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_steps': 5, 'ent_coef': 0.01, 'learning_rate': 0.0007}\n",
      "Using cpu device\n",
      "Logging to results/a2c\n"
     ]
    }
   ],
   "source": [
    "agent = DRLAgent(env = env_train)\n",
    "model_a2c = agent.get_model(\"a2c\")\n",
    "\n",
    "if if_using_a2c:\n",
    "  # set up logger\n",
    "  tmp_path = RESULTS_DIR + '/a2c'\n",
    "  new_logger_a2c = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
    "  # Set new logger\n",
    "  model_a2c.set_logger(new_logger_a2c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0GVpkWGqH4-D",
    "outputId": "f29cf145-e3b5-4e59-f64d-5921462a8f81"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 74          |\n",
      "|    iterations         | 100         |\n",
      "|    time_elapsed       | 6           |\n",
      "|    total_timesteps    | 500         |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -41.7       |\n",
      "|    explained_variance | -1.19e-07   |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 99          |\n",
      "|    policy_loss        | -4.33       |\n",
      "|    reward             | 0.042606108 |\n",
      "|    std                | 1.02        |\n",
      "|    value_loss         | 0.0223      |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 73         |\n",
      "|    iterations         | 200        |\n",
      "|    time_elapsed       | 13         |\n",
      "|    total_timesteps    | 1000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.9      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 199        |\n",
      "|    policy_loss        | -1.29      |\n",
      "|    reward             | -0.7247324 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 0.273      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 75         |\n",
      "|    iterations         | 300        |\n",
      "|    time_elapsed       | 19         |\n",
      "|    total_timesteps    | 1500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.9      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 299        |\n",
      "|    policy_loss        | 126        |\n",
      "|    reward             | -1.7186828 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 9.39       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 76         |\n",
      "|    iterations         | 400        |\n",
      "|    time_elapsed       | 26         |\n",
      "|    total_timesteps    | 2000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42        |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 399        |\n",
      "|    policy_loss        | -21.4      |\n",
      "|    reward             | -0.9224631 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 0.884      |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 73          |\n",
      "|    iterations         | 500         |\n",
      "|    time_elapsed       | 33          |\n",
      "|    total_timesteps    | 2500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -42         |\n",
      "|    explained_variance | 0.144       |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 499         |\n",
      "|    policy_loss        | -7.73       |\n",
      "|    reward             | 0.014059997 |\n",
      "|    std                | 1.03        |\n",
      "|    value_loss         | 0.061       |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 74          |\n",
      "|    iterations         | 600         |\n",
      "|    time_elapsed       | 40          |\n",
      "|    total_timesteps    | 3000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -42.2       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 599         |\n",
      "|    policy_loss        | -15.8       |\n",
      "|    reward             | 0.022396013 |\n",
      "|    std                | 1.04        |\n",
      "|    value_loss         | 0.165       |\n",
      "---------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 75           |\n",
      "|    iterations         | 700          |\n",
      "|    time_elapsed       | 46           |\n",
      "|    total_timesteps    | 3500         |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -42.3        |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 699          |\n",
      "|    policy_loss        | -13.6        |\n",
      "|    reward             | -0.092436716 |\n",
      "|    std                | 1.04         |\n",
      "|    value_loss         | 0.198        |\n",
      "----------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 77         |\n",
      "|    iterations         | 800        |\n",
      "|    time_elapsed       | 51         |\n",
      "|    total_timesteps    | 4000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.4      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 799        |\n",
      "|    policy_loss        | 9.09       |\n",
      "|    reward             | 0.27302313 |\n",
      "|    std                | 1.04       |\n",
      "|    value_loss         | 0.277      |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 78          |\n",
      "|    iterations         | 900         |\n",
      "|    time_elapsed       | 57          |\n",
      "|    total_timesteps    | 4500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -42.5       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 899         |\n",
      "|    policy_loss        | -63.3       |\n",
      "|    reward             | -0.49102175 |\n",
      "|    std                | 1.05        |\n",
      "|    value_loss         | 6.75        |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 79          |\n",
      "|    iterations         | 1000        |\n",
      "|    time_elapsed       | 63          |\n",
      "|    total_timesteps    | 5000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -42.6       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 999         |\n",
      "|    policy_loss        | -9.92       |\n",
      "|    reward             | -0.05952912 |\n",
      "|    std                | 1.05        |\n",
      "|    value_loss         | 0.073       |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 79        |\n",
      "|    iterations         | 1100      |\n",
      "|    time_elapsed       | 68        |\n",
      "|    total_timesteps    | 5500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.8     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1099      |\n",
      "|    policy_loss        | 50.5      |\n",
      "|    reward             | 0.4511977 |\n",
      "|    std                | 1.06      |\n",
      "|    value_loss         | 1.73      |\n",
      "-------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 80           |\n",
      "|    iterations         | 1200         |\n",
      "|    time_elapsed       | 74           |\n",
      "|    total_timesteps    | 6000         |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -42.9        |\n",
      "|    explained_variance | -1.19e-07    |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 1199         |\n",
      "|    policy_loss        | -12.8        |\n",
      "|    reward             | 0.0032727348 |\n",
      "|    std                | 1.06         |\n",
      "|    value_loss         | 0.109        |\n",
      "----------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 80         |\n",
      "|    iterations         | 1300       |\n",
      "|    time_elapsed       | 80         |\n",
      "|    total_timesteps    | 6500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.9      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1299       |\n",
      "|    policy_loss        | -20.5      |\n",
      "|    reward             | 0.36187842 |\n",
      "|    std                | 1.06       |\n",
      "|    value_loss         | 0.222      |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 80          |\n",
      "|    iterations         | 1400        |\n",
      "|    time_elapsed       | 86          |\n",
      "|    total_timesteps    | 7000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -43         |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1399        |\n",
      "|    policy_loss        | 8.13        |\n",
      "|    reward             | 0.015851298 |\n",
      "|    std                | 1.07        |\n",
      "|    value_loss         | 0.0368      |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 81          |\n",
      "|    iterations         | 1500        |\n",
      "|    time_elapsed       | 92          |\n",
      "|    total_timesteps    | 7500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -43.3       |\n",
      "|    explained_variance | 1.19e-07    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1499        |\n",
      "|    policy_loss        | 2.05        |\n",
      "|    reward             | 0.110878095 |\n",
      "|    std                | 1.08        |\n",
      "|    value_loss         | 0.0187      |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 81          |\n",
      "|    iterations         | 1600        |\n",
      "|    time_elapsed       | 98          |\n",
      "|    total_timesteps    | 8000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -43.4       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1599        |\n",
      "|    policy_loss        | -8.97       |\n",
      "|    reward             | -0.55290765 |\n",
      "|    std                | 1.08        |\n",
      "|    value_loss         | 0.125       |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 81          |\n",
      "|    iterations         | 1700        |\n",
      "|    time_elapsed       | 104         |\n",
      "|    total_timesteps    | 8500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -43.5       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1699        |\n",
      "|    policy_loss        | -24.8       |\n",
      "|    reward             | -0.19981991 |\n",
      "|    std                | 1.08        |\n",
      "|    value_loss         | 1.5         |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 81        |\n",
      "|    iterations         | 1800      |\n",
      "|    time_elapsed       | 110       |\n",
      "|    total_timesteps    | 9000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -43.4     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1799      |\n",
      "|    policy_loss        | -267      |\n",
      "|    reward             | 1.1382763 |\n",
      "|    std                | 1.08      |\n",
      "|    value_loss         | 49.8      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 81          |\n",
      "|    iterations         | 1900        |\n",
      "|    time_elapsed       | 116         |\n",
      "|    total_timesteps    | 9500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -43.6       |\n",
      "|    explained_variance | -1.19e-07   |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1899        |\n",
      "|    policy_loss        | -2.48       |\n",
      "|    reward             | -0.04935796 |\n",
      "|    std                | 1.09        |\n",
      "|    value_loss         | 0.0063      |\n",
      "---------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 82           |\n",
      "|    iterations         | 2000         |\n",
      "|    time_elapsed       | 121          |\n",
      "|    total_timesteps    | 10000        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -43.9        |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 1999         |\n",
      "|    policy_loss        | 4.62         |\n",
      "|    reward             | -0.018859955 |\n",
      "|    std                | 1.1          |\n",
      "|    value_loss         | 0.0223       |\n",
      "----------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 82          |\n",
      "|    iterations         | 2100        |\n",
      "|    time_elapsed       | 127         |\n",
      "|    total_timesteps    | 10500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -44         |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 2099        |\n",
      "|    policy_loss        | 16.3        |\n",
      "|    reward             | 0.044049054 |\n",
      "|    std                | 1.11        |\n",
      "|    value_loss         | 0.163       |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 82          |\n",
      "|    iterations         | 2200        |\n",
      "|    time_elapsed       | 133         |\n",
      "|    total_timesteps    | 11000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -44.2       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 2199        |\n",
      "|    policy_loss        | 10.6        |\n",
      "|    reward             | -0.07126048 |\n",
      "|    std                | 1.11        |\n",
      "|    value_loss         | 0.0811      |\n",
      "---------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 82           |\n",
      "|    iterations         | 2300         |\n",
      "|    time_elapsed       | 138          |\n",
      "|    total_timesteps    | 11500        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -44.3        |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 2299         |\n",
      "|    policy_loss        | 7.35         |\n",
      "|    reward             | -0.021035228 |\n",
      "|    std                | 1.12         |\n",
      "|    value_loss         | 0.034        |\n",
      "----------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 83         |\n",
      "|    iterations         | 2400       |\n",
      "|    time_elapsed       | 144        |\n",
      "|    total_timesteps    | 12000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -44.6      |\n",
      "|    explained_variance | 1.79e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 2399       |\n",
      "|    policy_loss        | 2.12       |\n",
      "|    reward             | 0.06537302 |\n",
      "|    std                | 1.13       |\n",
      "|    value_loss         | 0.00832    |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 83          |\n",
      "|    iterations         | 2500        |\n",
      "|    time_elapsed       | 150         |\n",
      "|    total_timesteps    | 12500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -44.8       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 2499        |\n",
      "|    policy_loss        | 6.28        |\n",
      "|    reward             | -0.09739391 |\n",
      "|    std                | 1.14        |\n",
      "|    value_loss         | 0.0211      |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 83          |\n",
      "|    iterations         | 2600        |\n",
      "|    time_elapsed       | 155         |\n",
      "|    total_timesteps    | 13000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -45.1       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 2599        |\n",
      "|    policy_loss        | 10.2        |\n",
      "|    reward             | -0.06961587 |\n",
      "|    std                | 1.15        |\n",
      "|    value_loss         | 0.0515      |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 83          |\n",
      "|    iterations         | 2700        |\n",
      "|    time_elapsed       | 161         |\n",
      "|    total_timesteps    | 13500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -45.3       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 2699        |\n",
      "|    policy_loss        | 0.424       |\n",
      "|    reward             | -0.07602931 |\n",
      "|    std                | 1.16        |\n",
      "|    value_loss         | 0.00897     |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 83        |\n",
      "|    iterations         | 2800      |\n",
      "|    time_elapsed       | 167       |\n",
      "|    total_timesteps    | 14000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -45.5     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2799      |\n",
      "|    policy_loss        | -3.25     |\n",
      "|    reward             | 0.2043822 |\n",
      "|    std                | 1.16      |\n",
      "|    value_loss         | 0.00725   |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 83       |\n",
      "|    iterations         | 2900     |\n",
      "|    time_elapsed       | 173      |\n",
      "|    total_timesteps    | 14500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -45.8    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2899     |\n",
      "|    policy_loss        | -11      |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.17     |\n",
      "|    value_loss         | 0.0771   |\n",
      "------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 83          |\n",
      "|    iterations         | 3000        |\n",
      "|    time_elapsed       | 179         |\n",
      "|    total_timesteps    | 15000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -46         |\n",
      "|    explained_variance | -1.19e-07   |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 2999        |\n",
      "|    policy_loss        | -21.3       |\n",
      "|    reward             | -0.16307645 |\n",
      "|    std                | 1.18        |\n",
      "|    value_loss         | 0.25        |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 83          |\n",
      "|    iterations         | 3100        |\n",
      "|    time_elapsed       | 185         |\n",
      "|    total_timesteps    | 15500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -46.2       |\n",
      "|    explained_variance | 2.98e-07    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 3099        |\n",
      "|    policy_loss        | 22.5        |\n",
      "|    reward             | 0.036852088 |\n",
      "|    std                | 1.19        |\n",
      "|    value_loss         | 0.276       |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 83         |\n",
      "|    iterations         | 3200       |\n",
      "|    time_elapsed       | 192        |\n",
      "|    total_timesteps    | 16000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -46.3      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 3199       |\n",
      "|    policy_loss        | -9.08      |\n",
      "|    reward             | 0.11692703 |\n",
      "|    std                | 1.19       |\n",
      "|    value_loss         | 0.0421     |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 83          |\n",
      "|    iterations         | 3300        |\n",
      "|    time_elapsed       | 198         |\n",
      "|    total_timesteps    | 16500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -46.6       |\n",
      "|    explained_variance | -1.19e-07   |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 3299        |\n",
      "|    policy_loss        | 3.38        |\n",
      "|    reward             | -0.28811547 |\n",
      "|    std                | 1.21        |\n",
      "|    value_loss         | 0.0264      |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 83          |\n",
      "|    iterations         | 3400        |\n",
      "|    time_elapsed       | 204         |\n",
      "|    total_timesteps    | 17000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -46.8       |\n",
      "|    explained_variance | 1.19e-07    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 3399        |\n",
      "|    policy_loss        | 15.8        |\n",
      "|    reward             | -0.08980935 |\n",
      "|    std                | 1.22        |\n",
      "|    value_loss         | 0.149       |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 83         |\n",
      "|    iterations         | 3500       |\n",
      "|    time_elapsed       | 210        |\n",
      "|    total_timesteps    | 17500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -46.9      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 3499       |\n",
      "|    policy_loss        | -12.1      |\n",
      "|    reward             | 0.18614393 |\n",
      "|    std                | 1.22       |\n",
      "|    value_loss         | 0.096      |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 83          |\n",
      "|    iterations         | 3600        |\n",
      "|    time_elapsed       | 216         |\n",
      "|    total_timesteps    | 18000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -47         |\n",
      "|    explained_variance | 5.96e-08    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 3599        |\n",
      "|    policy_loss        | 1.06        |\n",
      "|    reward             | -0.22664721 |\n",
      "|    std                | 1.22        |\n",
      "|    value_loss         | 0.0958      |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 83          |\n",
      "|    iterations         | 3700        |\n",
      "|    time_elapsed       | 221         |\n",
      "|    total_timesteps    | 18500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -47.3       |\n",
      "|    explained_variance | 1.19e-07    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 3699        |\n",
      "|    policy_loss        | -3.99       |\n",
      "|    reward             | -0.05658792 |\n",
      "|    std                | 1.24        |\n",
      "|    value_loss         | 0.0126      |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 83          |\n",
      "|    iterations         | 3800        |\n",
      "|    time_elapsed       | 227         |\n",
      "|    total_timesteps    | 19000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -47.6       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 3799        |\n",
      "|    policy_loss        | 8.53        |\n",
      "|    reward             | 0.025301995 |\n",
      "|    std                | 1.25        |\n",
      "|    value_loss         | 0.0451      |\n",
      "---------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 83           |\n",
      "|    iterations         | 3900         |\n",
      "|    time_elapsed       | 233          |\n",
      "|    total_timesteps    | 19500        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -47.8        |\n",
      "|    explained_variance | -1.19e-07    |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 3899         |\n",
      "|    policy_loss        | 10.6         |\n",
      "|    reward             | -0.038602553 |\n",
      "|    std                | 1.26         |\n",
      "|    value_loss         | 0.0665       |\n",
      "----------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 83         |\n",
      "|    iterations         | 4000       |\n",
      "|    time_elapsed       | 239        |\n",
      "|    total_timesteps    | 20000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -48        |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 3999       |\n",
      "|    policy_loss        | -10.6      |\n",
      "|    reward             | 0.17466703 |\n",
      "|    std                | 1.27       |\n",
      "|    value_loss         | 0.0766     |\n",
      "--------------------------------------\n",
      "day: 2265, episode: 10\n",
      "begin_total_asset: 100000.00\n",
      "end_total_asset: 147813.85\n",
      "total_reward: 47813.85\n",
      "total_cost: 8735.20\n",
      "total_trades: 38211\n",
      "Sharpe: 0.321\n",
      "=================================\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 83           |\n",
      "|    iterations         | 4100         |\n",
      "|    time_elapsed       | 244          |\n",
      "|    total_timesteps    | 20500        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -48.2        |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 4099         |\n",
      "|    policy_loss        | 12.4         |\n",
      "|    reward             | -0.012898894 |\n",
      "|    std                | 1.28         |\n",
      "|    value_loss         | 0.0987       |\n",
      "----------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 83          |\n",
      "|    iterations         | 4200        |\n",
      "|    time_elapsed       | 250         |\n",
      "|    total_timesteps    | 21000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -48.6       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 4199        |\n",
      "|    policy_loss        | 5.52        |\n",
      "|    reward             | 0.020644885 |\n",
      "|    std                | 1.29        |\n",
      "|    value_loss         | 0.0177      |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 83          |\n",
      "|    iterations         | 4300        |\n",
      "|    time_elapsed       | 256         |\n",
      "|    total_timesteps    | 21500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -48.8       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 4299        |\n",
      "|    policy_loss        | -28.9       |\n",
      "|    reward             | -0.07030593 |\n",
      "|    std                | 1.3         |\n",
      "|    value_loss         | 0.377       |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 83          |\n",
      "|    iterations         | 4400        |\n",
      "|    time_elapsed       | 262         |\n",
      "|    total_timesteps    | 22000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -49.1       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 4399        |\n",
      "|    policy_loss        | -2.45       |\n",
      "|    reward             | -0.28763855 |\n",
      "|    std                | 1.32        |\n",
      "|    value_loss         | 0.037       |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 83         |\n",
      "|    iterations         | 4500       |\n",
      "|    time_elapsed       | 268        |\n",
      "|    total_timesteps    | 22500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -49.2      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 4499       |\n",
      "|    policy_loss        | 20.8       |\n",
      "|    reward             | 0.21311617 |\n",
      "|    std                | 1.32       |\n",
      "|    value_loss         | 0.268      |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 83          |\n",
      "|    iterations         | 4600        |\n",
      "|    time_elapsed       | 275         |\n",
      "|    total_timesteps    | 23000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -49.4       |\n",
      "|    explained_variance | 5.96e-08    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 4599        |\n",
      "|    policy_loss        | 4.26        |\n",
      "|    reward             | -0.24767026 |\n",
      "|    std                | 1.33        |\n",
      "|    value_loss         | 0.0141      |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 83         |\n",
      "|    iterations         | 4700       |\n",
      "|    time_elapsed       | 281        |\n",
      "|    total_timesteps    | 23500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -49.6      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 4699       |\n",
      "|    policy_loss        | 16.2       |\n",
      "|    reward             | -0.2051953 |\n",
      "|    std                | 1.34       |\n",
      "|    value_loss         | 0.135      |\n",
      "--------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 83           |\n",
      "|    iterations         | 4800         |\n",
      "|    time_elapsed       | 287          |\n",
      "|    total_timesteps    | 24000        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -49.7        |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 4799         |\n",
      "|    policy_loss        | 13.7         |\n",
      "|    reward             | -0.061684053 |\n",
      "|    std                | 1.34         |\n",
      "|    value_loss         | 0.0976       |\n",
      "----------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 83         |\n",
      "|    iterations         | 4900       |\n",
      "|    time_elapsed       | 293        |\n",
      "|    total_timesteps    | 24500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -49.8      |\n",
      "|    explained_variance | 5.96e-08   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 4899       |\n",
      "|    policy_loss        | -61.5      |\n",
      "|    reward             | 0.09166518 |\n",
      "|    std                | 1.35       |\n",
      "|    value_loss         | 1.62       |\n",
      "--------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 83           |\n",
      "|    iterations         | 5000         |\n",
      "|    time_elapsed       | 298          |\n",
      "|    total_timesteps    | 25000        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -50          |\n",
      "|    explained_variance | 5.96e-08     |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 4999         |\n",
      "|    policy_loss        | -12.3        |\n",
      "|    reward             | -0.026585495 |\n",
      "|    std                | 1.36         |\n",
      "|    value_loss         | 0.0757       |\n",
      "----------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 83         |\n",
      "|    iterations         | 5100       |\n",
      "|    time_elapsed       | 304        |\n",
      "|    total_timesteps    | 25500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -50.3      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 5099       |\n",
      "|    policy_loss        | -0.25      |\n",
      "|    reward             | 0.21474418 |\n",
      "|    std                | 1.37       |\n",
      "|    value_loss         | 0.00986    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 83         |\n",
      "|    iterations         | 5200       |\n",
      "|    time_elapsed       | 310        |\n",
      "|    total_timesteps    | 26000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -50.4      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 5199       |\n",
      "|    policy_loss        | 7.18       |\n",
      "|    reward             | 0.30427992 |\n",
      "|    std                | 1.38       |\n",
      "|    value_loss         | 0.0341     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 83         |\n",
      "|    iterations         | 5300       |\n",
      "|    time_elapsed       | 316        |\n",
      "|    total_timesteps    | 26500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -50.6      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 5299       |\n",
      "|    policy_loss        | -9.15      |\n",
      "|    reward             | 0.68277186 |\n",
      "|    std                | 1.39       |\n",
      "|    value_loss         | 0.0454     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 83         |\n",
      "|    iterations         | 5400       |\n",
      "|    time_elapsed       | 321        |\n",
      "|    total_timesteps    | 27000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -50.7      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 5399       |\n",
      "|    policy_loss        | -38.6      |\n",
      "|    reward             | -0.4663477 |\n",
      "|    std                | 1.39       |\n",
      "|    value_loss         | 0.607      |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 83          |\n",
      "|    iterations         | 5500        |\n",
      "|    time_elapsed       | 327         |\n",
      "|    total_timesteps    | 27500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -51         |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 5499        |\n",
      "|    policy_loss        | -5.35       |\n",
      "|    reward             | 0.057172507 |\n",
      "|    std                | 1.4         |\n",
      "|    value_loss         | 0.019       |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 83         |\n",
      "|    iterations         | 5600       |\n",
      "|    time_elapsed       | 333        |\n",
      "|    total_timesteps    | 28000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -51.2      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 5599       |\n",
      "|    policy_loss        | 19.2       |\n",
      "|    reward             | 0.18423888 |\n",
      "|    std                | 1.42       |\n",
      "|    value_loss         | 0.16       |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 84       |\n",
      "|    iterations         | 5700     |\n",
      "|    time_elapsed       | 339      |\n",
      "|    total_timesteps    | 28500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -51.5    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 5699     |\n",
      "|    policy_loss        | 10.1     |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.43     |\n",
      "|    value_loss         | 0.0527   |\n",
      "------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 83           |\n",
      "|    iterations         | 5800         |\n",
      "|    time_elapsed       | 345          |\n",
      "|    total_timesteps    | 29000        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -51.7        |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 5799         |\n",
      "|    policy_loss        | 5.03         |\n",
      "|    reward             | -0.041987564 |\n",
      "|    std                | 1.44         |\n",
      "|    value_loss         | 0.0119       |\n",
      "----------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 83       |\n",
      "|    iterations         | 5900     |\n",
      "|    time_elapsed       | 351      |\n",
      "|    total_timesteps    | 29500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -51.9    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 5899     |\n",
      "|    policy_loss        | -11.4    |\n",
      "|    reward             | 0.135746 |\n",
      "|    std                | 1.45     |\n",
      "|    value_loss         | 0.087    |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 83        |\n",
      "|    iterations         | 6000      |\n",
      "|    time_elapsed       | 358       |\n",
      "|    total_timesteps    | 30000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -52.2     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 5999      |\n",
      "|    policy_loss        | 8.53      |\n",
      "|    reward             | 0.0927698 |\n",
      "|    std                | 1.47      |\n",
      "|    value_loss         | 0.0333    |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 83          |\n",
      "|    iterations         | 6100        |\n",
      "|    time_elapsed       | 364         |\n",
      "|    total_timesteps    | 30500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -52.4       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 6099        |\n",
      "|    policy_loss        | -6.33       |\n",
      "|    reward             | 0.065222606 |\n",
      "|    std                | 1.48        |\n",
      "|    value_loss         | 0.0302      |\n",
      "---------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 83           |\n",
      "|    iterations         | 6200         |\n",
      "|    time_elapsed       | 370          |\n",
      "|    total_timesteps    | 31000        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -52.7        |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 6199         |\n",
      "|    policy_loss        | -1.17        |\n",
      "|    reward             | -0.038359787 |\n",
      "|    std                | 1.49         |\n",
      "|    value_loss         | 0.00744      |\n",
      "----------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 83         |\n",
      "|    iterations         | 6300       |\n",
      "|    time_elapsed       | 376        |\n",
      "|    total_timesteps    | 31500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -53        |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 6299       |\n",
      "|    policy_loss        | 33         |\n",
      "|    reward             | 0.15588751 |\n",
      "|    std                | 1.51       |\n",
      "|    value_loss         | 0.433      |\n",
      "--------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 83           |\n",
      "|    iterations         | 6400         |\n",
      "|    time_elapsed       | 382          |\n",
      "|    total_timesteps    | 32000        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -53.3        |\n",
      "|    explained_variance | -1.19e-07    |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 6399         |\n",
      "|    policy_loss        | 10.9         |\n",
      "|    reward             | -0.033244208 |\n",
      "|    std                | 1.52         |\n",
      "|    value_loss         | 0.0611       |\n",
      "----------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 83         |\n",
      "|    iterations         | 6500       |\n",
      "|    time_elapsed       | 388        |\n",
      "|    total_timesteps    | 32500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -53.7      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 6499       |\n",
      "|    policy_loss        | -6.35      |\n",
      "|    reward             | 0.09544801 |\n",
      "|    std                | 1.54       |\n",
      "|    value_loss         | 0.0189     |\n",
      "--------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 83           |\n",
      "|    iterations         | 6600         |\n",
      "|    time_elapsed       | 394          |\n",
      "|    total_timesteps    | 33000        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -53.9        |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 6599         |\n",
      "|    policy_loss        | -10.3        |\n",
      "|    reward             | -0.087143056 |\n",
      "|    std                | 1.56         |\n",
      "|    value_loss         | 0.0748       |\n",
      "----------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 83        |\n",
      "|    iterations         | 6700      |\n",
      "|    time_elapsed       | 399       |\n",
      "|    total_timesteps    | 33500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -54.3     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6699      |\n",
      "|    policy_loss        | 10.1      |\n",
      "|    reward             | 0.1431697 |\n",
      "|    std                | 1.57      |\n",
      "|    value_loss         | 0.0564    |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 83         |\n",
      "|    iterations         | 6800       |\n",
      "|    time_elapsed       | 405        |\n",
      "|    total_timesteps    | 34000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -54.4      |\n",
      "|    explained_variance | -0.838     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 6799       |\n",
      "|    policy_loss        | -17.7      |\n",
      "|    reward             | 0.17095609 |\n",
      "|    std                | 1.58       |\n",
      "|    value_loss         | 0.143      |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 83          |\n",
      "|    iterations         | 6900        |\n",
      "|    time_elapsed       | 411         |\n",
      "|    total_timesteps    | 34500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -54.7       |\n",
      "|    explained_variance | 1.19e-07    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 6899        |\n",
      "|    policy_loss        | 0.409       |\n",
      "|    reward             | -0.03154636 |\n",
      "|    std                | 1.6         |\n",
      "|    value_loss         | 0.00164     |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 83         |\n",
      "|    iterations         | 7000       |\n",
      "|    time_elapsed       | 417        |\n",
      "|    total_timesteps    | 35000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -55        |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 6999       |\n",
      "|    policy_loss        | -5.3       |\n",
      "|    reward             | 0.14997105 |\n",
      "|    std                | 1.61       |\n",
      "|    value_loss         | 0.0152     |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 83        |\n",
      "|    iterations         | 7100      |\n",
      "|    time_elapsed       | 423       |\n",
      "|    total_timesteps    | 35500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -55.1     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 7099      |\n",
      "|    policy_loss        | 3.46      |\n",
      "|    reward             | 0.1259827 |\n",
      "|    std                | 1.62      |\n",
      "|    value_loss         | 0.00529   |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 83          |\n",
      "|    iterations         | 7200        |\n",
      "|    time_elapsed       | 428         |\n",
      "|    total_timesteps    | 36000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -55.4       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 7199        |\n",
      "|    policy_loss        | 20.3        |\n",
      "|    reward             | 0.025110174 |\n",
      "|    std                | 1.64        |\n",
      "|    value_loss         | 0.173       |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 83          |\n",
      "|    iterations         | 7300        |\n",
      "|    time_elapsed       | 434         |\n",
      "|    total_timesteps    | 36500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -55.6       |\n",
      "|    explained_variance | -1.19e-07   |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 7299        |\n",
      "|    policy_loss        | 16.7        |\n",
      "|    reward             | 0.073272966 |\n",
      "|    std                | 1.65        |\n",
      "|    value_loss         | 0.0749      |\n",
      "---------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 84           |\n",
      "|    iterations         | 7400         |\n",
      "|    time_elapsed       | 440          |\n",
      "|    total_timesteps    | 37000        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -55.8        |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 7399         |\n",
      "|    policy_loss        | -10.4        |\n",
      "|    reward             | -0.027240193 |\n",
      "|    std                | 1.66         |\n",
      "|    value_loss         | 0.0416       |\n",
      "----------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 84          |\n",
      "|    iterations         | 7500        |\n",
      "|    time_elapsed       | 446         |\n",
      "|    total_timesteps    | 37500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -56.1       |\n",
      "|    explained_variance | 1.19e-07    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 7499        |\n",
      "|    policy_loss        | -9.48       |\n",
      "|    reward             | 0.007263868 |\n",
      "|    std                | 1.68        |\n",
      "|    value_loss         | 0.0488      |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 84         |\n",
      "|    iterations         | 7600       |\n",
      "|    time_elapsed       | 452        |\n",
      "|    total_timesteps    | 38000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -56.3      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 7599       |\n",
      "|    policy_loss        | 29.4       |\n",
      "|    reward             | 0.34296954 |\n",
      "|    std                | 1.69       |\n",
      "|    value_loss         | 0.298      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 83         |\n",
      "|    iterations         | 7700       |\n",
      "|    time_elapsed       | 458        |\n",
      "|    total_timesteps    | 38500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -56.4      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 7699       |\n",
      "|    policy_loss        | 32.5       |\n",
      "|    reward             | 0.20259644 |\n",
      "|    std                | 1.69       |\n",
      "|    value_loss         | 0.367      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 83         |\n",
      "|    iterations         | 7800       |\n",
      "|    time_elapsed       | 464        |\n",
      "|    total_timesteps    | 39000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -56.5      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 7799       |\n",
      "|    policy_loss        | 3.51       |\n",
      "|    reward             | 0.06445928 |\n",
      "|    std                | 1.7        |\n",
      "|    value_loss         | 0.00967    |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 83          |\n",
      "|    iterations         | 7900        |\n",
      "|    time_elapsed       | 470         |\n",
      "|    total_timesteps    | 39500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -56.7       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 7899        |\n",
      "|    policy_loss        | -36.1       |\n",
      "|    reward             | -0.28923225 |\n",
      "|    std                | 1.71        |\n",
      "|    value_loss         | 0.496       |\n",
      "---------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 83           |\n",
      "|    iterations         | 8000         |\n",
      "|    time_elapsed       | 476          |\n",
      "|    total_timesteps    | 40000        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -56.9        |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 7999         |\n",
      "|    policy_loss        | -4.07        |\n",
      "|    reward             | -0.037054285 |\n",
      "|    std                | 1.72         |\n",
      "|    value_loss         | 0.0116       |\n",
      "----------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 83          |\n",
      "|    iterations         | 8100        |\n",
      "|    time_elapsed       | 482         |\n",
      "|    total_timesteps    | 40500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -57         |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 8099        |\n",
      "|    policy_loss        | -10.2       |\n",
      "|    reward             | -0.21474236 |\n",
      "|    std                | 1.73        |\n",
      "|    value_loss         | 0.0576      |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 83          |\n",
      "|    iterations         | 8200        |\n",
      "|    time_elapsed       | 488         |\n",
      "|    total_timesteps    | 41000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -57.2       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 8199        |\n",
      "|    policy_loss        | -10.9       |\n",
      "|    reward             | -0.21895301 |\n",
      "|    std                | 1.74        |\n",
      "|    value_loss         | 0.0416      |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 83          |\n",
      "|    iterations         | 8300        |\n",
      "|    time_elapsed       | 494         |\n",
      "|    total_timesteps    | 41500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -57.4       |\n",
      "|    explained_variance | -2.38e-07   |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 8299        |\n",
      "|    policy_loss        | -0.626      |\n",
      "|    reward             | 0.046111695 |\n",
      "|    std                | 1.76        |\n",
      "|    value_loss         | 0.00144     |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 83          |\n",
      "|    iterations         | 8400        |\n",
      "|    time_elapsed       | 500         |\n",
      "|    total_timesteps    | 42000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -57.6       |\n",
      "|    explained_variance | -1.19e-07   |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 8399        |\n",
      "|    policy_loss        | 5.1         |\n",
      "|    reward             | -0.13553937 |\n",
      "|    std                | 1.77        |\n",
      "|    value_loss         | 0.0238      |\n",
      "---------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 84           |\n",
      "|    iterations         | 8500         |\n",
      "|    time_elapsed       | 505          |\n",
      "|    total_timesteps    | 42500        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -57.8        |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 8499         |\n",
      "|    policy_loss        | 9.96         |\n",
      "|    reward             | -0.020624043 |\n",
      "|    std                | 1.78         |\n",
      "|    value_loss         | 0.0373       |\n",
      "----------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 83          |\n",
      "|    iterations         | 8600        |\n",
      "|    time_elapsed       | 512         |\n",
      "|    total_timesteps    | 43000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -58         |\n",
      "|    explained_variance | -1.19e-07   |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 8599        |\n",
      "|    policy_loss        | -33.3       |\n",
      "|    reward             | 0.023978729 |\n",
      "|    std                | 1.79        |\n",
      "|    value_loss         | 0.468       |\n",
      "---------------------------------------\n",
      "day: 2265, episode: 20\n",
      "begin_total_asset: 100000.00\n",
      "end_total_asset: 168394.52\n",
      "total_reward: 68394.52\n",
      "total_cost: 901.46\n",
      "total_trades: 28250\n",
      "Sharpe: 0.381\n",
      "=================================\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 83          |\n",
      "|    iterations         | 8700        |\n",
      "|    time_elapsed       | 517         |\n",
      "|    total_timesteps    | 43500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -58.2       |\n",
      "|    explained_variance | 5.96e-08    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 8699        |\n",
      "|    policy_loss        | 2.97        |\n",
      "|    reward             | -0.10576564 |\n",
      "|    std                | 1.81        |\n",
      "|    value_loss         | 0.00577     |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 83         |\n",
      "|    iterations         | 8800       |\n",
      "|    time_elapsed       | 524        |\n",
      "|    total_timesteps    | 44000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -58.6      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 8799       |\n",
      "|    policy_loss        | -15.4      |\n",
      "|    reward             | 0.19547395 |\n",
      "|    std                | 1.83       |\n",
      "|    value_loss         | 0.145      |\n",
      "--------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 83            |\n",
      "|    iterations         | 8900          |\n",
      "|    time_elapsed       | 530           |\n",
      "|    total_timesteps    | 44500         |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -58.8         |\n",
      "|    explained_variance | 0             |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 8899          |\n",
      "|    policy_loss        | -2.07         |\n",
      "|    reward             | -0.0075306487 |\n",
      "|    std                | 1.84          |\n",
      "|    value_loss         | 0.0378        |\n",
      "-----------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 83        |\n",
      "|    iterations         | 9000      |\n",
      "|    time_elapsed       | 537       |\n",
      "|    total_timesteps    | 45000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -59       |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 8999      |\n",
      "|    policy_loss        | -5.75     |\n",
      "|    reward             | 0.2362206 |\n",
      "|    std                | 1.86      |\n",
      "|    value_loss         | 0.0448    |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 83         |\n",
      "|    iterations         | 9100       |\n",
      "|    time_elapsed       | 544        |\n",
      "|    total_timesteps    | 45500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -59.2      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 9099       |\n",
      "|    policy_loss        | -13.2      |\n",
      "|    reward             | 0.14017956 |\n",
      "|    std                | 1.87       |\n",
      "|    value_loss         | 0.0552     |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 83        |\n",
      "|    iterations         | 9200      |\n",
      "|    time_elapsed       | 550       |\n",
      "|    total_timesteps    | 46000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -59.5     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 9199      |\n",
      "|    policy_loss        | 2.67      |\n",
      "|    reward             | 0.1165333 |\n",
      "|    std                | 1.89      |\n",
      "|    value_loss         | 0.00279   |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 83         |\n",
      "|    iterations         | 9300       |\n",
      "|    time_elapsed       | 557        |\n",
      "|    total_timesteps    | 46500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -59.8      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 9299       |\n",
      "|    policy_loss        | -12.5      |\n",
      "|    reward             | 0.06578322 |\n",
      "|    std                | 1.91       |\n",
      "|    value_loss         | 0.055      |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 83          |\n",
      "|    iterations         | 9400        |\n",
      "|    time_elapsed       | 563         |\n",
      "|    total_timesteps    | 47000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -60         |\n",
      "|    explained_variance | -1.19e-07   |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 9399        |\n",
      "|    policy_loss        | 12.5        |\n",
      "|    reward             | 0.050688565 |\n",
      "|    std                | 1.92        |\n",
      "|    value_loss         | 0.0411      |\n",
      "---------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 83           |\n",
      "|    iterations         | 9500         |\n",
      "|    time_elapsed       | 569          |\n",
      "|    total_timesteps    | 47500        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -60.2        |\n",
      "|    explained_variance | 1.19e-07     |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 9499         |\n",
      "|    policy_loss        | 9.07         |\n",
      "|    reward             | -0.016541477 |\n",
      "|    std                | 1.93         |\n",
      "|    value_loss         | 0.0324       |\n",
      "----------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 83          |\n",
      "|    iterations         | 9600        |\n",
      "|    time_elapsed       | 575         |\n",
      "|    total_timesteps    | 48000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -60.3       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 9599        |\n",
      "|    policy_loss        | 1.55        |\n",
      "|    reward             | 0.117897265 |\n",
      "|    std                | 1.94        |\n",
      "|    value_loss         | 0.00703     |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 83        |\n",
      "|    iterations         | 9700      |\n",
      "|    time_elapsed       | 581       |\n",
      "|    total_timesteps    | 48500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -60.6     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 9699      |\n",
      "|    policy_loss        | 0.136     |\n",
      "|    reward             | 0.2898299 |\n",
      "|    std                | 1.96      |\n",
      "|    value_loss         | 0.00174   |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 83         |\n",
      "|    iterations         | 9800       |\n",
      "|    time_elapsed       | 587        |\n",
      "|    total_timesteps    | 49000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -60.9      |\n",
      "|    explained_variance | 5.96e-08   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 9799       |\n",
      "|    policy_loss        | 13.2       |\n",
      "|    reward             | 0.02861532 |\n",
      "|    std                | 1.98       |\n",
      "|    value_loss         | 0.0957     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 83         |\n",
      "|    iterations         | 9900       |\n",
      "|    time_elapsed       | 593        |\n",
      "|    total_timesteps    | 49500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -61        |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 9899       |\n",
      "|    policy_loss        | 6.7        |\n",
      "|    reward             | 0.06348122 |\n",
      "|    std                | 1.99       |\n",
      "|    value_loss         | 0.0171     |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 83        |\n",
      "|    iterations         | 10000     |\n",
      "|    time_elapsed       | 599       |\n",
      "|    total_timesteps    | 50000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -61.3     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 9999      |\n",
      "|    policy_loss        | -13.4     |\n",
      "|    reward             | 0.1753232 |\n",
      "|    std                | 2.01      |\n",
      "|    value_loss         | 0.0696    |\n",
      "-------------------------------------\n"
     ]
    }
   ],
   "source": [
    "trained_a2c = agent.train_model(model=model_a2c, \n",
    "                             tb_log_name='a2c',\n",
    "                             total_timesteps=50000) if if_using_a2c else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "zjCWfgsg3sVa"
   },
   "outputs": [],
   "source": [
    "trained_a2c.save(TRAINED_MODEL_DIR + \"/agent_a2c\") if if_using_a2c else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MRiOtrywfAo1"
   },
   "source": [
    "### Agent 2: DDPG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "M2YadjfnLwgt"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_size': 128, 'buffer_size': 50000, 'learning_rate': 0.001}\n",
      "Using cpu device\n",
      "Logging to results/ddpg\n"
     ]
    }
   ],
   "source": [
    "agent = DRLAgent(env = env_train)\n",
    "model_ddpg = agent.get_model(\"ddpg\")\n",
    "\n",
    "if if_using_ddpg:\n",
    "  # set up logger\n",
    "  tmp_path = RESULTS_DIR + '/ddpg'\n",
    "  new_logger_ddpg = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
    "  # Set new logger\n",
    "  model_ddpg.set_logger(new_logger_ddpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "tCDa78rqfO_a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------\n",
      "| time/              |             |\n",
      "|    episodes        | 4           |\n",
      "|    fps             | 26          |\n",
      "|    time_elapsed    | 345         |\n",
      "|    total_timesteps | 9064        |\n",
      "| train/             |             |\n",
      "|    actor_loss      | -23.4       |\n",
      "|    critic_loss     | 1.32        |\n",
      "|    learning_rate   | 0.001       |\n",
      "|    n_updates       | 8963        |\n",
      "|    reward          | -0.42584813 |\n",
      "------------------------------------\n",
      "day: 2265, episode: 30\n",
      "begin_total_asset: 100000.00\n",
      "end_total_asset: 521220.01\n",
      "total_reward: 421220.01\n",
      "total_cost: 131.91\n",
      "total_trades: 22653\n",
      "Sharpe: 0.826\n",
      "=================================\n",
      "------------------------------------\n",
      "| time/              |             |\n",
      "|    episodes        | 8           |\n",
      "|    fps             | 26          |\n",
      "|    time_elapsed    | 682         |\n",
      "|    total_timesteps | 18128       |\n",
      "| train/             |             |\n",
      "|    actor_loss      | -13.3       |\n",
      "|    critic_loss     | 0.615       |\n",
      "|    learning_rate   | 0.001       |\n",
      "|    n_updates       | 18027       |\n",
      "|    reward          | -0.42584813 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/              |             |\n",
      "|    episodes        | 12          |\n",
      "|    fps             | 26          |\n",
      "|    time_elapsed    | 1008        |\n",
      "|    total_timesteps | 27192       |\n",
      "| train/             |             |\n",
      "|    actor_loss      | -10.1       |\n",
      "|    critic_loss     | 0.111       |\n",
      "|    learning_rate   | 0.001       |\n",
      "|    n_updates       | 27091       |\n",
      "|    reward          | -0.42584813 |\n",
      "------------------------------------\n",
      "day: 2265, episode: 40\n",
      "begin_total_asset: 100000.00\n",
      "end_total_asset: 521220.01\n",
      "total_reward: 421220.01\n",
      "total_cost: 131.91\n",
      "total_trades: 22653\n",
      "Sharpe: 0.826\n",
      "=================================\n",
      "------------------------------------\n",
      "| time/              |             |\n",
      "|    episodes        | 16          |\n",
      "|    fps             | 26          |\n",
      "|    time_elapsed    | 1348        |\n",
      "|    total_timesteps | 36256       |\n",
      "| train/             |             |\n",
      "|    actor_loss      | -7.26       |\n",
      "|    critic_loss     | 0.107       |\n",
      "|    learning_rate   | 0.001       |\n",
      "|    n_updates       | 36155       |\n",
      "|    reward          | -0.42584813 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/              |             |\n",
      "|    episodes        | 20          |\n",
      "|    fps             | 26          |\n",
      "|    time_elapsed    | 1687        |\n",
      "|    total_timesteps | 45320       |\n",
      "| train/             |             |\n",
      "|    actor_loss      | -3.61       |\n",
      "|    critic_loss     | 0.0984      |\n",
      "|    learning_rate   | 0.001       |\n",
      "|    n_updates       | 45219       |\n",
      "|    reward          | -0.42584813 |\n",
      "------------------------------------\n"
     ]
    }
   ],
   "source": [
    "trained_ddpg = agent.train_model(model=model_ddpg, \n",
    "                             tb_log_name='ddpg',\n",
    "                             total_timesteps=50000) if if_using_ddpg else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "ne6M2R-WvrUQ"
   },
   "outputs": [],
   "source": [
    "trained_ddpg.save(TRAINED_MODEL_DIR + \"/agent_ddpg\") if if_using_ddpg else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_gDkU-j-fCmZ"
   },
   "source": [
    "### Agent 3: PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "y5D5PFUhMzSV"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_steps': 2048, 'ent_coef': 0.01, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cpu device\n",
      "Logging to results/ppo\n"
     ]
    }
   ],
   "source": [
    "agent = DRLAgent(env = env_train)\n",
    "PPO_PARAMS = {\n",
    "    \"n_steps\": 2048,\n",
    "    \"ent_coef\": 0.01,\n",
    "    \"learning_rate\": 0.00025,\n",
    "    \"batch_size\": 128,\n",
    "}\n",
    "model_ppo = agent.get_model(\"ppo\",model_kwargs = PPO_PARAMS)\n",
    "\n",
    "if if_using_ppo:\n",
    "  # set up logger\n",
    "  tmp_path = RESULTS_DIR + '/ppo'\n",
    "  new_logger_ppo = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
    "  # Set new logger\n",
    "  model_ppo.set_logger(new_logger_ppo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "Gt8eIQKYM4G3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------\n",
      "| time/              |             |\n",
      "|    fps             | 114         |\n",
      "|    iterations      | 1           |\n",
      "|    time_elapsed    | 17          |\n",
      "|    total_timesteps | 2048        |\n",
      "| train/             |             |\n",
      "|    reward          | -0.06839348 |\n",
      "------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 93         |\n",
      "|    iterations           | 2          |\n",
      "|    time_elapsed         | 43         |\n",
      "|    total_timesteps      | 4096       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01293023 |\n",
      "|    clip_fraction        | 0.156      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -41.2      |\n",
      "|    explained_variance   | 0.258      |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | -0.328     |\n",
      "|    n_updates            | 10         |\n",
      "|    policy_gradient_loss | -0.0228    |\n",
      "|    reward               | 0.07791591 |\n",
      "|    std                  | 1          |\n",
      "|    value_loss           | 0.194      |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 92          |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 66          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020114463 |\n",
      "|    clip_fraction        | 0.229       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.3       |\n",
      "|    explained_variance   | 0.365       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.314      |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0246     |\n",
      "|    reward               | 0.15084505  |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 0.206       |\n",
      "-----------------------------------------\n",
      "day: 2265, episode: 50\n",
      "begin_total_asset: 100000.00\n",
      "end_total_asset: 84507.38\n",
      "total_reward: -15492.62\n",
      "total_cost: 135944.18\n",
      "total_trades: 39166\n",
      "Sharpe: 0.031\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 95           |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 85           |\n",
      "|    total_timesteps      | 8192         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.015066439  |\n",
      "|    clip_fraction        | 0.18         |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -41.3        |\n",
      "|    explained_variance   | 0.668        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.374       |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | -0.00986     |\n",
      "|    reward               | -0.077379815 |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 0.0687       |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 95          |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 107         |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.01677602  |\n",
      "|    clip_fraction        | 0.21        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.4       |\n",
      "|    explained_variance   | 0.417       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.333      |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0145     |\n",
      "|    reward               | -0.12064209 |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 0.19        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 92          |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 133         |\n",
      "|    total_timesteps      | 12288       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013591525 |\n",
      "|    clip_fraction        | 0.191       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.5       |\n",
      "|    explained_variance   | 0.54        |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.403      |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.0152     |\n",
      "|    reward               | 0.056987144 |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 0.0898      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 93          |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 153         |\n",
      "|    total_timesteps      | 14336       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017955273 |\n",
      "|    clip_fraction        | 0.208       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.6       |\n",
      "|    explained_variance   | 0.622       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.396      |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0155     |\n",
      "|    reward               | 0.059367966 |\n",
      "|    std                  | 1.02        |\n",
      "|    value_loss           | 0.0723      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 94          |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 173         |\n",
      "|    total_timesteps      | 16384       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016961742 |\n",
      "|    clip_fraction        | 0.226       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.6       |\n",
      "|    explained_variance   | 0.451       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.398      |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.0163     |\n",
      "|    reward               | 0.04692719  |\n",
      "|    std                  | 1.02        |\n",
      "|    value_loss           | 0.089       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 95          |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 192         |\n",
      "|    total_timesteps      | 18432       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014739301 |\n",
      "|    clip_fraction        | 0.227       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.7       |\n",
      "|    explained_variance   | 0.342       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.351      |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0131     |\n",
      "|    reward               | -0.06315981 |\n",
      "|    std                  | 1.02        |\n",
      "|    value_loss           | 0.179       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 96          |\n",
      "|    iterations           | 10          |\n",
      "|    time_elapsed         | 211         |\n",
      "|    total_timesteps      | 20480       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020437062 |\n",
      "|    clip_fraction        | 0.212       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.8       |\n",
      "|    explained_variance   | 0.465       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.348      |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.00843    |\n",
      "|    reward               | -0.10534678 |\n",
      "|    std                  | 1.02        |\n",
      "|    value_loss           | 0.136       |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 97         |\n",
      "|    iterations           | 11         |\n",
      "|    time_elapsed         | 230        |\n",
      "|    total_timesteps      | 22528      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01883401 |\n",
      "|    clip_fraction        | 0.221      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -41.9      |\n",
      "|    explained_variance   | 0.395      |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | -0.312     |\n",
      "|    n_updates            | 100        |\n",
      "|    policy_gradient_loss | -0.0089    |\n",
      "|    reward               | 0.05192202 |\n",
      "|    std                  | 1.03       |\n",
      "|    value_loss           | 0.206      |\n",
      "----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 96           |\n",
      "|    iterations           | 12           |\n",
      "|    time_elapsed         | 253          |\n",
      "|    total_timesteps      | 24576        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.011406636  |\n",
      "|    clip_fraction        | 0.178        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -42          |\n",
      "|    explained_variance   | 0.375        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.396       |\n",
      "|    n_updates            | 110          |\n",
      "|    policy_gradient_loss | -0.0128      |\n",
      "|    reward               | -0.026884122 |\n",
      "|    std                  | 1.03         |\n",
      "|    value_loss           | 0.151        |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 95          |\n",
      "|    iterations           | 13          |\n",
      "|    time_elapsed         | 278         |\n",
      "|    total_timesteps      | 26624       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011022916 |\n",
      "|    clip_fraction        | 0.146       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.1       |\n",
      "|    explained_variance   | 0.367       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.36       |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.016      |\n",
      "|    reward               | 0.17538948  |\n",
      "|    std                  | 1.03        |\n",
      "|    value_loss           | 0.162       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 95          |\n",
      "|    iterations           | 14          |\n",
      "|    time_elapsed         | 301         |\n",
      "|    total_timesteps      | 28672       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016717743 |\n",
      "|    clip_fraction        | 0.162       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.1       |\n",
      "|    explained_variance   | 0.272       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.284      |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.0102     |\n",
      "|    reward               | 0.054388937 |\n",
      "|    std                  | 1.03        |\n",
      "|    value_loss           | 0.258       |\n",
      "-----------------------------------------\n",
      "day: 2265, episode: 60\n",
      "begin_total_asset: 100000.00\n",
      "end_total_asset: 51016.85\n",
      "total_reward: -48983.15\n",
      "total_cost: 134207.96\n",
      "total_trades: 40232\n",
      "Sharpe: -0.199\n",
      "=================================\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 94          |\n",
      "|    iterations           | 15          |\n",
      "|    time_elapsed         | 323         |\n",
      "|    total_timesteps      | 30720       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017174326 |\n",
      "|    clip_fraction        | 0.248       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.2       |\n",
      "|    explained_variance   | 0.264       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.283      |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.00467    |\n",
      "|    reward               | -0.19655861 |\n",
      "|    std                  | 1.04        |\n",
      "|    value_loss           | 0.374       |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 95           |\n",
      "|    iterations           | 16           |\n",
      "|    time_elapsed         | 344          |\n",
      "|    total_timesteps      | 32768        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.010457676  |\n",
      "|    clip_fraction        | 0.135        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -42.3        |\n",
      "|    explained_variance   | 0.506        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.321       |\n",
      "|    n_updates            | 150          |\n",
      "|    policy_gradient_loss | -0.00826     |\n",
      "|    reward               | -0.012267775 |\n",
      "|    std                  | 1.04         |\n",
      "|    value_loss           | 0.193        |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 95          |\n",
      "|    iterations           | 17          |\n",
      "|    time_elapsed         | 363         |\n",
      "|    total_timesteps      | 34816       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010871236 |\n",
      "|    clip_fraction        | 0.134       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.4       |\n",
      "|    explained_variance   | 0.403       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.345      |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.01       |\n",
      "|    reward               | 0.08268069  |\n",
      "|    std                  | 1.04        |\n",
      "|    value_loss           | 0.265       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 96          |\n",
      "|    iterations           | 18          |\n",
      "|    time_elapsed         | 381         |\n",
      "|    total_timesteps      | 36864       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.01170833  |\n",
      "|    clip_fraction        | 0.141       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.4       |\n",
      "|    explained_variance   | 0.751       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.401      |\n",
      "|    n_updates            | 170         |\n",
      "|    policy_gradient_loss | -0.0119     |\n",
      "|    reward               | -0.06546215 |\n",
      "|    std                  | 1.05        |\n",
      "|    value_loss           | 0.0593      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 97          |\n",
      "|    iterations           | 19          |\n",
      "|    time_elapsed         | 400         |\n",
      "|    total_timesteps      | 38912       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011167703 |\n",
      "|    clip_fraction        | 0.145       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.5       |\n",
      "|    explained_variance   | 0.32        |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.352      |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | -0.0112     |\n",
      "|    reward               | 0.07861338  |\n",
      "|    std                  | 1.05        |\n",
      "|    value_loss           | 0.276       |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 97           |\n",
      "|    iterations           | 20           |\n",
      "|    time_elapsed         | 420          |\n",
      "|    total_timesteps      | 40960        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.010657135  |\n",
      "|    clip_fraction        | 0.14         |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -42.5        |\n",
      "|    explained_variance   | 0.476        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.322       |\n",
      "|    n_updates            | 190          |\n",
      "|    policy_gradient_loss | -0.0109      |\n",
      "|    reward               | -0.015572175 |\n",
      "|    std                  | 1.05         |\n",
      "|    value_loss           | 0.184        |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 97          |\n",
      "|    iterations           | 21          |\n",
      "|    time_elapsed         | 442         |\n",
      "|    total_timesteps      | 43008       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010058905 |\n",
      "|    clip_fraction        | 0.139       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.6       |\n",
      "|    explained_variance   | 0.466       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.364      |\n",
      "|    n_updates            | 200         |\n",
      "|    policy_gradient_loss | -0.0121     |\n",
      "|    reward               | -0.10997558 |\n",
      "|    std                  | 1.05        |\n",
      "|    value_loss           | 0.156       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 97          |\n",
      "|    iterations           | 22          |\n",
      "|    time_elapsed         | 463         |\n",
      "|    total_timesteps      | 45056       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008393066 |\n",
      "|    clip_fraction        | 0.116       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.7       |\n",
      "|    explained_variance   | 0.492       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.346      |\n",
      "|    n_updates            | 210         |\n",
      "|    policy_gradient_loss | -0.00927    |\n",
      "|    reward               | -0.43831536 |\n",
      "|    std                  | 1.06        |\n",
      "|    value_loss           | 0.212       |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 97           |\n",
      "|    iterations           | 23           |\n",
      "|    time_elapsed         | 485          |\n",
      "|    total_timesteps      | 47104        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0082761515 |\n",
      "|    clip_fraction        | 0.11         |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -42.7        |\n",
      "|    explained_variance   | 0.402        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.245       |\n",
      "|    n_updates            | 220          |\n",
      "|    policy_gradient_loss | -0.00939     |\n",
      "|    reward               | -0.17569517  |\n",
      "|    std                  | 1.06         |\n",
      "|    value_loss           | 0.374        |\n",
      "------------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 97         |\n",
      "|    iterations           | 24         |\n",
      "|    time_elapsed         | 505        |\n",
      "|    total_timesteps      | 49152      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01059553 |\n",
      "|    clip_fraction        | 0.141      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -42.8      |\n",
      "|    explained_variance   | 0.557      |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | -0.291     |\n",
      "|    n_updates            | 230        |\n",
      "|    policy_gradient_loss | -0.012     |\n",
      "|    reward               | 0.07056723 |\n",
      "|    std                  | 1.06       |\n",
      "|    value_loss           | 0.25       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 97          |\n",
      "|    iterations           | 25          |\n",
      "|    time_elapsed         | 525         |\n",
      "|    total_timesteps      | 51200       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008493245 |\n",
      "|    clip_fraction        | 0.095       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.8       |\n",
      "|    explained_variance   | 0.577       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.356      |\n",
      "|    n_updates            | 240         |\n",
      "|    policy_gradient_loss | -0.00904    |\n",
      "|    reward               | -0.13687724 |\n",
      "|    std                  | 1.06        |\n",
      "|    value_loss           | 0.199       |\n",
      "-----------------------------------------\n",
      "day: 2265, episode: 70\n",
      "begin_total_asset: 100000.00\n",
      "end_total_asset: 86908.60\n",
      "total_reward: -13091.40\n",
      "total_cost: 141867.88\n",
      "total_trades: 40746\n",
      "Sharpe: 0.050\n",
      "=================================\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 97            |\n",
      "|    iterations           | 26            |\n",
      "|    time_elapsed         | 544           |\n",
      "|    total_timesteps      | 53248         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.007359444   |\n",
      "|    clip_fraction        | 0.0777        |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -42.9         |\n",
      "|    explained_variance   | 0.496         |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.317        |\n",
      "|    n_updates            | 250           |\n",
      "|    policy_gradient_loss | -0.00679      |\n",
      "|    reward               | -0.0075103156 |\n",
      "|    std                  | 1.06          |\n",
      "|    value_loss           | 0.253         |\n",
      "-------------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 98         |\n",
      "|    iterations           | 27         |\n",
      "|    time_elapsed         | 563        |\n",
      "|    total_timesteps      | 55296      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01051438 |\n",
      "|    clip_fraction        | 0.121      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -43        |\n",
      "|    explained_variance   | 0.658      |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | -0.392     |\n",
      "|    n_updates            | 260        |\n",
      "|    policy_gradient_loss | -0.0113    |\n",
      "|    reward               | 0.12757786 |\n",
      "|    std                  | 1.07       |\n",
      "|    value_loss           | 0.109      |\n",
      "----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 98           |\n",
      "|    iterations           | 28           |\n",
      "|    time_elapsed         | 583          |\n",
      "|    total_timesteps      | 57344        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.007620837  |\n",
      "|    clip_fraction        | 0.0873       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -43          |\n",
      "|    explained_variance   | 0.728        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.392       |\n",
      "|    n_updates            | 270          |\n",
      "|    policy_gradient_loss | -0.00669     |\n",
      "|    reward               | -0.008863671 |\n",
      "|    std                  | 1.07         |\n",
      "|    value_loss           | 0.0723       |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 98          |\n",
      "|    iterations           | 29          |\n",
      "|    time_elapsed         | 602         |\n",
      "|    total_timesteps      | 59392       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009632139 |\n",
      "|    clip_fraction        | 0.104       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -43.1       |\n",
      "|    explained_variance   | 0.493       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.303      |\n",
      "|    n_updates            | 280         |\n",
      "|    policy_gradient_loss | -0.00994    |\n",
      "|    reward               | 0.13472874  |\n",
      "|    std                  | 1.07        |\n",
      "|    value_loss           | 0.197       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 98          |\n",
      "|    iterations           | 30          |\n",
      "|    time_elapsed         | 621         |\n",
      "|    total_timesteps      | 61440       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008568203 |\n",
      "|    clip_fraction        | 0.112       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -43.2       |\n",
      "|    explained_variance   | 0.549       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.37       |\n",
      "|    n_updates            | 290         |\n",
      "|    policy_gradient_loss | -0.0107     |\n",
      "|    reward               | 0.13920888  |\n",
      "|    std                  | 1.08        |\n",
      "|    value_loss           | 0.134       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 99          |\n",
      "|    iterations           | 31          |\n",
      "|    time_elapsed         | 640         |\n",
      "|    total_timesteps      | 63488       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010431776 |\n",
      "|    clip_fraction        | 0.136       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -43.3       |\n",
      "|    explained_variance   | 0.366       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.354      |\n",
      "|    n_updates            | 300         |\n",
      "|    policy_gradient_loss | -0.0135     |\n",
      "|    reward               | -0.03893297 |\n",
      "|    std                  | 1.08        |\n",
      "|    value_loss           | 0.272       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 99          |\n",
      "|    iterations           | 32          |\n",
      "|    time_elapsed         | 659         |\n",
      "|    total_timesteps      | 65536       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008567444 |\n",
      "|    clip_fraction        | 0.0889      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -43.3       |\n",
      "|    explained_variance   | 0.462       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.267      |\n",
      "|    n_updates            | 310         |\n",
      "|    policy_gradient_loss | -0.00883    |\n",
      "|    reward               | -0.21851677 |\n",
      "|    std                  | 1.08        |\n",
      "|    value_loss           | 0.262       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 99          |\n",
      "|    iterations           | 33          |\n",
      "|    time_elapsed         | 678         |\n",
      "|    total_timesteps      | 67584       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007671895 |\n",
      "|    clip_fraction        | 0.0999      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -43.4       |\n",
      "|    explained_variance   | 0.614       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.409      |\n",
      "|    n_updates            | 320         |\n",
      "|    policy_gradient_loss | -0.00807    |\n",
      "|    reward               | 0.165093    |\n",
      "|    std                  | 1.08        |\n",
      "|    value_loss           | 0.0834      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 99          |\n",
      "|    iterations           | 34          |\n",
      "|    time_elapsed         | 697         |\n",
      "|    total_timesteps      | 69632       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009831205 |\n",
      "|    clip_fraction        | 0.126       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -43.5       |\n",
      "|    explained_variance   | 0.554       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.418      |\n",
      "|    n_updates            | 330         |\n",
      "|    policy_gradient_loss | -0.0114     |\n",
      "|    reward               | -0.26506716 |\n",
      "|    std                  | 1.09        |\n",
      "|    value_loss           | 0.103       |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 100        |\n",
      "|    iterations           | 35         |\n",
      "|    time_elapsed         | 715        |\n",
      "|    total_timesteps      | 71680      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00913181 |\n",
      "|    clip_fraction        | 0.119      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -43.6      |\n",
      "|    explained_variance   | 0.47       |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | -0.419     |\n",
      "|    n_updates            | 340        |\n",
      "|    policy_gradient_loss | -0.0121    |\n",
      "|    reward               | 0.07832817 |\n",
      "|    std                  | 1.09       |\n",
      "|    value_loss           | 0.117      |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 100         |\n",
      "|    iterations           | 36          |\n",
      "|    time_elapsed         | 735         |\n",
      "|    total_timesteps      | 73728       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010693285 |\n",
      "|    clip_fraction        | 0.135       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -43.6       |\n",
      "|    explained_variance   | 0.307       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.241      |\n",
      "|    n_updates            | 350         |\n",
      "|    policy_gradient_loss | -0.0113     |\n",
      "|    reward               | -0.06430605 |\n",
      "|    std                  | 1.09        |\n",
      "|    value_loss           | 0.344       |\n",
      "-----------------------------------------\n",
      "day: 2265, episode: 80\n",
      "begin_total_asset: 100000.00\n",
      "end_total_asset: 114728.34\n",
      "total_reward: 14728.34\n",
      "total_cost: 154839.67\n",
      "total_trades: 41293\n",
      "Sharpe: 0.180\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 100          |\n",
      "|    iterations           | 37           |\n",
      "|    time_elapsed         | 756          |\n",
      "|    total_timesteps      | 75776        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.008653037  |\n",
      "|    clip_fraction        | 0.11         |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -43.7        |\n",
      "|    explained_variance   | 0.533        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.384       |\n",
      "|    n_updates            | 360          |\n",
      "|    policy_gradient_loss | -0.00998     |\n",
      "|    reward               | -0.020205036 |\n",
      "|    std                  | 1.09         |\n",
      "|    value_loss           | 0.146        |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 100         |\n",
      "|    iterations           | 38          |\n",
      "|    time_elapsed         | 776         |\n",
      "|    total_timesteps      | 77824       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009869007 |\n",
      "|    clip_fraction        | 0.126       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -43.8       |\n",
      "|    explained_variance   | 0.495       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.363      |\n",
      "|    n_updates            | 370         |\n",
      "|    policy_gradient_loss | -0.0111     |\n",
      "|    reward               | 0.035484772 |\n",
      "|    std                  | 1.1         |\n",
      "|    value_loss           | 0.162       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 100         |\n",
      "|    iterations           | 39          |\n",
      "|    time_elapsed         | 796         |\n",
      "|    total_timesteps      | 79872       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010683552 |\n",
      "|    clip_fraction        | 0.134       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -43.9       |\n",
      "|    explained_variance   | 0.418       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.374      |\n",
      "|    n_updates            | 380         |\n",
      "|    policy_gradient_loss | -0.0118     |\n",
      "|    reward               | -0.06869519 |\n",
      "|    std                  | 1.1         |\n",
      "|    value_loss           | 0.171       |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 99           |\n",
      "|    iterations           | 40           |\n",
      "|    time_elapsed         | 825          |\n",
      "|    total_timesteps      | 81920        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.008698868  |\n",
      "|    clip_fraction        | 0.0946       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -44          |\n",
      "|    explained_variance   | 0.497        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.406       |\n",
      "|    n_updates            | 390          |\n",
      "|    policy_gradient_loss | -0.0086      |\n",
      "|    reward               | -0.040723093 |\n",
      "|    std                  | 1.11         |\n",
      "|    value_loss           | 0.11         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 98          |\n",
      "|    iterations           | 41          |\n",
      "|    time_elapsed         | 853         |\n",
      "|    total_timesteps      | 83968       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009939242 |\n",
      "|    clip_fraction        | 0.117       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44.1       |\n",
      "|    explained_variance   | 0.453       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.402      |\n",
      "|    n_updates            | 400         |\n",
      "|    policy_gradient_loss | -0.0114     |\n",
      "|    reward               | 0.06689899  |\n",
      "|    std                  | 1.11        |\n",
      "|    value_loss           | 0.101       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 98          |\n",
      "|    iterations           | 42          |\n",
      "|    time_elapsed         | 872         |\n",
      "|    total_timesteps      | 86016       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010894923 |\n",
      "|    clip_fraction        | 0.123       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44.1       |\n",
      "|    explained_variance   | 0.523       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.419      |\n",
      "|    n_updates            | 410         |\n",
      "|    policy_gradient_loss | -0.0132     |\n",
      "|    reward               | 0.025773687 |\n",
      "|    std                  | 1.11        |\n",
      "|    value_loss           | 0.0823      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 98          |\n",
      "|    iterations           | 43          |\n",
      "|    time_elapsed         | 895         |\n",
      "|    total_timesteps      | 88064       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008267416 |\n",
      "|    clip_fraction        | 0.0971      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44.1       |\n",
      "|    explained_variance   | 0.336       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.368      |\n",
      "|    n_updates            | 420         |\n",
      "|    policy_gradient_loss | -0.00845    |\n",
      "|    reward               | 0.1425043   |\n",
      "|    std                  | 1.11        |\n",
      "|    value_loss           | 0.169       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 98          |\n",
      "|    iterations           | 44          |\n",
      "|    time_elapsed         | 917         |\n",
      "|    total_timesteps      | 90112       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010396761 |\n",
      "|    clip_fraction        | 0.109       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44.2       |\n",
      "|    explained_variance   | 0.335       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.343      |\n",
      "|    n_updates            | 430         |\n",
      "|    policy_gradient_loss | -0.0101     |\n",
      "|    reward               | 0.14066753  |\n",
      "|    std                  | 1.11        |\n",
      "|    value_loss           | 0.202       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 98          |\n",
      "|    iterations           | 45          |\n",
      "|    time_elapsed         | 937         |\n",
      "|    total_timesteps      | 92160       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009980062 |\n",
      "|    clip_fraction        | 0.116       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44.2       |\n",
      "|    explained_variance   | 0.397       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.362      |\n",
      "|    n_updates            | 440         |\n",
      "|    policy_gradient_loss | -0.0108     |\n",
      "|    reward               | -0.0992387  |\n",
      "|    std                  | 1.11        |\n",
      "|    value_loss           | 0.191       |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 98           |\n",
      "|    iterations           | 46           |\n",
      "|    time_elapsed         | 956          |\n",
      "|    total_timesteps      | 94208        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.008808611  |\n",
      "|    clip_fraction        | 0.0975       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -44.3        |\n",
      "|    explained_variance   | 0.304        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.346       |\n",
      "|    n_updates            | 450          |\n",
      "|    policy_gradient_loss | -0.00855     |\n",
      "|    reward               | 0.0009680493 |\n",
      "|    std                  | 1.12         |\n",
      "|    value_loss           | 0.397        |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 98          |\n",
      "|    iterations           | 47          |\n",
      "|    time_elapsed         | 975         |\n",
      "|    total_timesteps      | 96256       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010753849 |\n",
      "|    clip_fraction        | 0.125       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44.3       |\n",
      "|    explained_variance   | 0.413       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.334      |\n",
      "|    n_updates            | 460         |\n",
      "|    policy_gradient_loss | -0.0123     |\n",
      "|    reward               | -0.12361253 |\n",
      "|    std                  | 1.12        |\n",
      "|    value_loss           | 0.217       |\n",
      "-----------------------------------------\n",
      "day: 2265, episode: 90\n",
      "begin_total_asset: 100000.00\n",
      "end_total_asset: 104697.98\n",
      "total_reward: 4697.98\n",
      "total_cost: 221160.35\n",
      "total_trades: 42195\n",
      "Sharpe: 0.138\n",
      "=================================\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 98          |\n",
      "|    iterations           | 48          |\n",
      "|    time_elapsed         | 997         |\n",
      "|    total_timesteps      | 98304       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008954108 |\n",
      "|    clip_fraction        | 0.103       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44.4       |\n",
      "|    explained_variance   | 0.358       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.22       |\n",
      "|    n_updates            | 470         |\n",
      "|    policy_gradient_loss | -0.00799    |\n",
      "|    reward               | 0.017596146 |\n",
      "|    std                  | 1.12        |\n",
      "|    value_loss           | 0.4         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 98          |\n",
      "|    iterations           | 49          |\n",
      "|    time_elapsed         | 1017        |\n",
      "|    total_timesteps      | 100352      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012242603 |\n",
      "|    clip_fraction        | 0.14        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44.5       |\n",
      "|    explained_variance   | 0.272       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.179      |\n",
      "|    n_updates            | 480         |\n",
      "|    policy_gradient_loss | -0.011      |\n",
      "|    reward               | 0.095033266 |\n",
      "|    std                  | 1.13        |\n",
      "|    value_loss           | 0.551       |\n",
      "-----------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 98            |\n",
      "|    iterations           | 50            |\n",
      "|    time_elapsed         | 1037          |\n",
      "|    total_timesteps      | 102400        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.009435751   |\n",
      "|    clip_fraction        | 0.123         |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -44.6         |\n",
      "|    explained_variance   | 0.491         |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.395        |\n",
      "|    n_updates            | 490           |\n",
      "|    policy_gradient_loss | -0.00802      |\n",
      "|    reward               | -0.0092970105 |\n",
      "|    std                  | 1.13          |\n",
      "|    value_loss           | 0.178         |\n",
      "-------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 98          |\n",
      "|    iterations           | 51          |\n",
      "|    time_elapsed         | 1056        |\n",
      "|    total_timesteps      | 104448      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007848097 |\n",
      "|    clip_fraction        | 0.107       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44.6       |\n",
      "|    explained_variance   | 0.346       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.26       |\n",
      "|    n_updates            | 500         |\n",
      "|    policy_gradient_loss | -0.00971    |\n",
      "|    reward               | -0.16704673 |\n",
      "|    std                  | 1.13        |\n",
      "|    value_loss           | 0.296       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 98          |\n",
      "|    iterations           | 52          |\n",
      "|    time_elapsed         | 1075        |\n",
      "|    total_timesteps      | 106496      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009756757 |\n",
      "|    clip_fraction        | 0.128       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44.7       |\n",
      "|    explained_variance   | 0.544       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.433      |\n",
      "|    n_updates            | 510         |\n",
      "|    policy_gradient_loss | -0.0102     |\n",
      "|    reward               | 0.39523384  |\n",
      "|    std                  | 1.13        |\n",
      "|    value_loss           | 0.0893      |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 99           |\n",
      "|    iterations           | 53           |\n",
      "|    time_elapsed         | 1095         |\n",
      "|    total_timesteps      | 108544       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0104605295 |\n",
      "|    clip_fraction        | 0.123        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -44.7        |\n",
      "|    explained_variance   | 0.208        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.216       |\n",
      "|    n_updates            | 520          |\n",
      "|    policy_gradient_loss | -0.0116      |\n",
      "|    reward               | -0.009931814 |\n",
      "|    std                  | 1.13         |\n",
      "|    value_loss           | 0.358        |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 99          |\n",
      "|    iterations           | 54          |\n",
      "|    time_elapsed         | 1115        |\n",
      "|    total_timesteps      | 110592      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009620122 |\n",
      "|    clip_fraction        | 0.104       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44.8       |\n",
      "|    explained_variance   | 0.473       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.369      |\n",
      "|    n_updates            | 530         |\n",
      "|    policy_gradient_loss | -0.0102     |\n",
      "|    reward               | 0.041652568 |\n",
      "|    std                  | 1.14        |\n",
      "|    value_loss           | 0.12        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 99           |\n",
      "|    iterations           | 55           |\n",
      "|    time_elapsed         | 1135         |\n",
      "|    total_timesteps      | 112640       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.009947233  |\n",
      "|    clip_fraction        | 0.0991       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -44.9        |\n",
      "|    explained_variance   | 0.432        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.405       |\n",
      "|    n_updates            | 540          |\n",
      "|    policy_gradient_loss | -0.00973     |\n",
      "|    reward               | -0.051698856 |\n",
      "|    std                  | 1.14         |\n",
      "|    value_loss           | 0.122        |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 99          |\n",
      "|    iterations           | 56          |\n",
      "|    time_elapsed         | 1154        |\n",
      "|    total_timesteps      | 114688      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010929663 |\n",
      "|    clip_fraction        | 0.129       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -45         |\n",
      "|    explained_variance   | 0.27        |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.285      |\n",
      "|    n_updates            | 550         |\n",
      "|    policy_gradient_loss | -0.0121     |\n",
      "|    reward               | -0.05552242 |\n",
      "|    std                  | 1.14        |\n",
      "|    value_loss           | 0.285       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 99          |\n",
      "|    iterations           | 57          |\n",
      "|    time_elapsed         | 1173        |\n",
      "|    total_timesteps      | 116736      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010892751 |\n",
      "|    clip_fraction        | 0.131       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -45         |\n",
      "|    explained_variance   | 0.26        |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.245      |\n",
      "|    n_updates            | 560         |\n",
      "|    policy_gradient_loss | -0.0114     |\n",
      "|    reward               | 0.019567428 |\n",
      "|    std                  | 1.15        |\n",
      "|    value_loss           | 0.417       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 99          |\n",
      "|    iterations           | 58          |\n",
      "|    time_elapsed         | 1192        |\n",
      "|    total_timesteps      | 118784      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009189734 |\n",
      "|    clip_fraction        | 0.11        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -45.1       |\n",
      "|    explained_variance   | 0.459       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.345      |\n",
      "|    n_updates            | 570         |\n",
      "|    policy_gradient_loss | -0.0105     |\n",
      "|    reward               | 0.053823788 |\n",
      "|    std                  | 1.15        |\n",
      "|    value_loss           | 0.256       |\n",
      "-----------------------------------------\n",
      "day: 2265, episode: 100\n",
      "begin_total_asset: 100000.00\n",
      "end_total_asset: 31133.47\n",
      "total_reward: -68866.53\n",
      "total_cost: 116621.78\n",
      "total_trades: 40355\n",
      "Sharpe: -0.479\n",
      "=================================\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 99          |\n",
      "|    iterations           | 59          |\n",
      "|    time_elapsed         | 1213        |\n",
      "|    total_timesteps      | 120832      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011839451 |\n",
      "|    clip_fraction        | 0.185       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -45.2       |\n",
      "|    explained_variance   | 0.252       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 0.000314    |\n",
      "|    n_updates            | 580         |\n",
      "|    policy_gradient_loss | -0.0129     |\n",
      "|    reward               | 0.046327874 |\n",
      "|    std                  | 1.15        |\n",
      "|    value_loss           | 0.667       |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 99           |\n",
      "|    iterations           | 60           |\n",
      "|    time_elapsed         | 1234         |\n",
      "|    total_timesteps      | 122880       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0097249765 |\n",
      "|    clip_fraction        | 0.129        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -45.2        |\n",
      "|    explained_variance   | 0.599        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.396       |\n",
      "|    n_updates            | 590          |\n",
      "|    policy_gradient_loss | -0.0103      |\n",
      "|    reward               | 0.0463348    |\n",
      "|    std                  | 1.15         |\n",
      "|    value_loss           | 0.131        |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 99          |\n",
      "|    iterations           | 61          |\n",
      "|    time_elapsed         | 1257        |\n",
      "|    total_timesteps      | 124928      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010553819 |\n",
      "|    clip_fraction        | 0.117       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -45.3       |\n",
      "|    explained_variance   | 0.456       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.293      |\n",
      "|    n_updates            | 600         |\n",
      "|    policy_gradient_loss | -0.00812    |\n",
      "|    reward               | 0.24143818  |\n",
      "|    std                  | 1.16        |\n",
      "|    value_loss           | 0.271       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 99          |\n",
      "|    iterations           | 62          |\n",
      "|    time_elapsed         | 1281        |\n",
      "|    total_timesteps      | 126976      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008993229 |\n",
      "|    clip_fraction        | 0.102       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -45.3       |\n",
      "|    explained_variance   | 0.433       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.306      |\n",
      "|    n_updates            | 610         |\n",
      "|    policy_gradient_loss | -0.00874    |\n",
      "|    reward               | 0.019554667 |\n",
      "|    std                  | 1.16        |\n",
      "|    value_loss           | 0.311       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 99          |\n",
      "|    iterations           | 63          |\n",
      "|    time_elapsed         | 1300        |\n",
      "|    total_timesteps      | 129024      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010122252 |\n",
      "|    clip_fraction        | 0.122       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -45.4       |\n",
      "|    explained_variance   | 0.354       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.146      |\n",
      "|    n_updates            | 620         |\n",
      "|    policy_gradient_loss | -0.0102     |\n",
      "|    reward               | 0.03789953  |\n",
      "|    std                  | 1.16        |\n",
      "|    value_loss           | 0.627       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 99          |\n",
      "|    iterations           | 64          |\n",
      "|    time_elapsed         | 1322        |\n",
      "|    total_timesteps      | 131072      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009721439 |\n",
      "|    clip_fraction        | 0.11        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -45.4       |\n",
      "|    explained_variance   | 0.471       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.334      |\n",
      "|    n_updates            | 630         |\n",
      "|    policy_gradient_loss | -0.0101     |\n",
      "|    reward               | 0.10289787  |\n",
      "|    std                  | 1.16        |\n",
      "|    value_loss           | 0.346       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 99          |\n",
      "|    iterations           | 65          |\n",
      "|    time_elapsed         | 1341        |\n",
      "|    total_timesteps      | 133120      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011086507 |\n",
      "|    clip_fraction        | 0.137       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -45.5       |\n",
      "|    explained_variance   | 0.537       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.365      |\n",
      "|    n_updates            | 640         |\n",
      "|    policy_gradient_loss | -0.0104     |\n",
      "|    reward               | -0.13429365 |\n",
      "|    std                  | 1.16        |\n",
      "|    value_loss           | 0.27        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 99           |\n",
      "|    iterations           | 66           |\n",
      "|    time_elapsed         | 1360         |\n",
      "|    total_timesteps      | 135168       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.008506477  |\n",
      "|    clip_fraction        | 0.118        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -45.6        |\n",
      "|    explained_variance   | 0.589        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.407       |\n",
      "|    n_updates            | 650          |\n",
      "|    policy_gradient_loss | -0.0107      |\n",
      "|    reward               | -0.056345288 |\n",
      "|    std                  | 1.17         |\n",
      "|    value_loss           | 0.154        |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 99          |\n",
      "|    iterations           | 67          |\n",
      "|    time_elapsed         | 1379        |\n",
      "|    total_timesteps      | 137216      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010631831 |\n",
      "|    clip_fraction        | 0.137       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -45.7       |\n",
      "|    explained_variance   | 0.613       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.389      |\n",
      "|    n_updates            | 660         |\n",
      "|    policy_gradient_loss | -0.0126     |\n",
      "|    reward               | 0.059433546 |\n",
      "|    std                  | 1.17        |\n",
      "|    value_loss           | 0.188       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 99          |\n",
      "|    iterations           | 68          |\n",
      "|    time_elapsed         | 1398        |\n",
      "|    total_timesteps      | 139264      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012595079 |\n",
      "|    clip_fraction        | 0.154       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -45.7       |\n",
      "|    explained_variance   | 0.537       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.386      |\n",
      "|    n_updates            | 670         |\n",
      "|    policy_gradient_loss | -0.0138     |\n",
      "|    reward               | 0.2800136   |\n",
      "|    std                  | 1.17        |\n",
      "|    value_loss           | 0.209       |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 99         |\n",
      "|    iterations           | 69         |\n",
      "|    time_elapsed         | 1417       |\n",
      "|    total_timesteps      | 141312     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01022484 |\n",
      "|    clip_fraction        | 0.13       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -45.7      |\n",
      "|    explained_variance   | 0.45       |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | -0.334     |\n",
      "|    n_updates            | 680        |\n",
      "|    policy_gradient_loss | -0.0126    |\n",
      "|    reward               | 0.12533562 |\n",
      "|    std                  | 1.17       |\n",
      "|    value_loss           | 0.274      |\n",
      "----------------------------------------\n",
      "day: 2265, episode: 110\n",
      "begin_total_asset: 100000.00\n",
      "end_total_asset: 124978.68\n",
      "total_reward: 24978.68\n",
      "total_cost: 148029.62\n",
      "total_trades: 41287\n",
      "Sharpe: 0.223\n",
      "=================================\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 99          |\n",
      "|    iterations           | 70          |\n",
      "|    time_elapsed         | 1438        |\n",
      "|    total_timesteps      | 143360      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010578544 |\n",
      "|    clip_fraction        | 0.119       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -45.7       |\n",
      "|    explained_variance   | 0.475       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.311      |\n",
      "|    n_updates            | 690         |\n",
      "|    policy_gradient_loss | -0.0121     |\n",
      "|    reward               | 0.14358552  |\n",
      "|    std                  | 1.17        |\n",
      "|    value_loss           | 0.232       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 99          |\n",
      "|    iterations           | 71          |\n",
      "|    time_elapsed         | 1456        |\n",
      "|    total_timesteps      | 145408      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010928456 |\n",
      "|    clip_fraction        | 0.125       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -45.8       |\n",
      "|    explained_variance   | 0.374       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.32       |\n",
      "|    n_updates            | 700         |\n",
      "|    policy_gradient_loss | -0.0131     |\n",
      "|    reward               | -0.03747087 |\n",
      "|    std                  | 1.17        |\n",
      "|    value_loss           | 0.258       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 99          |\n",
      "|    iterations           | 72          |\n",
      "|    time_elapsed         | 1477        |\n",
      "|    total_timesteps      | 147456      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009966455 |\n",
      "|    clip_fraction        | 0.117       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -45.9       |\n",
      "|    explained_variance   | 0.404       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.393      |\n",
      "|    n_updates            | 710         |\n",
      "|    policy_gradient_loss | -0.0123     |\n",
      "|    reward               | -0.15790227 |\n",
      "|    std                  | 1.18        |\n",
      "|    value_loss           | 0.212       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 99          |\n",
      "|    iterations           | 73          |\n",
      "|    time_elapsed         | 1497        |\n",
      "|    total_timesteps      | 149504      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010324137 |\n",
      "|    clip_fraction        | 0.112       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -46         |\n",
      "|    explained_variance   | 0.307       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.328      |\n",
      "|    n_updates            | 720         |\n",
      "|    policy_gradient_loss | -0.0108     |\n",
      "|    reward               | 0.15263051  |\n",
      "|    std                  | 1.19        |\n",
      "|    value_loss           | 0.324       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 99          |\n",
      "|    iterations           | 74          |\n",
      "|    time_elapsed         | 1518        |\n",
      "|    total_timesteps      | 151552      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007881287 |\n",
      "|    clip_fraction        | 0.0982      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -46.1       |\n",
      "|    explained_variance   | 0.48        |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.444      |\n",
      "|    n_updates            | 730         |\n",
      "|    policy_gradient_loss | -0.00908    |\n",
      "|    reward               | -0.08808212 |\n",
      "|    std                  | 1.19        |\n",
      "|    value_loss           | 0.0977      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 99          |\n",
      "|    iterations           | 75          |\n",
      "|    time_elapsed         | 1539        |\n",
      "|    total_timesteps      | 153600      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009729498 |\n",
      "|    clip_fraction        | 0.109       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -46.2       |\n",
      "|    explained_variance   | 0.605       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.445      |\n",
      "|    n_updates            | 740         |\n",
      "|    policy_gradient_loss | -0.0101     |\n",
      "|    reward               | 0.038608164 |\n",
      "|    std                  | 1.19        |\n",
      "|    value_loss           | 0.079       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 99          |\n",
      "|    iterations           | 76          |\n",
      "|    time_elapsed         | 1558        |\n",
      "|    total_timesteps      | 155648      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008895867 |\n",
      "|    clip_fraction        | 0.106       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -46.3       |\n",
      "|    explained_variance   | 0.41        |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.326      |\n",
      "|    n_updates            | 750         |\n",
      "|    policy_gradient_loss | -0.0109     |\n",
      "|    reward               | 0.07818423  |\n",
      "|    std                  | 1.2         |\n",
      "|    value_loss           | 0.312       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 100         |\n",
      "|    iterations           | 77          |\n",
      "|    time_elapsed         | 1576        |\n",
      "|    total_timesteps      | 157696      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010580422 |\n",
      "|    clip_fraction        | 0.113       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -46.4       |\n",
      "|    explained_variance   | 0.443       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.314      |\n",
      "|    n_updates            | 760         |\n",
      "|    policy_gradient_loss | -0.00814    |\n",
      "|    reward               | -0.11766554 |\n",
      "|    std                  | 1.2         |\n",
      "|    value_loss           | 0.32        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 100         |\n",
      "|    iterations           | 78          |\n",
      "|    time_elapsed         | 1596        |\n",
      "|    total_timesteps      | 159744      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009050218 |\n",
      "|    clip_fraction        | 0.0958      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -46.5       |\n",
      "|    explained_variance   | 0.523       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.348      |\n",
      "|    n_updates            | 770         |\n",
      "|    policy_gradient_loss | -0.00855    |\n",
      "|    reward               | -0.17426825 |\n",
      "|    std                  | 1.2         |\n",
      "|    value_loss           | 0.207       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 100         |\n",
      "|    iterations           | 79          |\n",
      "|    time_elapsed         | 1616        |\n",
      "|    total_timesteps      | 161792      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008450184 |\n",
      "|    clip_fraction        | 0.0965      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -46.5       |\n",
      "|    explained_variance   | 0.404       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.35       |\n",
      "|    n_updates            | 780         |\n",
      "|    policy_gradient_loss | -0.00999    |\n",
      "|    reward               | -0.1041998  |\n",
      "|    std                  | 1.2         |\n",
      "|    value_loss           | 0.329       |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 100        |\n",
      "|    iterations           | 80         |\n",
      "|    time_elapsed         | 1638       |\n",
      "|    total_timesteps      | 163840     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01067204 |\n",
      "|    clip_fraction        | 0.118      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -46.5      |\n",
      "|    explained_variance   | 0.523      |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | -0.357     |\n",
      "|    n_updates            | 790        |\n",
      "|    policy_gradient_loss | -0.00993   |\n",
      "|    reward               | 0.08500281 |\n",
      "|    std                  | 1.21       |\n",
      "|    value_loss           | 0.156      |\n",
      "----------------------------------------\n",
      "day: 2265, episode: 120\n",
      "begin_total_asset: 100000.00\n",
      "end_total_asset: 176934.63\n",
      "total_reward: 76934.63\n",
      "total_cost: 184962.71\n",
      "total_trades: 41566\n",
      "Sharpe: 0.407\n",
      "=================================\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 99          |\n",
      "|    iterations           | 81          |\n",
      "|    time_elapsed         | 1659        |\n",
      "|    total_timesteps      | 165888      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008923064 |\n",
      "|    clip_fraction        | 0.124       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -46.6       |\n",
      "|    explained_variance   | 0.43        |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.348      |\n",
      "|    n_updates            | 800         |\n",
      "|    policy_gradient_loss | -0.013      |\n",
      "|    reward               | -0.16850716 |\n",
      "|    std                  | 1.21        |\n",
      "|    value_loss           | 0.192       |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 99           |\n",
      "|    iterations           | 82           |\n",
      "|    time_elapsed         | 1680         |\n",
      "|    total_timesteps      | 167936       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.010540472  |\n",
      "|    clip_fraction        | 0.117        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -46.7        |\n",
      "|    explained_variance   | 0.385        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.319       |\n",
      "|    n_updates            | 810          |\n",
      "|    policy_gradient_loss | -0.0119      |\n",
      "|    reward               | -0.010369295 |\n",
      "|    std                  | 1.21         |\n",
      "|    value_loss           | 0.337        |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 99          |\n",
      "|    iterations           | 83          |\n",
      "|    time_elapsed         | 1701        |\n",
      "|    total_timesteps      | 169984      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010032611 |\n",
      "|    clip_fraction        | 0.101       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -46.8       |\n",
      "|    explained_variance   | 0.39        |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.299      |\n",
      "|    n_updates            | 820         |\n",
      "|    policy_gradient_loss | -0.00986    |\n",
      "|    reward               | -0.14833826 |\n",
      "|    std                  | 1.22        |\n",
      "|    value_loss           | 0.444       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 99          |\n",
      "|    iterations           | 84          |\n",
      "|    time_elapsed         | 1722        |\n",
      "|    total_timesteps      | 172032      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008981023 |\n",
      "|    clip_fraction        | 0.097       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -46.9       |\n",
      "|    explained_variance   | 0.457       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.202      |\n",
      "|    n_updates            | 830         |\n",
      "|    policy_gradient_loss | -0.0102     |\n",
      "|    reward               | 0.02842059  |\n",
      "|    std                  | 1.22        |\n",
      "|    value_loss           | 0.431       |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 99           |\n",
      "|    iterations           | 85           |\n",
      "|    time_elapsed         | 1744         |\n",
      "|    total_timesteps      | 174080       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.008114837  |\n",
      "|    clip_fraction        | 0.0938       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -46.9        |\n",
      "|    explained_variance   | 0.638        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.406       |\n",
      "|    n_updates            | 840          |\n",
      "|    policy_gradient_loss | -0.00865     |\n",
      "|    reward               | -0.109711975 |\n",
      "|    std                  | 1.22         |\n",
      "|    value_loss           | 0.138        |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 99          |\n",
      "|    iterations           | 86          |\n",
      "|    time_elapsed         | 1766        |\n",
      "|    total_timesteps      | 176128      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010405574 |\n",
      "|    clip_fraction        | 0.124       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -47         |\n",
      "|    explained_variance   | 0.537       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.372      |\n",
      "|    n_updates            | 850         |\n",
      "|    policy_gradient_loss | -0.0127     |\n",
      "|    reward               | 0.16359271  |\n",
      "|    std                  | 1.23        |\n",
      "|    value_loss           | 0.201       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 99          |\n",
      "|    iterations           | 87          |\n",
      "|    time_elapsed         | 1787        |\n",
      "|    total_timesteps      | 178176      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009163184 |\n",
      "|    clip_fraction        | 0.091       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -47.1       |\n",
      "|    explained_variance   | 0.427       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.348      |\n",
      "|    n_updates            | 860         |\n",
      "|    policy_gradient_loss | -0.00895    |\n",
      "|    reward               | 0.06222023  |\n",
      "|    std                  | 1.23        |\n",
      "|    value_loss           | 0.2         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 99          |\n",
      "|    iterations           | 88          |\n",
      "|    time_elapsed         | 1807        |\n",
      "|    total_timesteps      | 180224      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008654877 |\n",
      "|    clip_fraction        | 0.112       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -47.2       |\n",
      "|    explained_variance   | 0.533       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.412      |\n",
      "|    n_updates            | 870         |\n",
      "|    policy_gradient_loss | -0.00721    |\n",
      "|    reward               | 0.33722913  |\n",
      "|    std                  | 1.24        |\n",
      "|    value_loss           | 0.116       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 99          |\n",
      "|    iterations           | 89          |\n",
      "|    time_elapsed         | 1828        |\n",
      "|    total_timesteps      | 182272      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010415852 |\n",
      "|    clip_fraction        | 0.126       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -47.3       |\n",
      "|    explained_variance   | 0.628       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.457      |\n",
      "|    n_updates            | 880         |\n",
      "|    policy_gradient_loss | -0.0129     |\n",
      "|    reward               | -0.18604574 |\n",
      "|    std                  | 1.24        |\n",
      "|    value_loss           | 0.0787      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 99          |\n",
      "|    iterations           | 90          |\n",
      "|    time_elapsed         | 1850        |\n",
      "|    total_timesteps      | 184320      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008567282 |\n",
      "|    clip_fraction        | 0.109       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -47.4       |\n",
      "|    explained_variance   | 0.341       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.385      |\n",
      "|    n_updates            | 890         |\n",
      "|    policy_gradient_loss | -0.0115     |\n",
      "|    reward               | 0.09400841  |\n",
      "|    std                  | 1.25        |\n",
      "|    value_loss           | 0.233       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 99          |\n",
      "|    iterations           | 91          |\n",
      "|    time_elapsed         | 1872        |\n",
      "|    total_timesteps      | 186368      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009021642 |\n",
      "|    clip_fraction        | 0.0993      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -47.5       |\n",
      "|    explained_variance   | 0.33        |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.356      |\n",
      "|    n_updates            | 900         |\n",
      "|    policy_gradient_loss | -0.00927    |\n",
      "|    reward               | 0.018443769 |\n",
      "|    std                  | 1.25        |\n",
      "|    value_loss           | 0.218       |\n",
      "-----------------------------------------\n",
      "day: 2265, episode: 130\n",
      "begin_total_asset: 100000.00\n",
      "end_total_asset: 127275.51\n",
      "total_reward: 27275.51\n",
      "total_cost: 153541.60\n",
      "total_trades: 39917\n",
      "Sharpe: 0.233\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 99           |\n",
      "|    iterations           | 92           |\n",
      "|    time_elapsed         | 1892         |\n",
      "|    total_timesteps      | 188416       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0082920175 |\n",
      "|    clip_fraction        | 0.0907       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -47.6        |\n",
      "|    explained_variance   | 0.143        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.326       |\n",
      "|    n_updates            | 910          |\n",
      "|    policy_gradient_loss | -0.00779     |\n",
      "|    reward               | -0.02590301  |\n",
      "|    std                  | 1.25         |\n",
      "|    value_loss           | 0.416        |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 99          |\n",
      "|    iterations           | 93          |\n",
      "|    time_elapsed         | 1913        |\n",
      "|    total_timesteps      | 190464      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009367485 |\n",
      "|    clip_fraction        | 0.0906      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -47.6       |\n",
      "|    explained_variance   | 0.259       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.387      |\n",
      "|    n_updates            | 920         |\n",
      "|    policy_gradient_loss | -0.00878    |\n",
      "|    reward               | 0.035072908 |\n",
      "|    std                  | 1.25        |\n",
      "|    value_loss           | 0.305       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 99          |\n",
      "|    iterations           | 94          |\n",
      "|    time_elapsed         | 1934        |\n",
      "|    total_timesteps      | 192512      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008388054 |\n",
      "|    clip_fraction        | 0.104       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -47.7       |\n",
      "|    explained_variance   | 0.436       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.422      |\n",
      "|    n_updates            | 930         |\n",
      "|    policy_gradient_loss | -0.00894    |\n",
      "|    reward               | -0.15320575 |\n",
      "|    std                  | 1.25        |\n",
      "|    value_loss           | 0.184       |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 99           |\n",
      "|    iterations           | 95           |\n",
      "|    time_elapsed         | 1955         |\n",
      "|    total_timesteps      | 194560       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0073689166 |\n",
      "|    clip_fraction        | 0.0875       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -47.7        |\n",
      "|    explained_variance   | 0.532        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.447       |\n",
      "|    n_updates            | 940          |\n",
      "|    policy_gradient_loss | -0.00917     |\n",
      "|    reward               | -0.04513174  |\n",
      "|    std                  | 1.26         |\n",
      "|    value_loss           | 0.11         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 99          |\n",
      "|    iterations           | 96          |\n",
      "|    time_elapsed         | 1976        |\n",
      "|    total_timesteps      | 196608      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007643791 |\n",
      "|    clip_fraction        | 0.0924      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -47.8       |\n",
      "|    explained_variance   | 0.534       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.43       |\n",
      "|    n_updates            | 950         |\n",
      "|    policy_gradient_loss | -0.00986    |\n",
      "|    reward               | -0.18219008 |\n",
      "|    std                  | 1.26        |\n",
      "|    value_loss           | 0.114       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 99          |\n",
      "|    iterations           | 97          |\n",
      "|    time_elapsed         | 1995        |\n",
      "|    total_timesteps      | 198656      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009165008 |\n",
      "|    clip_fraction        | 0.0998      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -47.8       |\n",
      "|    explained_variance   | 0.461       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.409      |\n",
      "|    n_updates            | 960         |\n",
      "|    policy_gradient_loss | -0.0107     |\n",
      "|    reward               | 0.066408016 |\n",
      "|    std                  | 1.26        |\n",
      "|    value_loss           | 0.126       |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 99           |\n",
      "|    iterations           | 98           |\n",
      "|    time_elapsed         | 2014         |\n",
      "|    total_timesteps      | 200704       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0081176665 |\n",
      "|    clip_fraction        | 0.0883       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -47.9        |\n",
      "|    explained_variance   | 0.356        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.39        |\n",
      "|    n_updates            | 970          |\n",
      "|    policy_gradient_loss | -0.00942     |\n",
      "|    reward               | -0.048490588 |\n",
      "|    std                  | 1.27         |\n",
      "|    value_loss           | 0.243        |\n",
      "------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "trained_ppo = agent.train_model(model=model_ppo, \n",
    "                             tb_log_name='ppo',\n",
    "                             total_timesteps=200000) if if_using_ppo else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "C6AidlWyvwzm"
   },
   "outputs": [],
   "source": [
    "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            ew----trained_ppo.save(TRAINED_MODEL_DIR + \"/agent_ppo\") if if_using_ppo else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Zpv4S0-fDBv"
   },
   "source": [
    "### Agent 4: TD3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "JSAHhV4Xc-bh"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_size': 100, 'buffer_size': 1000000, 'learning_rate': 0.001}\n",
      "Using cpu device\n",
      "Logging to results/td3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Program Files\\Python310\\lib\\site-packages\\stable_baselines3\\common\\buffers.py:241: UserWarning: This system does not have apparently enough memory to store the complete replay buffer 2.45GB > 1.81GB\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "agent = DRLAgent(env = env_train)\n",
    "TD3_PARAMS = {\"batch_size\": 100, \n",
    "              \"buffer_size\": 1000000, \n",
    "              \"learning_rate\": 0.001}\n",
    "\n",
    "model_td3 = agent.get_model(\"td3\",model_kwargs = TD3_PARAMS)\n",
    "\n",
    "if if_using_td3:\n",
    "  # set up logger\n",
    "  tmp_path = RESULTS_DIR + '/td3'\n",
    "  new_logger_td3 = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
    "  # Set new logger\n",
    "  model_td3.set_logger(new_logger_td3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "OSRxNYAxdKpU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "day: 2265, episode: 140\n",
      "begin_total_asset: 100000.00\n",
      "end_total_asset: 556416.84\n",
      "total_reward: 456416.84\n",
      "total_cost: 99.89\n",
      "total_trades: 43035\n",
      "Sharpe: 1.248\n",
      "=================================\n",
      "-----------------------------------\n",
      "| time/              |            |\n",
      "|    episodes        | 4          |\n",
      "|    fps             | 29         |\n",
      "|    time_elapsed    | 303        |\n",
      "|    total_timesteps | 9064       |\n",
      "| train/             |            |\n",
      "|    actor_loss      | -23.9      |\n",
      "|    critic_loss     | 0.951      |\n",
      "|    learning_rate   | 0.001      |\n",
      "|    n_updates       | 8963       |\n",
      "|    reward          | 0.27600092 |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| time/              |            |\n",
      "|    episodes        | 8          |\n",
      "|    fps             | 28         |\n",
      "|    time_elapsed    | 645        |\n",
      "|    total_timesteps | 18128      |\n",
      "| train/             |            |\n",
      "|    actor_loss      | -16.4      |\n",
      "|    critic_loss     | 0.393      |\n",
      "|    learning_rate   | 0.001      |\n",
      "|    n_updates       | 18027      |\n",
      "|    reward          | 0.27600092 |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| time/              |            |\n",
      "|    episodes        | 12         |\n",
      "|    fps             | 27         |\n",
      "|    time_elapsed    | 976        |\n",
      "|    total_timesteps | 27192      |\n",
      "| train/             |            |\n",
      "|    actor_loss      | -11.4      |\n",
      "|    critic_loss     | 1.55       |\n",
      "|    learning_rate   | 0.001      |\n",
      "|    n_updates       | 27091      |\n",
      "|    reward          | 0.27600092 |\n",
      "-----------------------------------\n",
      "day: 2265, episode: 150\n",
      "begin_total_asset: 100000.00\n",
      "end_total_asset: 556416.84\n",
      "total_reward: 456416.84\n",
      "total_cost: 99.89\n",
      "total_trades: 43035\n",
      "Sharpe: 1.248\n",
      "=================================\n",
      "-----------------------------------\n",
      "| time/              |            |\n",
      "|    episodes        | 16         |\n",
      "|    fps             | 28         |\n",
      "|    time_elapsed    | 1282       |\n",
      "|    total_timesteps | 36256      |\n",
      "| train/             |            |\n",
      "|    actor_loss      | -7.2       |\n",
      "|    critic_loss     | 0.442      |\n",
      "|    learning_rate   | 0.001      |\n",
      "|    n_updates       | 36155      |\n",
      "|    reward          | 0.27600092 |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| time/              |            |\n",
      "|    episodes        | 20         |\n",
      "|    fps             | 28         |\n",
      "|    time_elapsed    | 1570       |\n",
      "|    total_timesteps | 45320      |\n",
      "| train/             |            |\n",
      "|    actor_loss      | -3.6       |\n",
      "|    critic_loss     | 0.212      |\n",
      "|    learning_rate   | 0.001      |\n",
      "|    n_updates       | 45219      |\n",
      "|    reward          | 0.27600092 |\n",
      "-----------------------------------\n"
     ]
    }
   ],
   "source": [
    "trained_td3 = agent.train_model(model=model_td3, \n",
    "                             tb_log_name='td3',\n",
    "                             total_timesteps=50000) if if_using_td3 else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "OkJV6V_mv2hw"
   },
   "outputs": [],
   "source": [
    "trained_td3.save(TRAINED_MODEL_DIR + \"/agent_td3\") if if_using_td3 else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dr49PotrfG01"
   },
   "source": [
    "### Agent 5: SAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "xwOhVjqRkCdM"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_size': 128, 'buffer_size': 100000, 'learning_rate': 0.0001, 'learning_starts': 100, 'ent_coef': 'auto_0.1'}\n",
      "Using cpu device\n",
      "Logging to results/sac\n"
     ]
    }
   ],
   "source": [
    "agent = DRLAgent(env = env_train)\n",
    "SAC_PARAMS = {\n",
    "    \"batch_size\": 128,\n",
    "    \"buffer_size\": 100000,\n",
    "    \"learning_rate\": 0.0001,\n",
    "    \"learning_starts\": 100,\n",
    "    \"ent_coef\": \"auto_0.1\",\n",
    "}\n",
    "\n",
    "model_sac = agent.get_model(\"sac\",model_kwargs = SAC_PARAMS)\n",
    "\n",
    "if if_using_sac:\n",
    "  # set up logger\n",
    "  tmp_path = RESULTS_DIR + '/sac'\n",
    "  new_logger_sac = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
    "  # Set new logger\n",
    "  model_sac.set_logger(new_logger_sac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "K8RSdKCckJyH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "day: 2265, episode: 160\n",
      "begin_total_asset: 100000.00\n",
      "end_total_asset: 54683.75\n",
      "total_reward: -45316.25\n",
      "total_cost: 40539.73\n",
      "total_trades: 34462\n",
      "Sharpe: 0.022\n",
      "=================================\n",
      "-----------------------------------\n",
      "| time/              |            |\n",
      "|    episodes        | 4          |\n",
      "|    fps             | 21         |\n",
      "|    time_elapsed    | 417        |\n",
      "|    total_timesteps | 9064       |\n",
      "| train/             |            |\n",
      "|    actor_loss      | 487        |\n",
      "|    critic_loss     | 19.2       |\n",
      "|    ent_coef        | 0.182      |\n",
      "|    ent_coef_loss   | 9.2        |\n",
      "|    learning_rate   | 0.0001     |\n",
      "|    n_updates       | 8963       |\n",
      "|    reward          | 0.23417696 |\n",
      "-----------------------------------\n",
      "-------------------------------------\n",
      "| time/              |              |\n",
      "|    episodes        | 8            |\n",
      "|    fps             | 21           |\n",
      "|    time_elapsed    | 845          |\n",
      "|    total_timesteps | 18128        |\n",
      "| train/             |              |\n",
      "|    actor_loss      | 403          |\n",
      "|    critic_loss     | 5.77         |\n",
      "|    ent_coef        | 0.0793       |\n",
      "|    ent_coef_loss   | -62.8        |\n",
      "|    learning_rate   | 0.0001       |\n",
      "|    n_updates       | 18027        |\n",
      "|    reward          | -0.017095948 |\n",
      "-------------------------------------\n",
      "day: 2265, episode: 170\n",
      "begin_total_asset: 100000.00\n",
      "end_total_asset: 14065.38\n",
      "total_reward: -85934.62\n",
      "total_cost: 90623.87\n",
      "total_trades: 38016\n",
      "Sharpe: -0.661\n",
      "=================================\n",
      "------------------------------------\n",
      "| time/              |             |\n",
      "|    episodes        | 12          |\n",
      "|    fps             | 21          |\n",
      "|    time_elapsed    | 1274        |\n",
      "|    total_timesteps | 27192       |\n",
      "| train/             |             |\n",
      "|    actor_loss      | 262         |\n",
      "|    critic_loss     | 2.94        |\n",
      "|    ent_coef        | 0.0319      |\n",
      "|    ent_coef_loss   | -84.9       |\n",
      "|    learning_rate   | 0.0001      |\n",
      "|    n_updates       | 27091       |\n",
      "|    reward          | 0.049659535 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/              |             |\n",
      "|    episodes        | 16          |\n",
      "|    fps             | 21          |\n",
      "|    time_elapsed    | 1704        |\n",
      "|    total_timesteps | 36256       |\n",
      "| train/             |             |\n",
      "|    actor_loss      | 178         |\n",
      "|    critic_loss     | 1.26        |\n",
      "|    ent_coef        | 0.0128      |\n",
      "|    ent_coef_loss   | -132        |\n",
      "|    learning_rate   | 0.0001      |\n",
      "|    n_updates       | 36155       |\n",
      "|    reward          | 0.019281033 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/              |              |\n",
      "|    episodes        | 20           |\n",
      "|    fps             | 21           |\n",
      "|    time_elapsed    | 2133         |\n",
      "|    total_timesteps | 45320        |\n",
      "| train/             |              |\n",
      "|    actor_loss      | 111          |\n",
      "|    critic_loss     | 0.367        |\n",
      "|    ent_coef        | 0.00495      |\n",
      "|    ent_coef_loss   | -242         |\n",
      "|    learning_rate   | 0.0001       |\n",
      "|    n_updates       | 45219        |\n",
      "|    reward          | -0.056254026 |\n",
      "-------------------------------------\n",
      "day: 2265, episode: 180\n",
      "begin_total_asset: 100000.00\n",
      "end_total_asset: 136454.55\n",
      "total_reward: 36454.55\n",
      "total_cost: 77785.53\n",
      "total_trades: 39786\n",
      "Sharpe: 0.267\n",
      "=================================\n",
      "------------------------------------\n",
      "| time/              |             |\n",
      "|    episodes        | 24          |\n",
      "|    fps             | 21          |\n",
      "|    time_elapsed    | 2560        |\n",
      "|    total_timesteps | 54384       |\n",
      "| train/             |             |\n",
      "|    actor_loss      | 67.5        |\n",
      "|    critic_loss     | 0.257       |\n",
      "|    ent_coef        | 0.00201     |\n",
      "|    ent_coef_loss   | -262        |\n",
      "|    learning_rate   | 0.0001      |\n",
      "|    n_updates       | 54283       |\n",
      "|    reward          | -0.20778376 |\n",
      "------------------------------------\n",
      "-----------------------------------\n",
      "| time/              |            |\n",
      "|    episodes        | 28         |\n",
      "|    fps             | 21         |\n",
      "|    time_elapsed    | 2993       |\n",
      "|    total_timesteps | 63448      |\n",
      "| train/             |            |\n",
      "|    actor_loss      | 45.1       |\n",
      "|    critic_loss     | 0.128      |\n",
      "|    ent_coef        | 0.000825   |\n",
      "|    ent_coef_loss   | -249       |\n",
      "|    learning_rate   | 0.0001     |\n",
      "|    n_updates       | 63347      |\n",
      "|    reward          | -0.3046309 |\n",
      "-----------------------------------\n"
     ]
    }
   ],
   "source": [
    "trained_sac = agent.train_model(model=model_sac, \n",
    "                             tb_log_name='sac',\n",
    "                             total_timesteps=70000) if if_using_sac else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "_SpZoQgPv7GO"
   },
   "outputs": [],
   "source": [
    "trained_sac.save(TRAINED_MODEL_DIR + \"/agent_sac\") if if_using_sac else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PgGm3dQZfRks"
   },
   "source": [
    "## Save the trained agent\n",
    "Trained agents should have already been saved in the \"trained_models\" drectory after you run the code blocks above.\n",
    "\n",
    "For Colab users, the zip files should be at \"./trained_models\" or \"/content/trained_models\".\n",
    "\n",
    "For users running on your local environment, the zip files should be at \"./trained_models\"."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "MRiOtrywfAo1",
    "_gDkU-j-fCmZ",
    "3Zpv4S0-fDBv",
    "Dr49PotrfG01"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
